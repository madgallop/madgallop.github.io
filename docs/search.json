[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/perceptron_blog/perceptronBlogPost.html",
    "href": "posts/perceptron_blog/perceptronBlogPost.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "To view the source code containing the perceptron algorithm itself, pelase follow this link: https://github.com/madgallop/madgallop.github.io/blob/main/posts/perceptron_blog/perceptron.py\n\n\n\nFirst of all, what is the perceptron algorithm?\n\n\n\nScreen Shot 2023-02-27 at 5.05.36 PM.png\n\n\nThe source code containing my implementation of the perceptron algorithm can be found here: https://github.com/madgallop/madgallop.github.io/blob/main/posts/perceptron_blog/perceptron.py\nThis source code contains three primary functions: fit, predict, and score. Fit is the primary method of the perceptron, and is the one that actually performs the update. This function initializes a random initial weight vector and then until convergence, picks a random index, and updates the algorithm. Predict and score compare the labels predicted by the update with the true labels provided, and calculate the algorithm’s accuracy at each time stamp.\nThis blog post exemplifies the perceptron algorithm using three different examples: linearly seperable data in 2 dimensions, non-linearly seperable data in 2 dimensions, and data in 5 dimensions.\nBefore beginning, import all necessary packages\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\n\n\n\nIn this example, the perceptron algorithm converges, indicating that the data set is linearly seperable and can be divided well by a certain line.\n\n\n\nnp.random.seed(12345)\n\nn = 100 \np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Seperable Data in 2D\")\n\n\n\n\n\n\n\n\nfrom perceptron import Perceptron #import class from .py file\n\n\n#create an instance of the class and fit it to data.\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\np.w\nprint(\"Accuracy of the last 10 timestamps of the perceptron:\")\nprint(p.history[-10:]) #just the last few values\n\nAccuracy of the last 10 timestamps of the perceptron:\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy Over Time\")\n\n\n\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Perceptron on Seperable Data\")\n\n\n\n\n\n\n\n\np.score(X, y)\n\n1.0\n\n\n\n\n\n\nIn this example, the perceptron algorithm will not converge or settle on a final value, but will run until the specified maximum number of iterations is reached. In this example, the algorithm will not reach perfect accuracy.\n\n\n\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (-1.7, -1.7)])\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\nxlab2 = plt.xlabel(\"Feature 1\")\nylab2 = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Non-Seperable Data in 2D\")\n\n\n\n\n\n\n\n\nfrom perceptron import Perceptron\n\n#create an instance of the class and fit it to data.\np2 = Perceptron()\np2.fit(X2, y2, max_steps = 1000)\n\np2.w\nprint(\"Accuracy of the last 10 timestamps of the perceptron:\")\nprint(p2.history[-10:]) \n\nAccuracy of the last 10 timestamps of the perceptron:\n[0.58, 0.58, 0.5, 0.5, 0.48, 0.48, 0.5, 0.49, 0.49, 0.47]\n\n\n\n\n\n\n#visualize how the score evolved over time\nfig = plt.plot(p2.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy Over Time\")\n\n\n\n\n\n\n\n\n#visualize the line that the algorithm finds to separate the data\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2)\nfig = draw_line(p2.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Perceptron on Non-Seperable Data\")\n\n\n\n\nHere, the seperating line is far from perfect. The line simply falls on whichever point was selected in the final iteration of the algorithm.\n\n\n\n\nIn this experiment, the algorithm will run on 5 features and then evaluate whether or not the data is linearly separable.\n\np_features3 = 6\n\nX3, y3 = make_blobs(n_samples = 100, n_features = p_features3 - 1, centers = [(-1.7,-1.7), (1.7,1.7)])\n\n\n\n\nfrom perceptron import Perceptron \n\n#create an instance of the class and fit it to data.\np3 = Perceptron()\np3.fit(X3, y3, max_steps = 1000)\n\np3.w\nprint(p3.history[-10:]) #just the last few values\n\n[0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 1.0]\n\n\n\n\n\n\n#visualize how the score evolved over time\nfig = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy Over Time\")\n\n\n\n\nIn this example, the perceptron converges and does so realtively quickly. The data originates from the data in Example 1, which was linearly seperable in 2 dimensions. Given that, the data should still be seperable in 5.\n\n\n\n\nThe runtime complexity of the perceptron algorithm is dependent on the number of features p the data has, but not with the number of points n for a single iteration of the perceptron algorithm."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post implementing and experimenting with the Perceptron Algorithm on a variety of data sets\n\n\n\n\n\n\nFeb 27, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Madeleine. This blog documents my learning in CSCI 0451- Machine Learning in Spring 2023."
  }
]