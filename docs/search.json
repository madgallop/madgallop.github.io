[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/perceptron_blog/perceptronBlogPost.html",
    "href": "posts/perceptron_blog/perceptronBlogPost.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "To view the source code containing the perceptron algorithm itself, pelase follow this link: https://github.com/madgallop/madgallop.github.io/blob/main/posts/perceptron_blog/perceptron.py\n\n\n\nFirst of all, what is the perceptron algorithm?\n\n\n\nScreen Shot 2023-02-27 at 5.05.36 PM.png\n\n\nThe source code containing my implementation of the perceptron algorithm can be found here: https://github.com/madgallop/madgallop.github.io/blob/main/posts/perceptron_blog/perceptron.py\nThis source code contains three primary functions: fit, predict, and score. Fit is the primary method of the perceptron, and is the one that actually performs the update. This function initializes a random initial weight vector and then until convergence, picks a random index, and updates the algorithm. Predict and score compare the labels predicted by the update with the true labels provided, and calculate the algorithm’s accuracy at each time stamp.\nThis blog post exemplifies the perceptron algorithm using three different examples: linearly seperable data in 2 dimensions, non-linearly seperable data in 2 dimensions, and data in 5 dimensions.\nBefore beginning, import all necessary packages\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\n\n\n\nIn this example, the perceptron algorithm converges, indicating that the data set is linearly seperable and can be divided well by a certain line.\n\n\n\nnp.random.seed(12345)\n\nn = 100 \np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Seperable Data in 2D\")\n\n\n\n\n\n\n\n\nfrom perceptron import Perceptron #import class from .py file\n\n\n#create an instance of the class and fit it to data.\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\np.w\nprint(\"Accuracy of the last 10 timestamps of the perceptron:\")\nprint(p.history[-10:]) #just the last few values\n\nAccuracy of the last 10 timestamps of the perceptron:\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy Over Time\")\n\n\n\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Perceptron on Seperable Data\")\n\n\n\n\n\n\n\n\np.score(X, y)\n\n1.0\n\n\n\n\n\n\nIn this example, the perceptron algorithm will not converge or settle on a final value, but will run until the specified maximum number of iterations is reached. In this example, the algorithm will not reach perfect accuracy.\n\n\n\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (-1.7, -1.7)])\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\nxlab2 = plt.xlabel(\"Feature 1\")\nylab2 = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Non-Seperable Data in 2D\")\n\n\n\n\n\n\n\n\nfrom perceptron import Perceptron\n\n#create an instance of the class and fit it to data.\np2 = Perceptron()\np2.fit(X2, y2, max_steps = 1000)\n\np2.w\nprint(\"Accuracy of the last 10 timestamps of the perceptron:\")\nprint(p2.history[-10:]) \n\nAccuracy of the last 10 timestamps of the perceptron:\n[0.58, 0.58, 0.5, 0.5, 0.48, 0.48, 0.5, 0.49, 0.49, 0.47]\n\n\n\n\n\n\n#visualize how the score evolved over time\nfig = plt.plot(p2.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy Over Time\")\n\n\n\n\n\n\n\n\n#visualize the line that the algorithm finds to separate the data\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2)\nfig = draw_line(p2.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Perceptron on Non-Seperable Data\")\n\n\n\n\nHere, the seperating line is far from perfect. The line simply falls on whichever point was selected in the final iteration of the algorithm.\n\n\n\n\nIn this experiment, the algorithm will run on 5 features and then evaluate whether or not the data is linearly separable.\n\np_features3 = 6\n\nX3, y3 = make_blobs(n_samples = 100, n_features = p_features3 - 1, centers = [(-1.7,-1.7), (1.7,1.7)])\n\n\n\n\nfrom perceptron import Perceptron \n\n#create an instance of the class and fit it to data.\np3 = Perceptron()\np3.fit(X3, y3, max_steps = 1000)\n\np3.w\nprint(p3.history[-10:]) #just the last few values\n\n[0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 1.0]\n\n\n\n\n\n\n#visualize how the score evolved over time\nfig = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy Over Time\")\n\n\n\n\nIn this example, the perceptron converges and does so realtively quickly. The data originates from the data in Example 1, which was linearly seperable in 2 dimensions. Given that, the data should still be seperable in 5.\n\n\n\n\nThe runtime complexity of the perceptron algorithm is dependent on the number of features p the data has, but not with the number of points n for a single iteration of the perceptron algorithm."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "This blog post implements simple gradient descent and stochastic gradient descent and compares their performances for training logistic regression.\n\n\n\n\n\n\nMar 3, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post implementing and experimenting with the Perceptron Algorithm on a variety of data sets\n\n\n\n\n\n\nFeb 27, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Madeleine. This blog documents my learning in CSCI 0451- Machine Learning in Spring 2023."
  },
  {
    "objectID": "posts/regression_blog/LogisticRegressionBlogPost.html",
    "href": "posts/regression_blog/LogisticRegressionBlogPost.html",
    "title": "Optimization for Logistic Regression Blog",
    "section": "",
    "text": "The following blog post will implement gradient descent for logistic regression in an object-oriented paradigm and implement a key variant of gradient descent called stochastic gradient descent. It will then perform several simple experiments on synthetic data to see which of these algorithms converges most quickly to a satisfactory logistic regression model.\nTo view the source code, pelase follow this link: https://github.com/madgallop/madgallop.github.io/blob/main/posts/regression_blog/logistic.py\n\n\nLinear regression is an approach to modeling the relationship between independent variable/s and a dependent variable. It is the best-fit regression line to describes data with minimal error, as measured by loss. Gradient descent and the algorithm in this blogpost aim to minimize the loss function.\n\n\n\nIn order to implement this algorithm, I built up tools and concepts we discussed in class. I used functions from the lecture on Convex Linear Models and Logistic Regression (predict, sigmoid, loss), and adapted the score method from the perceptron blogpost. The primary challenge of this assignment was implementing the gradient. I first time to spend some time figuring out what the gradient descent function actually is for linear regression. I first tried to implement a partial deriatives approach to find the slope and intercept, but eventaully found the key to be in equation below from the Optimization with Gradient Descent lecture. I finally understood what is happening in gradient descent of logisitic regression, and was able to replicate the equation in code.\n\n\n\nScreen Shot 2023-03-03 at 7.35.26 PM.png\n\n\nThe other challenging part of this assignment was formatting vectors and matrices to have dimensions that allowed for multiplication and other operations. I had to always be asking myself what dimension matrices certain lines of code (like the gradient) were taking in and spitting out, and adjusting the algorithm with lines such as y[:,np.newaxis] and .T to transform matrices.\n\n\n\nBefore beginning, import all the required packages:\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n{'divide': 'ignore', 'over': 'ignore', 'under': 'ignore', 'invalid': 'ignore'}\n\n\nScript to autoload logistic.py file\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\nVisualize the example data\n\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nNow, fit the model\n\nfrom logistic import LogisticRegression \nLR = LogisticRegression()\nLR.fit(X, y, alpha = 1, max_epochs = 1000)\n\n# inspect the fitted value of w\nprint(LR.w)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\nprint(LR.w.shape)\n\nplt.plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\nplt.title(\"Gradient Descent\")\n\n[[ 2.01961227]\n [ 2.46881413]\n [-0.32132074]]\n(3, 1)\n\n\nText(0.5, 1.0, 'Gradient Descent')\n\n\n\n\n\nAs you can see, the model performs fairly well. Let’s take a look at the evolution of the loss over time, as well as the overall accuracy.\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \nplt.title(\"Loss History of Gradient Descent\")\n\nText(0.5, 1.0, 'Loss History of Gradient Descent')\n\n\n\n\n\n\nLR.score(X,y)\n\n0.95\n\n\nGradient descent computes a fairly accurate value of the parameter vector w. Because the algorithm mainly utilized numpy operations instead of loops, it is efficient, and as the loss history graph depicts, starts to converge. Let’s see if we can improve upon this algorithm by altering batch size!\n\n\n\nStochastic gradient descent computes the stochastic gradient by computing the gradient on subsets of the data. The algorithm works as follows: 1. Shuffle the points randomly. 2. Pick the first k random points, compute the stochastic gradient, and then perform an update. 3. Pick the next k random points and repeat.. 4. When we have gone through all n points, reshuffle them all randomly and proceed again.\n\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X, y, \n                  alpha = .05, \n                  batch_size = 100,\n                  m_epochs = 100\n                  )\n# inspect the fitted value of w\nprint(LR2.w)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\nplt.plot(f1, (LR2.w[2] - f1*LR2.w[0])/LR2.w[1], color = \"black\")\nplt.title(\"Stochastic Gradient Descent\")\n\n[[1.38594862]\n [1.58157147]\n [0.35018386]]\n\n\nText(0.5, 1.0, 'Stochastic Gradient Descent')\n\n\n\n\n\nThe model fits the data quite well. Let’s examine the loss history and accuracy again!\n\nnum_steps = len(LR2.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"stochastic gradient\")\nplt.title(\"Loss History Stochastic Gradient Descent\")\n\nText(0.5, 1.0, 'Loss History Stochastic Gradient Descent')\n\n\n\n\n\n\nLR2.score(X,y)\n\n0.935\n\n\nThe loss history chart bounces around more than for typical gradient descent, but gives us a good sense of how the stochastic gradient affects the regression over time. The algorithm is quite accurate as well! As I’ve learned, stochastic gradient descent tends to get to a “pretty good” result faster than standard gradient descent, but can “bounce around” near the good solution. Standard gradient descent might need more epochs to find a good solution, but quickly “settles down” once it finds it.\n\n\n\n\nLR_01 = LogisticRegression()\n\nLR_01.fit(X, y, alpha = .01, max_epochs = 10000)\nnum_steps01 = len(LR_01.loss_history)\n\nLR_1 = LogisticRegression()\n\nLR_1.fit(X, y, alpha = 1, max_epochs = 1000)\nnum_steps1 = len(LR_1.loss_history)\n\nLR_100 = LogisticRegression()\n\nLR_100.fit(X, y, alpha = 100, max_epochs = 100)\nnum_steps100 = len(LR_100.loss_history)\n\n\nfig, axarr = plt.subplots(1,4, layout = \"tight\", figsize = (12,6))\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = \".01\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR_01.w[2] - f1*LR_01.w[0])/LR_01.w[1], color = \"black\")\n\naxarr[1].plot(np.arange(num_steps01) + 1, LR_01.loss_history, label = \"Loss alpha = .01\")\naxarr[1].loglog()\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \".01\")\n\naxarr[2].plot(np.arange(num_steps1) + 1, LR_1.loss_history, label = \"Loss alpha = 1\")\naxarr[2].loglog()\naxarr[2].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"1\")\n\naxarr[3].plot(np.arange(num_steps100) + 1, LR_100.loss_history, label = \"Loss alpha = 100\")\naxarr[3].loglog()\naxarr[3].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"100\")\n\n[Text(0.5, 0, 'Iteration number'),\n Text(0, 0.5, 'Empirical Risk'),\n Text(0.5, 1.0, '100')]\n\n\n\n\n\nThis example illustrates the loss history for different learning rate choices. When the learning rate is small, the algorithm converges relatively slowely. With a larger alpha value (such as 1), the algorithm converges with fewer iterations, and in a more direct manner. And when alpha is very large, say, 100, the algorithm fails to converge, and fails rather fast.\n\n\n\nCreate a new data set with 10 features\n\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nRun stochastic gradient descent, investigating how choice of batch size affects how fast the algorithm converges\n\nLR_ex2 = LogisticRegression()\nLR_ex2.fit_stochastic(X, y, alpha = .01, batch_size = 2, m_epochs = 1000)\nnum_steps2 = len(LR_ex2.loss_history)\n\nLR_ex10 = LogisticRegression()\nLR_ex10.fit_stochastic(X, y, alpha = .01, batch_size = 10, m_epochs = 1000)\nnum_steps10 = len(LR_ex10.loss_history)\n\nLR_ex100 = LogisticRegression()\nLR_ex100.fit_stochastic(X, y, alpha = .01, batch_size = 100, m_epochs = 1000)\nnum_steps100 = len(LR_ex100.loss_history)\n\nLR_ex5000 = LogisticRegression()\nLR_ex5000.fit_stochastic(X, y, alpha = .01, batch_size = 5000, m_epochs = 1000)\nnum_steps5000 = len(LR_ex5000.loss_history)\n\n\nfig, axarr = plt.subplots(1,4, layout = \"tight\", figsize = (15,6))\n\naxarr[0].plot(np.arange(num_steps2) + 1, LR_ex2.loss_history, label = \"2\")\naxarr[0].loglog()\naxarr[0].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"2\")\n\naxarr[1].plot(np.arange(num_steps10) + 1, LR_ex10.loss_history, label = \"Loss alpha = 10\")\naxarr[1].loglog()\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"10\")\n\naxarr[2].plot(np.arange(num_steps100) + 1, LR_ex100.loss_history, label = \"Loss alpha = 100\")\naxarr[2].loglog()\naxarr[2].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"100\")\n\naxarr[3].plot(np.arange(num_steps5000) + 1, LR_ex5000.loss_history, label = \"Loss alpha = 5000\")\naxarr[3].loglog()\naxarr[3].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"5000\")\n\n[Text(0.5, 0, 'Iteration number'),\n Text(0, 0.5, 'Empirical Risk'),\n Text(0.5, 1.0, '5000')]\n\n\n\n\n\nAs the batch size increases, the model works through more training samples before updating its parameters. Thus, really large batch sizes converge faster without as much “bouncing” as compared to smaller batch sizes. While still displaying a downward trend, smaller batch sizes bounce around much more and converge at a slower rate."
  }
]