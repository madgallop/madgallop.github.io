[
  {
    "objectID": "posts/quant_bias_blog/quantBiasBlog.html",
    "href": "posts/quant_bias_blog/quantBiasBlog.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness Blog",
    "section": "",
    "text": "References\n\nAhmad, Afra Saeed, Isaac Sabat, Rachel Trump-Steele, and Eden King. 2019. “Evidence-Based Strategies for Improving Diversity and Inclusion in Undergraduate Research Labs.” Frontiers in Psychology 10: 1305.\n\n\nArellano, Lucy. 2022. “Questioning the Science: How Quantitative Methodologies Perpetuate Inequity in Higher Education.” Education Sciences 12 (2): 116.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” In Conference on Fairness, Accountability and Transparency, 77–91. PMLR.\n\n\nGillborn, David, Paul Warmington, and Sean Demack. 2018. “QuantCrit: Education, Policy,‘big Data’and Principles for a Critical Race Theory of Statistics.” Race Ethnicity and Education 21 (2): 158–79.\n\n\nNarayanan, Arvind. 2022. “The Limits of the Quantitative Approach to Discrimination.” Speech.\n\n\nOakley, Ann. 1998. “Gender, Methodology and People’s Ways of Knowing: Some Problems with Feminism and the Paradigm Debate in Social Science.” Sociology 32 (4): 707–31."
  },
  {
    "objectID": "posts/penguins_blog/penguins_blog.html",
    "href": "posts/penguins_blog/penguins_blog.html",
    "title": "Classifying Palmer Penguins Blog",
    "section": "",
    "text": "Chinstrap!\n\nGentoo!\n\nAdelie!\n\nTo view the source code for this project, please visit this link: https://github.com/madgallop/madgallop.github.io/blob/main/posts/penguins_blog/penguins_blog.ipynb\n\n\nThe following post will run through a complete example of a standard machine learning workflow. It will ultimately confidently classify the species of a penguin into the three categories pictured above in the smallest number of measurements necessary.\nThis post classifies penguins into three different species based on three key penguin characteristics, and builds upon on more basic machine learning models (such as perceptron) that only classify two features.\n\n\nFirst, we will import the required packages.\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom matplotlib.patches import Patch\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nNext, we will import our dataset. This dataset is the Palmer Penguins data set collected by the Palmer Station in Antarcitica in collaboration with Dr. Kristen Gorman. The data contains a number of measurements for a number of penguins from each of the three species pictured above.\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n#train.head()\n\n\n\n\nThe end goal of this post is to predict the species of a penguin based on its measurements.\nTo begin, I will explore the penguins dataset by constructing a figure and a table.\nI will first create a dataframe so that I can better group data and create more informative charts\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    #df = pd.get_dummies(df) #comment in for data analysis, out for table construction\n    return df, y\n\nX_train, y_train = prepare_data(train)\n\nBefore making some tables and figures, I am curious to run some basic statistics on the data\n\nspecies3pct = (y_train==2).mean()\nspecies2pct = (y_train==1).mean()\nspecies1pct = (y_train==0).mean() \nspecies3cnt = (y_train==2).sum()\nspecies2cnt = (y_train==1).sum()\nspecies1cnt = (y_train==0).sum()\n\nprint(\"There are approximately\",species3cnt,\"penguins in Species 3, or\",species3pct.round(2)*100,\"% of the penguins\")\nprint(\"There are approximately\",species2cnt,\"penguins in Species 2, or\",species2pct.round(2)*100,\"% of the penguins\")\nprint(\"There are approximately\",species1cnt,\"penguins in Species 1, or\",species1pct.round(2)*100,\"% of the penguins\")\n\nThere are approximately 95 penguins in Species 3, or 37.0 % of the penguins\nThere are approximately 55 penguins in Species 2, or 21.0 % of the penguins\nThere are approximately 106 penguins in Species 1, or 41.0 % of the penguins\n\n\nThese numbers are the base rates for our model. The base rate is the accuracy rate of a trivial model that doesn’t use the features. It is the accuracy I would have if I made predictions without looking at the data.\nNext, I am curious in the following:\n\nWhere do the penguins live?\nDoes their flipper length/culmen length vary by sex or island?\nDoes weight vary by sex or island?\n\n\nX_train.groupby([\"Island\",\"Sex\"])[['Body Mass (g)','Culmen Length (mm)','Flipper Length (mm)']].aggregate([np.mean]).round(2)\n\n\n\n\n\n  \n    \n      \n      \n      Body Mass (g)\n      Culmen Length (mm)\n      Flipper Length (mm)\n    \n    \n      \n      \n      mean\n      mean\n      mean\n    \n    \n      Island\n      Sex\n      \n      \n      \n    \n  \n  \n    \n      Biscoe\n      FEMALE\n      4253.28\n      42.94\n      204.67\n    \n    \n      MALE\n      5168.12\n      47.54\n      214.49\n    \n    \n      Dream\n      FEMALE\n      3435.20\n      42.45\n      190.08\n    \n    \n      MALE\n      3973.30\n      46.64\n      196.73\n    \n    \n      Torgersen\n      FEMALE\n      3371.88\n      37.46\n      189.50\n    \n    \n      MALE\n      4016.18\n      40.88\n      195.65\n    \n  \n\n\n\n\n\nX_train.groupby([\"Island\"])[['Body Mass (g)','Culmen Length (mm)','Flipper Length (mm)']].aggregate([np.mean]).round(2)\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n      Culmen Length (mm)\n      Flipper Length (mm)\n    \n    \n      \n      mean\n      mean\n      mean\n    \n    \n      Island\n      \n      \n      \n    \n  \n  \n    \n      Biscoe\n      4738.85\n      45.38\n      209.88\n    \n    \n      Dream\n      3689.78\n      44.43\n      193.23\n    \n    \n      Torgersen\n      3703.79\n      39.22\n      192.67\n    \n  \n\n\n\n\n\nX_train.groupby([\"Island\", \"Sex\"]).size().reset_index()\n\n\n\n\n\n  \n    \n      \n      Island\n      Sex\n      0\n    \n  \n  \n    \n      0\n      Biscoe\n      FEMALE\n      61\n    \n    \n      1\n      Biscoe\n      MALE\n      69\n    \n    \n      2\n      Dream\n      FEMALE\n      49\n    \n    \n      3\n      Dream\n      MALE\n      44\n    \n    \n      4\n      Torgersen\n      FEMALE\n      16\n    \n    \n      5\n      Torgersen\n      MALE\n      17\n    \n  \n\n\n\n\n\nX_train.groupby([\"Island\"]).size().reset_index()\n\n\n\n\n\n  \n    \n      \n      Island\n      0\n    \n  \n  \n    \n      0\n      Biscoe\n      130\n    \n    \n      1\n      Dream\n      93\n    \n    \n      2\n      Torgersen\n      33\n    \n  \n\n\n\n\n\nX_train.groupby([\"Sex\"])[['Body Mass (g)']].aggregate([len]).round(2)\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n    \n    \n      \n      len\n    \n    \n      Sex\n      \n    \n  \n  \n    \n      FEMALE\n      126\n    \n    \n      MALE\n      130\n    \n  \n\n\n\n\nI learned: - females tend to weigh less than males - females generally have shorter beaks - females generally have and shorter flippers. - there are slightly more males in the study - there are the most penguins on Biscoe island, then Dream, then Torgersen - there are similar amounts of penguins of different sexes on the different islands - Bisco island is home to penguins with heavier bodies, and longer beaks and flippers.\nNow I want to create a chart to better understand weight distributions of the penguins by island.\n\nsns.set_theme(style=\"whitegrid\", palette={\"Gold\",\"HotPink\",\"CornflowerBlue\"})\n\n# Draw a categorical scatterplot to show each observation\nax = sns.swarmplot(data=X_train, x='Body Mass (g)', y=\"Sex\", hue=\"Island\")\nax.set(ylabel=\"\")\nax.set(title = \"Body Mass by Island\")\n\n[Text(0.5, 1.0, 'Body Mass by Island')]\n\n\n\n\n\n\nsns.set_theme(style=\"whitegrid\", palette={\"Gold\",\"HotPink\",\"CornflowerBlue\"})\n\n# Draw a categorical scatterplot to show each observation\nax = sns.swarmplot(data=X_train, x='Flipper Length (mm)', y=\"Sex\", hue=\"Island\")\nax.set(ylabel=\"\")\nax.set(title = \"Flipper Length by Island\")\n\n[Text(0.5, 1.0, 'Flipper Length by Island')]\n\n\n\n\n\n\nsns.set_theme(style=\"whitegrid\", palette={\"Gold\",\"HotPink\",\"CornflowerBlue\"})\n\n# Draw a categorical scatterplot to show each observation\nax = sns.swarmplot(data=X_train, x='Culmen Length (mm)', y=\"Sex\", hue=\"Island\")\nax.set(ylabel=\"\")\nax.set(title = \"Culmen Length by Island\")\n\n[Text(0.5, 1.0, 'Culmen Length by Island')]\n\n\n\n\n\nWhile Biscoe island defintly hosts penguins with more mass and longer flippers, the same is not necessarily true for beak (Culmen) length. The male/female distinction does not appear to make a difference island by island.\n\n\n\nNow that we have a better sense of the data, I will prepare to implement a model. First, I modify the dataframe using pd.get_dummies to encode nominal data:\n\ndef prepare_data2(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df) #comment in for data analysis, out for table construction\n    return df, y\n\nX_train2, y_train2 = prepare_data2(train)\n\nOur first task is to find three features of the data and a model trained on those features which achieves 100% testing accuracy - One feature must be qualitative (like Island or Clutch Completion). - The other two features must be quantitative (like Body Mass (g) or Culmen Depth (mm)).\nFirst, lets figure out what features are best.\n\n\n\nAfter some experimenting, I decided to select the optimal features using Ridge CV.\n\nridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(X_train2, y_train2) #Ridge regression with built-in cross-validation.\nimportance = np.abs(ridge.coef_) # assign importance to each feature through a specific attribute\n#The features with the highest absolute coef_ value are considered the most important\n\n#create a plot to visualize the importance of the features\nfeature_names = np.array(X_train2.columns)\nplt.barh(feature_names, importance)\nplt.title(\"Feature importances via coefficients\")\nplt.show()\n\n#select a certain number of features\nthreshold = np.sort(importance)[-5]\n\nsfm = SelectFromModel(ridge, threshold=threshold).fit(X_train2, y_train2)\n#only select the first three features from the list\nselected_features = feature_names[sfm.get_support()][0:3]\n\nprint(selected_features)\nprint(f\"Features selected by SelectFromModel: {selected_features}\")\n\n\n\n\n['Culmen Depth (mm)' 'Delta 13 C (o/oo)' 'Island_Torgersen']\nFeatures selected by SelectFromModel: ['Culmen Depth (mm)' 'Delta 13 C (o/oo)' 'Island_Torgersen']\n\n\nUsing the select by model method, Culmen Depth, Delta 13 C and Island_Torgersen are the most important for correct classification of penguins.\n\n\n\nI will now explore how well linear regression works for this data, and then experiment with DecisionTreeClassifier and RandomForestClassifier.\n\n\n\n#change selected features to invlude Island_Dream and Island_Biscoe as well \nfeatures_to_test = X_train2[selected_features].join(X_train2[\"Island_Dream\"]).join(X_train2[\"Island_Biscoe\"])\nLR = LogisticRegression(random_state = 10, max_iter = 1000)\nLR.fit(features_to_test, y_train2)\n\nLR.score(features_to_test, y_train2)\n\n0.93359375\n\n\nLogistic regression is 93% accurate on the training data.\n\n\n\n\nNow, we will evaluate the model’s efficacy on the test data\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ndef prepare_data3(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df) #comment in for data analysis, out for table construction\n    return df, y\n\nX_test, y_test = prepare_data3(test)\n# print(X_test)\n# print(y_test)\n\nfeatures_to_test2 = X_test[selected_features].join(X_test[\"Island_Dream\"]).join(X_test[\"Island_Biscoe\"])\n\nLR.fit(features_to_test2, y_test)\nLR.score(features_to_test2, y_test)\n\n0.9411764705882353\n\n\nThe model is slightly more effective on the test data, at 94%. In the two cells below, I experiment with different parameters to allow for this particular model to reach 100% accuracy on the test data.\n\n#change selected features to invlude Island_Dream and Island_Biscoe as well \nfeatures_to_test_extra = ['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Torgersen', \"Island_Dream\", \"Island_Biscoe\"]\nLR_extra = LogisticRegression(random_state = 10, max_iter = 1000)\nLR_extra.fit(X_train2[features_to_test_extra], y_train2)\n\nLR_extra.score(X_train2[features_to_test_extra], y_train2)\n\n0.99609375\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ndef prepare_data3(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df) #comment in for data analysis, out for table construction\n    return df, y\n\nX_test, y_test = prepare_data3(test)\n# print(X_test)\n# print(y_test)\n\n# features_to_test2 = X_test[selected_features].join(X_test[\"Island_Dream\"]).join(X_test[\"Island_Biscoe\"])\nfeatures_to_test2_extra = ['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Torgersen', \"Island_Dream\", \"Island_Biscoe\"]\n\nLR_extra.fit(X_test[features_to_test2_extra], y_test)\nLR_extra.score(X_test[features_to_test2_extra], y_test)\n\n1.0\n\n\nWhen I changed my selected features to Culmen Depth (mm), Culment Length (mm), Island_Torgersen, Island_Dream, Island_Biscoe, the model achieved 100% accuracy on the test data. I will not plot this data, but it is nice that some selected features allow for this model to reach 100% accuracy, just not the features chosen by the “Select by Model” classification system.\n\n\n\nNow we will plot the decision regions, seperated by qualitative feature (Island: Torgersen, Dream, Biscoe)\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n\n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (15, 3)) #maybe put 1 there\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n        XY = pd.DataFrame({\n            X.columns[0] : XX,\n            X.columns[1] : YY\n        })\n\n        for j in qual_features:\n            XY[j] = 0\n\n        XY[qual_features[i]] = 1\n\n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n        axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n        ix = X[qual_features[i]] == 1\n      # plot the data\n        axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n        axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n        patches = []\n        for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n            patches.append(Patch(color = color, label = spec))\n\n        plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n        plt.tight_layout()\n\n\nplot_regions(LR, features_to_test, y_train2)\n\n\n\n\nNext, we will experiment with a couple other classifiers.\n\n\n\ndtc = DecisionTreeClassifier(max_depth=5)\n\ncross_val_score(dtc, features_to_test, y_train2, cv=5)\n\narray([0.92307692, 0.92156863, 0.84313725, 0.90196078, 0.92156863])\n\n\n\ndtc.fit(features_to_test, y_train2, sample_weight=None, check_input=True)\n\ndtc.score(features_to_test, y_train2, sample_weight=None)\n\n0.96875\n\n\nThis model is more effective than regression on the training data.\n\n\n\ndtc.fit(features_to_test2, y_test, sample_weight=None, check_input=True)\n\ndtc.score(features_to_test2, y_test)\n\n0.9852941176470589\n\n\nIt is also more effective than regression on the test data.\n\n\n\n\nplot_regions(dtc, features_to_test, y_train2)\n\n\n\n\nThis model uses more advanced geometries to separate the data\n\n\n\n\nLet’s look at one more type of classifier!\n\nrfc = RandomForestClassifier(max_depth=7, random_state=1)\nrfc.fit(features_to_test, y_train2)\n\nrfc.apply(features_to_test)\nrfc.score(features_to_test, y_train2, sample_weight=None)\n\n0.99609375\n\n\nThis model is incredibly effective for the chosen parameters!\n\n\n\nrfc.fit(features_to_test2, y_test)\n\nrfc.score(features_to_test2, y_test)\n\n1.0\n\n\nThis model achieves 100% testing accuracy, making it the best choice for the features chosen by the select from model classificiation.\n\n\n\n\nplot_regions(rfc, features_to_test, y_train2)\n\n\n\n\nThis model also uses complex geometries, but is the superior choice in classifying the penguins for the selected features.\n\n\n\n\n\n\nIn this post, we selected features from the Palmer dataset and classified penguins using machine learning. We found Ridge CV to be an effective approach in selecting features, and selected Culmen Depth, Delta 13 C and Island_Torgersen. Then, we tried a number of machine learning algorithms, including logistic regression, decision trees, and random forest with the goal of obtaining 100% accuracy on the testing data. While all of the models performed relatively well, the Random Forest Classifier yielded the best results (100% accuracy on the test data, and 99.6% on the training data), making it the best choice for these features. Thanks!"
  },
  {
    "objectID": "posts/linear_regression_blog/linear_regression_blog.html",
    "href": "posts/linear_regression_blog/linear_regression_blog.html",
    "title": "Implementing Linear Regression Blog",
    "section": "",
    "text": "Regression, as opposed to classification, is a method to predict a real number for each data point based on its features. This post will focus on least squares linear regression, which falls into our framework of convex linear models.\nThe following post begins by implementing least-squares linear regression in two ways: using the analytical formula for the optimal weight vector (requiring matrix inversion and several matrix multiplications), and using the formula for the gradient of the loss function to implement gradient descent for linear regression.\nTo view the source code, pelase follow this link: https://github.com/madgallop/madgallop.github.io/blob/main/posts/linear_regression_blog/linear_regression.py\n\n\nFirst, import required packages:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import Lasso\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nNow, create a function to make both testing and validation data to test the implementation:\n\ndef pad(X): #ensure X is the proper shape\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nVisualize the testing and validation data:\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nIt is only possible to easily visualize this problem when p_features = 1. Now, let’s implement linear regression using two different methods.\n\n\n\nWe begin by implementing the analytical formula for the optimal weight vector w_hat from the lecture notes. The following is the explicit formula for w:\n\\[\\\\{\\\\{wHat}}={(X^TX)^{-1}X^Ty}\\]\nFirst, auto-refresh the source code:\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom linear_regression import LinearRegression #call method from source code\nLR = LinearRegression()\nLR.fit_analytical(X_train, y_train) #use implementation of above formula \n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.7271\nValidation score = 0.6894\n\n\nThe score (coefficient) is always smaller than 1, with a higher value indicating better predictive performance. As usual, gaps between the training and validation scores suggest the possibility of overfitting, although further investigation is required to see whether improvement on validation data is possible. In this case, the gap between the scores is relatively small.\nThe estimated weight vector w is:\n\nLR.w\n\narray([1.07026516, 1.05566706])\n\n\nNow, let’s visualize the performance of the analytic formula on our training and validation data.\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\naxarr[0].plot(X_train, LR.predict(X_train), color = \"black\")\naxarr[1].plot(X_val, LR.predict(X_val), color = \"black\")\nplt.tight_layout()\n\n\n\n\nThese predicted regression lines coorespond pretty well with both the training and test data! Let’s compare the weights and scores using the analytical method to those using gradient descent.\n\n\n\nThe following is the formula for the gradient that our source code implements:\n\\[\\\\{\\\\{w^{(t+1)}}}={w^{(t)}- \\alpha (Pw ^{(t)}-q})\\]\nNow, let’s see if we get the same weight vector from this method using the same testing and training data:\n\nLR2 = LinearRegression() #call method\n\nLR2.fit_gradient(X_train, y_train, alpha = 0.01, max_iter = 100) #gradient does not have the divide by points thing, need to adapt the learning rate to account for fact gradietn is too bigh .\nLR2.w\n\narray([1.07026515, 1.05566706])\n\n\nThis method lends the same value for w as the analytic method. Now, let’s visualize how the score using gradient descent changes over time.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\nAs pictured, the score increases monotonically in each iteration, increasing rapidly until about 50 iterations, and then increasing at a much slower rate. Now, we will experiment with increasing the number of features.\n\n\n\nIn the following experiment, I allow p_features, the number of features used, to increase, while holding n_train, the number of training points, constant. I then assess and explain the differences between the training and validation scores.\n\nLR_ex = LinearRegression()\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\ntrainScoreArray = []\ntestScoreArray = []\n\nfor feature in range(0,(n_train-1)):\n    p_features = feature\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR_ex.fit_analytical(X_train, y_train) \n    trainScore = LR_ex.score(X_train, y_train).round(4)\n    testScore = LR_ex.score(X_val, y_val).round(4)\n    trainScoreArray.append(trainScore)\n    testScoreArray.append(testScore)\nplt.plot(trainScoreArray, label = 'training')\nplt.plot(testScoreArray, label = 'validation')\nlabels = plt.gca().set(xlabel = \"Features\", ylabel = \"Score\")\nplt.legend(loc=\"lower left\")\n\nplt.show()    \n    \n\n\n\n\nAs the number of features in this experiment increases, the validation score tends to decrease relative to the training score. It follows that increasing the number of features leads to overfitting, where the model is too finely tuned to the training data. An overfit model struggles to simply representing overarching trends. In this case, as the number of features approaches the number of training points, the model began scoring much more highly on the training data (show in blue) while not achieving as high of an accuracy score for the test data (shown in orange). Next, we will try to combat overfit models using LASSO regularization.\n\n\n\nThe LASSO algorithm uses a modified loss function with a regularization term, the effect of which is to make the entries of the weight vector w small. The LASSO algorithm tends to force each entry of the weight vector to be exactly zero. This is a helpful property in overfitted problems, especially when the number of features p is larger than the number of data points n.\n\nL = Lasso(alpha = 0.001)\n\nHere, alpha controls the strength of the regularization. Now, let’s fit this model on some data and check the coefficients:\n\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\n\nLasso(alpha=0.001)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=0.001)\n\n\nNow, let’s calculate the score:\n\nL.score(X_val, y_val)\n\n0.8268398547106713\n\n\nThe LASSO algorithm yields a high validation score.\nNow, I will replicate the same experiment I did with linear regression, increasing the number of to exceed the number of training points using the LASSO algorithm. The following also experiments with different values of alpha.\n\nL1 = Lasso(alpha = 0.001)\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\ntrainScoreArray = []\ntestScoreArray = []\n\n#alpha = 0.001\nfor feature in range(1,(n_train+1)): \n    p_features = feature\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L1.fit(X_train, y_train) \n    trainScore = L1.score(X_train, y_train).round(4)\n    testScore = L1.score(X_val, y_val).round(4)\n    trainScoreArray.append(trainScore)\n    testScoreArray.append(testScore)\n    \n#alpha = 0.01\nL2 = Lasso(alpha = 0.01)\ntrainScoreArray2 = []\ntestScoreArray2 = []\n\nfor feature in range(1,(n_train+1)): \n    p_features = feature\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L2.fit(X_train, y_train) \n    trainScore2 = L2.score(X_train, y_train).round(4)\n    testScore2 = L2.score(X_val, y_val).round(4)\n    trainScoreArray2.append(trainScore2)\n    testScoreArray2.append(testScore2)   \n    \n#alpha = 0.0001, made 0.0004 to allow model to converge \nL0 = Lasso(alpha = 0.0004)\ntrainScoreArray0 = []\ntestScoreArray0 = []\n\nfor feature in range(1,(n_train+1)): \n    p_features = feature\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L0.fit(X_train, y_train) \n    trainScore0 = L0.score(X_train, y_train).round(4)\n    testScore0 = L0.score(X_val, y_val).round(4)\n    trainScoreArray0.append(trainScore0)\n    testScoreArray0.append(testScore0)    \n    \n    \nfig, axarr = plt.subplots(1, 3, sharex = True, sharey = True, figsize = (15,6))\n#alpha = 0.0001\naxarr[0].plot(trainScoreArray0, label = 'training')\naxarr[0].plot(testScoreArray0, label = 'validation')\nlabels = plt.gca().set(xlabel = \"Features\", ylabel = \"Score\")\naxarr[0].legend(loc=\"lower left\")\naxarr[0].set(xlabel = \"Features\", ylabel = \"Score\", title = \"alpha = 0.0004\")\n\n#alpha = 0.001\naxarr[1].plot(trainScoreArray, label = 'training')\naxarr[1].plot(testScoreArray, label = 'validation')\nlabels = plt.gca().set(xlabel = \"Features\", ylabel = \"Score\")\naxarr[1].legend(loc=\"lower left\")\naxarr[1].set(xlabel = \"Features\", ylabel = \"Score\", title = \"alpha = 0.001\")\n\n#alpha = 0.01\naxarr[2].plot(trainScoreArray2, label = 'training')\naxarr[2].plot(testScoreArray2, label = 'validation')\nlabels = plt.gca().set(xlabel = \"Features\", ylabel = \"Score\")\naxarr[2].legend(loc=\"lower left\")\nplt.title(\"alpha = 0.01\")\naxarr[2].set(xlabel = \"Features\", ylabel = \"Score\", title = \"alpha = 0.01\")\n\n\nplt.show()     \n   \n\n\n\n\nCompared to standard linear regression, the LASSO algorithm tends to improve the validation score relative to the training score, especially when alpha (regularization strength) is small. For example, in standard linear regression, after about 95 features, the validation score dips into the negatives. Using the LASSO algorithm, the validation score never dips below 0.4, even when alpha is larger. Finally, let’s apply the linear regression algorithm to a real-world dataset.\n\n\n\nThe following code will use data from the Capital Bikeshare system in Washington DC, which incldues the count of bicycle users on each day for two years. The following experiment uses linear regression to see what factors most influence the numer of bike users over time.\n\nbikeshare = pd.read_csv(\"https://philchodrow.github.io/PIC16A/datasets/Bike-Sharing-Dataset/day.csv\")\n\nbikeshare.head()\n\n\n\n\n\n  \n    \n      \n      instant\n      dteday\n      season\n      yr\n      mnth\n      holiday\n      weekday\n      workingday\n      weathersit\n      temp\n      atemp\n      hum\n      windspeed\n      casual\n      registered\n      cnt\n    \n  \n  \n    \n      0\n      1\n      2011-01-01\n      1\n      0\n      1\n      0\n      6\n      0\n      2\n      0.344167\n      0.363625\n      0.805833\n      0.160446\n      331\n      654\n      985\n    \n    \n      1\n      2\n      2011-01-02\n      1\n      0\n      1\n      0\n      0\n      0\n      2\n      0.363478\n      0.353739\n      0.696087\n      0.248539\n      131\n      670\n      801\n    \n    \n      2\n      3\n      2011-01-03\n      1\n      0\n      1\n      0\n      1\n      1\n      1\n      0.196364\n      0.189405\n      0.437273\n      0.248309\n      120\n      1229\n      1349\n    \n    \n      3\n      4\n      2011-01-04\n      1\n      0\n      1\n      0\n      2\n      1\n      1\n      0.200000\n      0.212122\n      0.590435\n      0.160296\n      108\n      1454\n      1562\n    \n    \n      4\n      5\n      2011-01-05\n      1\n      0\n      1\n      0\n      3\n      1\n      1\n      0.226957\n      0.229270\n      0.436957\n      0.186900\n      82\n      1518\n      1600\n    \n  \n\n\n\n\nIn this example, we want to understand trends only for casual users, not registered ones. First, let’s plot casual users over time:\n\n# import datetime\nfig, ax = plt.subplots(1, figsize = (7, 3))\nax.plot(pd.to_datetime(bikeshare['dteday']), bikeshare['casual'])\nax.set(xlabel = \"Day\", ylabel = \"# of casual users\")\nl = plt.tight_layout()\n\n\n\n\nNow, let’s work with a smaller subset of columns:\n\ncols = [\"casual\", \n        \"mnth\", \n        \"weathersit\", \n        \"workingday\",\n        \"yr\",\n        \"temp\", \n        \"hum\", \n        \"windspeed\",\n        \"holiday\"]\n\nbikeshare = bikeshare[cols]\n\nbikeshare = pd.get_dummies(bikeshare, columns = ['mnth'], drop_first = \"if_binary\")\nbikeshare\n\n\n\n\n\n  \n    \n      \n      casual\n      weathersit\n      workingday\n      yr\n      temp\n      hum\n      windspeed\n      holiday\n      mnth_2\n      mnth_3\n      mnth_4\n      mnth_5\n      mnth_6\n      mnth_7\n      mnth_8\n      mnth_9\n      mnth_10\n      mnth_11\n      mnth_12\n    \n  \n  \n    \n      0\n      331\n      2\n      0\n      0\n      0.344167\n      0.805833\n      0.160446\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      131\n      2\n      0\n      0\n      0.363478\n      0.696087\n      0.248539\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      120\n      1\n      1\n      0\n      0.196364\n      0.437273\n      0.248309\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      108\n      1\n      1\n      0\n      0.200000\n      0.590435\n      0.160296\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      82\n      1\n      1\n      0\n      0.226957\n      0.436957\n      0.186900\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      726\n      247\n      2\n      1\n      1\n      0.254167\n      0.652917\n      0.350133\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      727\n      644\n      2\n      1\n      1\n      0.253333\n      0.590000\n      0.155471\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      728\n      159\n      2\n      0\n      1\n      0.253333\n      0.752917\n      0.124383\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      729\n      364\n      1\n      0\n      1\n      0.255833\n      0.483333\n      0.350754\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      730\n      439\n      2\n      1\n      1\n      0.215833\n      0.577500\n      0.154846\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n  \n\n731 rows × 19 columns\n\n\n\nNow that our data is clean, let’s split the data into test data and training data:\n\ntrain, test = train_test_split(bikeshare, test_size = .2, shuffle = False)\n\nX_train = train.drop([\"casual\"], axis = 1)\ny_train = train[\"casual\"]\n\nX_test = test.drop([\"casual\"], axis = 1)\ny_test = test[\"casual\"]\n\nWe can then run linear regression on the bikeshare training data. I chose to run regression using the analytical method, becuase the gradient descent runtime seemed to be much longer.\n\nLR_bike = LinearRegression()\nLR_bike.fit_analytical(X_train, y_train)\n\narray([ -108.37113627,  -791.69054913,   280.58692733,  1498.71511272,\n        -490.10033978, -1242.80038075,  -235.87934918,    -3.35439712,\n         369.27195552,   518.40875345,   537.30188616,   360.80799815,\n         228.88148125,   241.31641202,   371.50385387,   437.60084787,\n         252.43300405,    90.8214605 ,   919.07676215])\n\n\nNow let’s compute the score:\n\nprint(f\"Training score = {LR_bike.score(X_train, y_train).round(4)}\")\n\nTraining score = 0.7318\n\n\nThe training score is fairly high! Now let’s compute the predictions for each day and visualize them in relation to the actual ridership on the test set:\n\nprint(len(y_test)) #147 days will be predicted and then compared\n\n147\n\n\n\ny_pred = LR_bike.predict(X_test) #predicitons on the test data. \n\nfig, ax = plt.subplots(1, figsize = (7, 3))\nax.plot(np.arange(len(y_test)), y_test, label=\"test\") #test data\nax.plot(np.arange(len(y_test)), y_pred, label=\"prediction\") #model prediction data\nax.set(xlabel = \"Day\", ylabel = \"Casual Riders\")\nl = plt.tight_layout()\n\nlegend = plt.legend()\n\n\n\n\nOur predicitons (orange) coorespond pretty well with the test data (blue). Now, we will compare the entries w to the entries in X_train.columns to see which features our model thinks best predict ridership.\n\nLR_bike.w\n\narray([ -108.37113627,  -791.69054913,   280.58692733,  1498.71511272,\n        -490.10033978, -1242.80038075,  -235.87934918,    -3.35439712,\n         369.27195552,   518.40875345,   537.30188616,   360.80799815,\n         228.88148125,   241.31641202,   371.50385387,   437.60084787,\n         252.43300405,    90.8214605 ,   919.07676215])\n\n\n\nX_train.columns\n\nIndex(['weathersit', 'workingday', 'yr', 'temp', 'hum', 'windspeed', 'holiday',\n       'mnth_2', 'mnth_3', 'mnth_4', 'mnth_5', 'mnth_6', 'mnth_7', 'mnth_8',\n       'mnth_9', 'mnth_10', 'mnth_11', 'mnth_12'],\n      dtype='object')\n\n\nThe most negative coeficient is windspeed, and the most positive coeficient is temperature. Thus, windspeed is the factor that most deters causal useres from biking, and temperature is the one that most entices these users to biking. Our model predicts that people want to bike more when it is warm outside, and less when it is windy. Our model also identifies factors such as workday and humidity as having negative relationships with the number of casual riders. Thus, we predict that less people are likely to bike outside of the weekends, and also if humidity is high. On the flip side, our model predicts a positive coorelation between the number of casual riders and the months of March and April, indicating that casual riders might be more likely to bike and get outside once the weather starts becoming nicer in the springtime. Cool!"
  },
  {
    "objectID": "posts/regression_blog/LogisticRegressionBlogPost.html",
    "href": "posts/regression_blog/LogisticRegressionBlogPost.html",
    "title": "Optimization for Logistic Regression Blog",
    "section": "",
    "text": "The following blog post will implement gradient descent for logistic regression in an object-oriented paradigm and implement a key variant of gradient descent called stochastic gradient descent. It will then perform several simple experiments on synthetic data to see which of these algorithms converges most quickly to a satisfactory logistic regression model.\nTo view the source code, pelase follow this link: https://github.com/madgallop/madgallop.github.io/blob/main/posts/regression_blog/logistic.py\n\n\nLinear regression is an approach to modeling the relationship between independent variable/s and a dependent variable. It is the best-fit regression line to describes data with minimal error, as measured by loss. Gradient descent and the algorithm in this blogpost aim to minimize the loss function.\n\n\n\nIn order to implement this algorithm, I built up tools and concepts we discussed in class. I used functions from the lecture on Convex Linear Models and Logistic Regression (predict, sigmoid, loss), and adapted the score method from the perceptron blogpost. The primary challenge of this assignment was implementing the gradient. I first time to spend some time figuring out what the gradient descent function actually is for linear regression. I first tried to implement a partial deriatives approach to find the slope and intercept, but eventaully found the key to be in equation below from the Optimization with Gradient Descent lecture. I finally understood what is happening in gradient descent of logisitic regression, and was able to replicate the equation in code.\n\n\n\nScreen Shot 2023-03-03 at 7.35.26 PM.png\n\n\nThe other challenging part of this assignment was formatting vectors and matrices to have dimensions that allowed for multiplication and other operations. I had to always be asking myself what dimension matrices certain lines of code (like the gradient) were taking in and spitting out, and adjusting the algorithm with lines such as y[:,np.newaxis] and .T to transform matrices.\n\n\n\nBefore beginning, import all the required packages:\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n{'divide': 'ignore', 'over': 'ignore', 'under': 'ignore', 'invalid': 'ignore'}\n\n\nScript to autoload logistic.py file\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\nVisualize the example data\n\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nNow, fit the model\n\nfrom logistic import LogisticRegression \nLR = LogisticRegression()\nLR.fit(X, y, alpha = 1, max_epochs = 1000)\n\n# inspect the fitted value of w\nprint(LR.w)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\nprint(LR.w.shape)\n\nplt.plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\nplt.title(\"Gradient Descent\")\n\n[[ 2.01961227]\n [ 2.46881413]\n [-0.32132074]]\n(3, 1)\n\n\nText(0.5, 1.0, 'Gradient Descent')\n\n\n\n\n\nAs you can see, the model performs fairly well. Let’s take a look at the evolution of the loss over time, as well as the overall accuracy.\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \nplt.title(\"Loss History of Gradient Descent\")\n\nText(0.5, 1.0, 'Loss History of Gradient Descent')\n\n\n\n\n\n\nLR.score(X,y)\n\n0.95\n\n\nGradient descent computes a fairly accurate value of the parameter vector w. Because the algorithm mainly utilized numpy operations instead of loops, it is efficient, and as the loss history graph depicts, starts to converge. Let’s see if we can improve upon this algorithm by altering batch size!\n\n\n\nStochastic gradient descent computes the stochastic gradient by computing the gradient on subsets of the data. The algorithm works as follows: 1. Shuffle the points randomly. 2. Pick the first k random points, compute the stochastic gradient, and then perform an update. 3. Pick the next k random points and repeat.. 4. When we have gone through all n points, reshuffle them all randomly and proceed again.\n\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X, y, \n                  alpha = .05, \n                  batch_size = 100,\n                  m_epochs = 100\n                  )\n# inspect the fitted value of w\nprint(LR2.w)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\nplt.plot(f1, (LR2.w[2] - f1*LR2.w[0])/LR2.w[1], color = \"black\")\nplt.title(\"Stochastic Gradient Descent\")\n\n[[1.38594862]\n [1.58157147]\n [0.35018386]]\n\n\nText(0.5, 1.0, 'Stochastic Gradient Descent')\n\n\n\n\n\nThe model fits the data quite well. Let’s examine the loss history and accuracy again!\n\nnum_steps = len(LR2.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"stochastic gradient\")\nplt.title(\"Loss History Stochastic Gradient Descent\")\n\nText(0.5, 1.0, 'Loss History Stochastic Gradient Descent')\n\n\n\n\n\n\nLR2.score(X,y)\n\n0.935\n\n\nThe loss history chart bounces around more than for typical gradient descent, but gives us a good sense of how the stochastic gradient affects the regression over time. The algorithm is quite accurate as well! As I’ve learned, stochastic gradient descent tends to get to a “pretty good” result faster than standard gradient descent, but can “bounce around” near the good solution. Standard gradient descent might need more epochs to find a good solution, but quickly “settles down” once it finds it.\n\n\n\n\nLR_01 = LogisticRegression()\n\nLR_01.fit(X, y, alpha = .01, max_epochs = 10000)\nnum_steps01 = len(LR_01.loss_history)\n\nLR_1 = LogisticRegression()\n\nLR_1.fit(X, y, alpha = 1, max_epochs = 1000)\nnum_steps1 = len(LR_1.loss_history)\n\nLR_100 = LogisticRegression()\n\nLR_100.fit(X, y, alpha = 100, max_epochs = 100)\nnum_steps100 = len(LR_100.loss_history)\n\n\nfig, axarr = plt.subplots(1,4, layout = \"tight\", figsize = (12,6))\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = \".01\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR_01.w[2] - f1*LR_01.w[0])/LR_01.w[1], color = \"black\")\n\naxarr[1].plot(np.arange(num_steps01) + 1, LR_01.loss_history, label = \"Loss alpha = .01\")\naxarr[1].loglog()\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \".01\")\n\naxarr[2].plot(np.arange(num_steps1) + 1, LR_1.loss_history, label = \"Loss alpha = 1\")\naxarr[2].loglog()\naxarr[2].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"1\")\n\naxarr[3].plot(np.arange(num_steps100) + 1, LR_100.loss_history, label = \"Loss alpha = 100\")\naxarr[3].loglog()\naxarr[3].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"100\")\n\n[Text(0.5, 0, 'Iteration number'),\n Text(0, 0.5, 'Empirical Risk'),\n Text(0.5, 1.0, '100')]\n\n\n\n\n\nThis example illustrates the loss history for different learning rate choices. When the learning rate is small, the algorithm converges relatively slowely. With a larger alpha value (such as 1), the algorithm converges with fewer iterations, and in a more direct manner. And when alpha is very large, say, 100, the algorithm fails to converge, and fails rather fast.\n\n\n\nCreate a new data set with 10 features\n\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nRun stochastic gradient descent, investigating how choice of batch size affects how fast the algorithm converges\n\nLR_ex2 = LogisticRegression()\nLR_ex2.fit_stochastic(X, y, alpha = .01, batch_size = 2, m_epochs = 1000)\nnum_steps2 = len(LR_ex2.loss_history)\n\nLR_ex10 = LogisticRegression()\nLR_ex10.fit_stochastic(X, y, alpha = .01, batch_size = 10, m_epochs = 1000)\nnum_steps10 = len(LR_ex10.loss_history)\n\nLR_ex100 = LogisticRegression()\nLR_ex100.fit_stochastic(X, y, alpha = .01, batch_size = 100, m_epochs = 1000)\nnum_steps100 = len(LR_ex100.loss_history)\n\nLR_ex5000 = LogisticRegression()\nLR_ex5000.fit_stochastic(X, y, alpha = .01, batch_size = 5000, m_epochs = 1000)\nnum_steps5000 = len(LR_ex5000.loss_history)\n\n\nfig, axarr = plt.subplots(1,4, layout = \"tight\", figsize = (15,6))\n\naxarr[0].plot(np.arange(num_steps2) + 1, LR_ex2.loss_history, label = \"2\")\naxarr[0].loglog()\naxarr[0].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"2\")\n\naxarr[1].plot(np.arange(num_steps10) + 1, LR_ex10.loss_history, label = \"Loss alpha = 10\")\naxarr[1].loglog()\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"10\")\n\naxarr[2].plot(np.arange(num_steps100) + 1, LR_ex100.loss_history, label = \"Loss alpha = 100\")\naxarr[2].loglog()\naxarr[2].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"100\")\n\naxarr[3].plot(np.arange(num_steps5000) + 1, LR_ex5000.loss_history, label = \"Loss alpha = 5000\")\naxarr[3].loglog()\naxarr[3].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"5000\")\n\n[Text(0.5, 0, 'Iteration number'),\n Text(0, 0.5, 'Empirical Risk'),\n Text(0.5, 1.0, '5000')]\n\n\n\n\n\nAs the batch size increases, the model works through more training samples before updating its parameters. Thus, really large batch sizes converge faster without as much “bouncing” as compared to smaller batch sizes. While still displaying a downward trend, smaller batch sizes bounce around much more and converge at a slower rate."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/perceptron_blog/perceptronBlogPost.html",
    "href": "posts/perceptron_blog/perceptronBlogPost.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "To view the source code containing the perceptron algorithm itself, pelase follow this link: https://github.com/madgallop/madgallop.github.io/blob/main/posts/perceptron_blog/perceptron.py\n\n\n\nFirst of all, what is the perceptron algorithm?\n\n\n\nScreen Shot 2023-02-27 at 5.05.36 PM.png\n\n\nThe source code containing my implementation of the perceptron algorithm can be found here: https://github.com/madgallop/madgallop.github.io/blob/main/posts/perceptron_blog/perceptron.py\nThis source code contains three primary functions: fit, predict, and score. Fit is the primary method of the perceptron, and is the one that actually performs the update. This function initializes a random initial weight vector and then until convergence, picks a random index, and updates the algorithm. Predict and score compare the labels predicted by the update with the true labels provided, and calculate the algorithm’s accuracy at each time stamp.\nThis blog post exemplifies the perceptron algorithm using three different examples: linearly seperable data in 2 dimensions, non-linearly seperable data in 2 dimensions, and data in 5 dimensions.\nBefore beginning, import all necessary packages\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\n\n\n\nIn this example, the perceptron algorithm converges, indicating that the data set is linearly seperable and can be divided well by a certain line.\n\n\n\nnp.random.seed(12345)\n\nn = 100 \np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Seperable Data in 2D\")\n\n\n\n\n\n\n\n\nfrom perceptron import Perceptron #import class from .py file\n\n\n#create an instance of the class and fit it to data.\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\np.w\nprint(\"Accuracy of the last 10 timestamps of the perceptron:\")\nprint(p.history[-10:]) #just the last few values\n\nAccuracy of the last 10 timestamps of the perceptron:\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy Over Time\")\n\n\n\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Perceptron on Seperable Data\")\n\n\n\n\n\n\n\n\np.score(X, y)\n\n1.0\n\n\n\n\n\n\nIn this example, the perceptron algorithm will not converge or settle on a final value, but will run until the specified maximum number of iterations is reached. In this example, the algorithm will not reach perfect accuracy.\n\n\n\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (-1.7, -1.7)])\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\nxlab2 = plt.xlabel(\"Feature 1\")\nylab2 = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Non-Seperable Data in 2D\")\n\n\n\n\n\n\n\n\nfrom perceptron import Perceptron\n\n#create an instance of the class and fit it to data.\np2 = Perceptron()\np2.fit(X2, y2, max_steps = 1000)\n\np2.w\nprint(\"Accuracy of the last 10 timestamps of the perceptron:\")\nprint(p2.history[-10:]) \n\nAccuracy of the last 10 timestamps of the perceptron:\n[0.58, 0.58, 0.5, 0.5, 0.48, 0.48, 0.5, 0.49, 0.49, 0.47]\n\n\n\n\n\n\n#visualize how the score evolved over time\nfig = plt.plot(p2.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy Over Time\")\n\n\n\n\n\n\n\n\n#visualize the line that the algorithm finds to separate the data\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2)\nfig = draw_line(p2.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Perceptron on Non-Seperable Data\")\n\n\n\n\nHere, the seperating line is far from perfect. The line simply falls on whichever point was selected in the final iteration of the algorithm.\n\n\n\n\nIn this experiment, the algorithm will run on 5 features and then evaluate whether or not the data is linearly separable.\n\np_features3 = 6\n\nX3, y3 = make_blobs(n_samples = 100, n_features = p_features3 - 1, centers = [(-1.7,-1.7), (1.7,1.7)])\n\n\n\n\nfrom perceptron import Perceptron \n\n#create an instance of the class and fit it to data.\np3 = Perceptron()\np3.fit(X3, y3, max_steps = 1000)\n\np3.w\nprint(p3.history[-10:]) #just the last few values\n\n[0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 1.0]\n\n\n\n\n\n\n#visualize how the score evolved over time\nfig = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy Over Time\")\n\n\n\n\nIn this example, the perceptron converges and does so realtively quickly. The data originates from the data in Example 1, which was linearly seperable in 2 dimensions. Given that, the data should still be seperable in 5.\n\n\n\n\nThe runtime complexity of the perceptron algorithm is dependent on the number of features p the data has, but not with the number of points n for a single iteration of the perceptron algorithm."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "This blog post includes a reflection on a recorded lecture by Dr. Timit Gebru, proposes a question for her based on this lecture and background research, and later will reflecton her talk to the wider Middlebury campus.\n\n\n\n\n\n\nApr 19, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis blog post implements least-squares linear regression, and experiments with LASSO regularization for overparameterized problems.\n\n\n\n\n\n\nApr 1, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis blog post is an essay discussing the limits of the quantitative approach to bias and fairness in allocative decision-making.\n\n\n\n\n\n\nMar 27, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis blog post will explore a dataset and construct a model that will predict the species of a pengin based on its measurements with 100% testing accuracy.\n\n\n\n\n\n\nMar 26, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis blog post implements simple gradient descent and stochastic gradient descent and compares their performances for training logistic regression.\n\n\n\n\n\n\nMar 3, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post implementing and experimenting with the Perceptron Algorithm on a variety of data sets\n\n\n\n\n\n\nFeb 27, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Madeleine. This blog documents my learning in CSCI 0451- Machine Learning in Spring 2023."
  },
  {
    "objectID": "posts/gebru_blog/GebruBlogpost.html",
    "href": "posts/gebru_blog/GebruBlogpost.html",
    "title": "Learning from Timnit Gebru",
    "section": "",
    "text": "Next Monday, Dr. Timnit Gebru is coming to Middlebury to visit our class and give a talk to the wider campus community. I am excited to have the opportunity to learn from Dr. Gebru next week, as I found her research, advocacy, persistence, and honesty in sharing her personal experiences to be incredibly inspiring (even reading through her Wiki page made me a little emotional!)\nDr. Gebru is a well-recognized voice in artificial intelligence and its ethical implications. She was born in Ethiopia and attended Stanford, then worked for Google and on projects such using Street View and deep learning to predict social variables of communities based on observations of cars in different places. Dr. Gebru experienced discrimination throughout her early life and professional career, and noticed a lack of Black researchers (and especially Black women) in AI. She founded Black in AI to provide a community for Black researchers in the field.\nIn 2019, Gebru and other researchers called out Amazon’s facial-recognition system for model discrimination against darker-skinned females, and in 2020, when Gebru was the co-lead of its Ethical Artificial Intelligence Team, Google dismissed her for refusing to withdraw an unpublished research paper she co-authored on the dangers of large language models (i.e. bais, costs, deception). The incident became a public controversy, as over 7,000 people signed a letter condemning her firing.\nIn December of 2021, Dr. Gebru launched the Distributed Artificial Intelligence Research Institute to research the effects of AI on marginalized communities."
  },
  {
    "objectID": "posts/gebru_blog/GebruBlogpost.html#introduction",
    "href": "posts/gebru_blog/GebruBlogpost.html#introduction",
    "title": "Learning from Timnit Gebru",
    "section": "Introduction",
    "text": "Introduction\nNext Monday, Dr. Timnit Gebru is coming to Middlebury to visit our class and give a talk to the wider campus community. I am excited to have the opportunity to learn from Dr. Gebru next week, as I found her research, advocacy, persistence, and honesty in sharing her personal experiences to be incredibly inspiring (even reading through her Wiki page made me a little emotional!)\nDr. Gebru is a well-recognized voice in artificial intelligence and its ethical implications. She was born in Ethiopia and attended Stanford, then worked for Google and on projects such using Street View and deep learning to predict social variables of communities based on observations of cars in different places. Dr. Gebru experienced discrimination throughout her early life and professional career, and noticed a lack of Black researchers (and especially Black women) in AI. She founded Black in AI to provide a community for Black researchers in the field.\nIn 2019, Gebru and other researchers called out Amazon’s facial-recognition system for model discrimination against darker-skinned females, and in 2020, when Gebru was the co-lead of its Ethical Artificial Intelligence Team, Google dismissed her for refusing to withdraw an unpublished research paper she co-authored on the dangers of large language models (i.e. bais, costs, deception). The incident became a public controversy, as over 7,000 people signed a letter condemning her firing.\nIn December of 2021, Dr. Gebru launched the Distributed Artificial Intelligence Research Institute to research the effects of AI on marginalized communities."
  },
  {
    "objectID": "posts/gebru_blog/GebruBlogpost.html#dr.-gebrus-2020-talk",
    "href": "posts/gebru_blog/GebruBlogpost.html#dr.-gebrus-2020-talk",
    "title": "Learning from Timnit Gebru",
    "section": "Dr. Gebru’s 2020 Talk",
    "text": "Dr. Gebru’s 2020 Talk\nIn 2020, Dr. Gebru gave a talk as part of a Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision at the conference on Computer Vision and Pattern Recognition.\nIn this talk, I really appreciated Dr. Gebru’s honest description of the lack of diversity and representation in artificial intelligence. I also found it interesting that she feels there have been improvements in diversity in the machine learning field, but that computer vision is not quite there yet. As a woman in STEM fields, I have definitely experienced being one of only a few women in classes or at conferences or other events. Even in my Conservation Planning class this semester (which is a technical geography course based in Python), I am one of two women in the class. While I have experienced sexism, I have obviously never experienced intersectional discrimination, and value Dr. Gebru’s candor in describing her experiences.\nAfter speaking about her own experiences, Dr. Gebru discusses what computer vision is, and how it has the potential to disproportionately harm marginalized communities. She goes into the racist way that computer vision screens potential employees through facial recognition. She urges us to consider questions that we have engaged with a little bit in this course already, such as who produces these technologies, and what groups they serve/leave out. She brings up an important quote from Mimi Onuoha, which describes how “Every data set that involves people implies subjects and objects… it is imperative to remember that on both sides we have human beings.” She discusses how in ML and a variety of other fields, we do not see a diversity in datasets. But she notes that even in cases where we do, they do little good if we do not think critically about how we acquire them.\nI think the main takeaways I have and one of the most important points that Dr. Gebru discusses is that making something fair is not equivalent to making it the same for everyone. Fairness is not about math or statistics, but more about society and social structure. Even tools that are inclusive of multiple communities can still do harm, and the tools that we rely on, such as in computer vision, disproportionately harm marginalized communities on a national level."
  },
  {
    "objectID": "posts/gebru_blog/GebruBlogpost.html#question",
    "href": "posts/gebru_blog/GebruBlogpost.html#question",
    "title": "Learning from Timnit Gebru",
    "section": "Question",
    "text": "Question\nYou spoke at the end of your talk on fairness in computer vision at the conference on Computer Vision and Pattern Recognition about technology that harms and marginalizes various communities, as well as examples of refusal to combat these strategies. Since you gave this talk, have you seen improvements in either diversity in the field of computer vision or further examples of resistance to harmful technologies in the AI world?"
  }
]