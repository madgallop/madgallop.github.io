[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/perceptron_blog/perceptronBlogPost.html",
    "href": "posts/perceptron_blog/perceptronBlogPost.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "To view the source code containing the perceptron algorithm itself, pelase follow this link: https://github.com/madgallop/madgallop.github.io/blob/main/posts/perceptron_blog/perceptron.py\n\n\n\nFirst of all, what is the perceptron algorithm?\n\n\n\nScreen Shot 2023-02-27 at 5.05.36 PM.png\n\n\nThe source code containing my implementation of the perceptron algorithm can be found here: https://github.com/madgallop/madgallop.github.io/blob/main/posts/perceptron_blog/perceptron.py\nThis source code contains three primary functions: fit, predict, and score. Fit is the primary method of the perceptron, and is the one that actually performs the update. This function initializes a random initial weight vector and then until convergence, picks a random index, and updates the algorithm. Predict and score compare the labels predicted by the update with the true labels provided, and calculate the algorithm’s accuracy at each time stamp.\nThis blog post exemplifies the perceptron algorithm using three different examples: linearly seperable data in 2 dimensions, non-linearly seperable data in 2 dimensions, and data in 5 dimensions.\nBefore beginning, import all necessary packages\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\n\n\n\nIn this example, the perceptron algorithm converges, indicating that the data set is linearly seperable and can be divided well by a certain line.\n\n\n\nnp.random.seed(12345)\n\nn = 100 \np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Seperable Data in 2D\")\n\n\n\n\n\n\n\n\nfrom perceptron import Perceptron #import class from .py file\n\n\n#create an instance of the class and fit it to data.\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\np.w\nprint(\"Accuracy of the last 10 timestamps of the perceptron:\")\nprint(p.history[-10:]) #just the last few values\n\nAccuracy of the last 10 timestamps of the perceptron:\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy Over Time\")\n\n\n\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Perceptron on Seperable Data\")\n\n\n\n\n\n\n\n\np.score(X, y)\n\n1.0\n\n\n\n\n\n\nIn this example, the perceptron algorithm will not converge or settle on a final value, but will run until the specified maximum number of iterations is reached. In this example, the algorithm will not reach perfect accuracy.\n\n\n\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (-1.7, -1.7)])\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\nxlab2 = plt.xlabel(\"Feature 1\")\nylab2 = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Non-Seperable Data in 2D\")\n\n\n\n\n\n\n\n\nfrom perceptron import Perceptron\n\n#create an instance of the class and fit it to data.\np2 = Perceptron()\np2.fit(X2, y2, max_steps = 1000)\n\np2.w\nprint(\"Accuracy of the last 10 timestamps of the perceptron:\")\nprint(p2.history[-10:]) \n\nAccuracy of the last 10 timestamps of the perceptron:\n[0.58, 0.58, 0.5, 0.5, 0.48, 0.48, 0.5, 0.49, 0.49, 0.47]\n\n\n\n\n\n\n#visualize how the score evolved over time\nfig = plt.plot(p2.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy Over Time\")\n\n\n\n\n\n\n\n\n#visualize the line that the algorithm finds to separate the data\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2)\nfig = draw_line(p2.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Perceptron on Non-Seperable Data\")\n\n\n\n\nHere, the seperating line is far from perfect. The line simply falls on whichever point was selected in the final iteration of the algorithm.\n\n\n\n\nIn this experiment, the algorithm will run on 5 features and then evaluate whether or not the data is linearly separable.\n\np_features3 = 6\n\nX3, y3 = make_blobs(n_samples = 100, n_features = p_features3 - 1, centers = [(-1.7,-1.7), (1.7,1.7)])\n\n\n\n\nfrom perceptron import Perceptron \n\n#create an instance of the class and fit it to data.\np3 = Perceptron()\np3.fit(X3, y3, max_steps = 1000)\n\np3.w\nprint(p3.history[-10:]) #just the last few values\n\n[0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 1.0]\n\n\n\n\n\n\n#visualize how the score evolved over time\nfig = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy Over Time\")\n\n\n\n\nIn this example, the perceptron converges and does so realtively quickly. The data originates from the data in Example 1, which was linearly seperable in 2 dimensions. Given that, the data should still be seperable in 5.\n\n\n\n\nThe runtime complexity of the perceptron algorithm is dependent on the number of features p the data has, but not with the number of points n for a single iteration of the perceptron algorithm."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "This blog post is an essay discussing the limits of the quantitative approach to bias and fairness in allocative decision-making.\n\n\n\n\n\n\nMar 27, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis blog post will explore a dataset and construct a model that will predict the species of a pengin based on its measurements with 100% testing accuracy.\n\n\n\n\n\n\nMar 26, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis blog post implements simple gradient descent and stochastic gradient descent and compares their performances for training logistic regression.\n\n\n\n\n\n\nMar 3, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post implementing and experimenting with the Perceptron Algorithm on a variety of data sets\n\n\n\n\n\n\nFeb 27, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Madeleine. This blog documents my learning in CSCI 0451- Machine Learning in Spring 2023."
  },
  {
    "objectID": "posts/regression_blog/LogisticRegressionBlogPost.html",
    "href": "posts/regression_blog/LogisticRegressionBlogPost.html",
    "title": "Optimization for Logistic Regression Blog",
    "section": "",
    "text": "The following blog post will implement gradient descent for logistic regression in an object-oriented paradigm and implement a key variant of gradient descent called stochastic gradient descent. It will then perform several simple experiments on synthetic data to see which of these algorithms converges most quickly to a satisfactory logistic regression model.\nTo view the source code, pelase follow this link: https://github.com/madgallop/madgallop.github.io/blob/main/posts/regression_blog/logistic.py\n\n\nLinear regression is an approach to modeling the relationship between independent variable/s and a dependent variable. It is the best-fit regression line to describes data with minimal error, as measured by loss. Gradient descent and the algorithm in this blogpost aim to minimize the loss function.\n\n\n\nIn order to implement this algorithm, I built up tools and concepts we discussed in class. I used functions from the lecture on Convex Linear Models and Logistic Regression (predict, sigmoid, loss), and adapted the score method from the perceptron blogpost. The primary challenge of this assignment was implementing the gradient. I first time to spend some time figuring out what the gradient descent function actually is for linear regression. I first tried to implement a partial deriatives approach to find the slope and intercept, but eventaully found the key to be in equation below from the Optimization with Gradient Descent lecture. I finally understood what is happening in gradient descent of logisitic regression, and was able to replicate the equation in code.\n\n\n\nScreen Shot 2023-03-03 at 7.35.26 PM.png\n\n\nThe other challenging part of this assignment was formatting vectors and matrices to have dimensions that allowed for multiplication and other operations. I had to always be asking myself what dimension matrices certain lines of code (like the gradient) were taking in and spitting out, and adjusting the algorithm with lines such as y[:,np.newaxis] and .T to transform matrices.\n\n\n\nBefore beginning, import all the required packages:\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n{'divide': 'ignore', 'over': 'ignore', 'under': 'ignore', 'invalid': 'ignore'}\n\n\nScript to autoload logistic.py file\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\nVisualize the example data\n\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nNow, fit the model\n\nfrom logistic import LogisticRegression \nLR = LogisticRegression()\nLR.fit(X, y, alpha = 1, max_epochs = 1000)\n\n# inspect the fitted value of w\nprint(LR.w)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\nprint(LR.w.shape)\n\nplt.plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\nplt.title(\"Gradient Descent\")\n\n[[ 2.01961227]\n [ 2.46881413]\n [-0.32132074]]\n(3, 1)\n\n\nText(0.5, 1.0, 'Gradient Descent')\n\n\n\n\n\nAs you can see, the model performs fairly well. Let’s take a look at the evolution of the loss over time, as well as the overall accuracy.\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \nplt.title(\"Loss History of Gradient Descent\")\n\nText(0.5, 1.0, 'Loss History of Gradient Descent')\n\n\n\n\n\n\nLR.score(X,y)\n\n0.95\n\n\nGradient descent computes a fairly accurate value of the parameter vector w. Because the algorithm mainly utilized numpy operations instead of loops, it is efficient, and as the loss history graph depicts, starts to converge. Let’s see if we can improve upon this algorithm by altering batch size!\n\n\n\nStochastic gradient descent computes the stochastic gradient by computing the gradient on subsets of the data. The algorithm works as follows: 1. Shuffle the points randomly. 2. Pick the first k random points, compute the stochastic gradient, and then perform an update. 3. Pick the next k random points and repeat.. 4. When we have gone through all n points, reshuffle them all randomly and proceed again.\n\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X, y, \n                  alpha = .05, \n                  batch_size = 100,\n                  m_epochs = 100\n                  )\n# inspect the fitted value of w\nprint(LR2.w)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\nplt.plot(f1, (LR2.w[2] - f1*LR2.w[0])/LR2.w[1], color = \"black\")\nplt.title(\"Stochastic Gradient Descent\")\n\n[[1.38594862]\n [1.58157147]\n [0.35018386]]\n\n\nText(0.5, 1.0, 'Stochastic Gradient Descent')\n\n\n\n\n\nThe model fits the data quite well. Let’s examine the loss history and accuracy again!\n\nnum_steps = len(LR2.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"stochastic gradient\")\nplt.title(\"Loss History Stochastic Gradient Descent\")\n\nText(0.5, 1.0, 'Loss History Stochastic Gradient Descent')\n\n\n\n\n\n\nLR2.score(X,y)\n\n0.935\n\n\nThe loss history chart bounces around more than for typical gradient descent, but gives us a good sense of how the stochastic gradient affects the regression over time. The algorithm is quite accurate as well! As I’ve learned, stochastic gradient descent tends to get to a “pretty good” result faster than standard gradient descent, but can “bounce around” near the good solution. Standard gradient descent might need more epochs to find a good solution, but quickly “settles down” once it finds it.\n\n\n\n\nLR_01 = LogisticRegression()\n\nLR_01.fit(X, y, alpha = .01, max_epochs = 10000)\nnum_steps01 = len(LR_01.loss_history)\n\nLR_1 = LogisticRegression()\n\nLR_1.fit(X, y, alpha = 1, max_epochs = 1000)\nnum_steps1 = len(LR_1.loss_history)\n\nLR_100 = LogisticRegression()\n\nLR_100.fit(X, y, alpha = 100, max_epochs = 100)\nnum_steps100 = len(LR_100.loss_history)\n\n\nfig, axarr = plt.subplots(1,4, layout = \"tight\", figsize = (12,6))\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = \".01\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR_01.w[2] - f1*LR_01.w[0])/LR_01.w[1], color = \"black\")\n\naxarr[1].plot(np.arange(num_steps01) + 1, LR_01.loss_history, label = \"Loss alpha = .01\")\naxarr[1].loglog()\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \".01\")\n\naxarr[2].plot(np.arange(num_steps1) + 1, LR_1.loss_history, label = \"Loss alpha = 1\")\naxarr[2].loglog()\naxarr[2].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"1\")\n\naxarr[3].plot(np.arange(num_steps100) + 1, LR_100.loss_history, label = \"Loss alpha = 100\")\naxarr[3].loglog()\naxarr[3].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"100\")\n\n[Text(0.5, 0, 'Iteration number'),\n Text(0, 0.5, 'Empirical Risk'),\n Text(0.5, 1.0, '100')]\n\n\n\n\n\nThis example illustrates the loss history for different learning rate choices. When the learning rate is small, the algorithm converges relatively slowely. With a larger alpha value (such as 1), the algorithm converges with fewer iterations, and in a more direct manner. And when alpha is very large, say, 100, the algorithm fails to converge, and fails rather fast.\n\n\n\nCreate a new data set with 10 features\n\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nRun stochastic gradient descent, investigating how choice of batch size affects how fast the algorithm converges\n\nLR_ex2 = LogisticRegression()\nLR_ex2.fit_stochastic(X, y, alpha = .01, batch_size = 2, m_epochs = 1000)\nnum_steps2 = len(LR_ex2.loss_history)\n\nLR_ex10 = LogisticRegression()\nLR_ex10.fit_stochastic(X, y, alpha = .01, batch_size = 10, m_epochs = 1000)\nnum_steps10 = len(LR_ex10.loss_history)\n\nLR_ex100 = LogisticRegression()\nLR_ex100.fit_stochastic(X, y, alpha = .01, batch_size = 100, m_epochs = 1000)\nnum_steps100 = len(LR_ex100.loss_history)\n\nLR_ex5000 = LogisticRegression()\nLR_ex5000.fit_stochastic(X, y, alpha = .01, batch_size = 5000, m_epochs = 1000)\nnum_steps5000 = len(LR_ex5000.loss_history)\n\n\nfig, axarr = plt.subplots(1,4, layout = \"tight\", figsize = (15,6))\n\naxarr[0].plot(np.arange(num_steps2) + 1, LR_ex2.loss_history, label = \"2\")\naxarr[0].loglog()\naxarr[0].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"2\")\n\naxarr[1].plot(np.arange(num_steps10) + 1, LR_ex10.loss_history, label = \"Loss alpha = 10\")\naxarr[1].loglog()\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"10\")\n\naxarr[2].plot(np.arange(num_steps100) + 1, LR_ex100.loss_history, label = \"Loss alpha = 100\")\naxarr[2].loglog()\naxarr[2].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"100\")\n\naxarr[3].plot(np.arange(num_steps5000) + 1, LR_ex5000.loss_history, label = \"Loss alpha = 5000\")\naxarr[3].loglog()\naxarr[3].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"5000\")\n\n[Text(0.5, 0, 'Iteration number'),\n Text(0, 0.5, 'Empirical Risk'),\n Text(0.5, 1.0, '5000')]\n\n\n\n\n\nAs the batch size increases, the model works through more training samples before updating its parameters. Thus, really large batch sizes converge faster without as much “bouncing” as compared to smaller batch sizes. While still displaying a downward trend, smaller batch sizes bounce around much more and converge at a slower rate."
  },
  {
    "objectID": "posts/penguins_blog/penguins_blog.html",
    "href": "posts/penguins_blog/penguins_blog.html",
    "title": "Classifying Palmer Penguins Blog",
    "section": "",
    "text": "Chinstrap!\n\nGentoo!\n\nAdelie!\n\nTo view the source code for this project, please visit this link: https://github.com/madgallop/madgallop.github.io/blob/main/posts/penguins_blog/penguins_blog.ipynb\n\n\nImport required packages\n\n#%%background OldLace\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom matplotlib.patches import Patch\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\n\n#%%background OldLace\nfrom IPython.core.magic import register_cell_magic\n\n@register_cell_magic\ndef background(color, cell):\n    set_background(color)\n    return eval(cell)\n\nImport dataset\n\n#%%background OldLace\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n#train.head()\n\n\n\n\nTo begin, I will explore the penguins dataset by constructing a figure and a table.\nI will first create a dataframe so that I can better group data and create more informative charts\n\n#%%background OldLace\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    #df = pd.get_dummies(df) #comment in for data analysis, out for table construction\n    return df, y\n\nX_train, y_train = prepare_data(train)\n\nBefore making some tables and figures, I am curious to run some basic statistics on the data\n\n#%%background OldLace\nspecies3pct = (y_train==2).mean()\nspecies2pct = (y_train==1).mean()\nspecies1pct = (y_train==0).mean() \nspecies3cnt = (y_train==2).sum()\nspecies2cnt = (y_train==1).sum()\nspecies1cnt = (y_train==0).sum()\n\nprint(\"There are approximately\",species3cnt,\"penguins in Species 3, or\",species3pct.round(2)*100,\"% of the penguins\")\nprint(\"There are approximately\",species2cnt,\"penguins in Species 2, or\",species2pct.round(2)*100,\"% of the penguins\")\nprint(\"There are approximately\",species1cnt,\"penguins in Species 1, or\",species1pct.round(2)*100,\"% of the penguins\")\n\nThere are approximately 95 penguins in Species 3, or 37.0 % of the penguins\nThere are approximately 55 penguins in Species 2, or 21.0 % of the penguins\nThere are approximately 106 penguins in Species 1, or 41.0 % of the penguins\n\n\nThese numbers are the base rates for our model. The base rate is the accuracy rate of a trivial model that doesn’t use the features. It is the accuracy I would have if I made predictions without looking at the data.\nNext, I am curious in the following:\n\nWhere do the penguins live?\nDoes their flipper length/culmen length vary by sex or island?\nDoes weight vary by sex or island?\n\n\n#%%background OldLace\nX_train.groupby([\"Island\",\"Sex\"])[['Body Mass (g)','Culmen Length (mm)','Flipper Length (mm)']].aggregate([np.mean]).round(2)\n\n\n\n\n\n\n\n\n  \n    \n      \n      \n      Body Mass (g)\n      Culmen Length (mm)\n      Flipper Length (mm)\n    \n    \n      \n      \n      mean\n      mean\n      mean\n    \n    \n      Island\n      Sex\n      \n      \n      \n    \n  \n  \n    \n      Biscoe\n      FEMALE\n      4253.28\n      42.94\n      204.67\n    \n    \n      MALE\n      5168.12\n      47.54\n      214.49\n    \n    \n      Dream\n      FEMALE\n      3435.20\n      42.45\n      190.08\n    \n    \n      MALE\n      3973.30\n      46.64\n      196.73\n    \n    \n      Torgersen\n      FEMALE\n      3371.88\n      37.46\n      189.50\n    \n    \n      MALE\n      4016.18\n      40.88\n      195.65\n    \n  \n\n\n\n\n\n#%%background OldLace\nX_train.groupby([\"Island\"])[['Body Mass (g)','Culmen Length (mm)','Flipper Length (mm)']].aggregate([np.mean]).round(2)\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n      Culmen Length (mm)\n      Flipper Length (mm)\n    \n    \n      \n      mean\n      mean\n      mean\n    \n    \n      Island\n      \n      \n      \n    \n  \n  \n    \n      Biscoe\n      4738.85\n      45.38\n      209.88\n    \n    \n      Dream\n      3689.78\n      44.43\n      193.23\n    \n    \n      Torgersen\n      3703.79\n      39.22\n      192.67\n    \n  \n\n\n\n\n\n#%%background OldLace\nX_train.groupby([\"Island\", \"Sex\"]).size().reset_index()\n\n\n\n\n\n  \n    \n      \n      Island\n      Sex\n      0\n    \n  \n  \n    \n      0\n      Biscoe\n      FEMALE\n      61\n    \n    \n      1\n      Biscoe\n      MALE\n      69\n    \n    \n      2\n      Dream\n      FEMALE\n      49\n    \n    \n      3\n      Dream\n      MALE\n      44\n    \n    \n      4\n      Torgersen\n      FEMALE\n      16\n    \n    \n      5\n      Torgersen\n      MALE\n      17\n    \n  \n\n\n\n\n\n#%%background OldLace\nX_train.groupby([\"Island\"]).size().reset_index()\n\n\n\n\n\n  \n    \n      \n      Island\n      0\n    \n  \n  \n    \n      0\n      Biscoe\n      130\n    \n    \n      1\n      Dream\n      93\n    \n    \n      2\n      Torgersen\n      33\n    \n  \n\n\n\n\n\n#%%background OldLace\nX_train.groupby([\"Sex\"])[['Body Mass (g)']].aggregate([len]).round(2)\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n    \n    \n      \n      len\n    \n    \n      Sex\n      \n    \n  \n  \n    \n      FEMALE\n      126\n    \n    \n      MALE\n      130\n    \n  \n\n\n\n\nI learned: - females tend to weigh less than males - females generally have shorter beaks - females generally have and shorter flippers. - there are slightly more males in the study - there are the most penguins on Biscoe island, then Dream, then Torgersen - there are similar amounts of penguins of different sexes on the different islands - Bisco island is home to penguins with heavier bodies, and longer beaks and flippers.\nNow I want to create a chart to better understand weight distributions of the penguins by island.\n\n#%%background OldLace\nsns.set_theme(style=\"whitegrid\", palette={\"Gold\",\"HotPink\",\"CornflowerBlue\"})\n\n# Draw a categorical scatterplot to show each observation\nax = sns.swarmplot(data=X_train, x='Body Mass (g)', y=\"Sex\", hue=\"Island\")\nax.set(ylabel=\"\")\nax.set(title = \"Body Mass by Island\")\n\n[Text(0.5, 1.0, 'Body Mass by Island')]\n\n\n\n\n\n\n#%%background OldLace\nsns.set_theme(style=\"whitegrid\", palette={\"Gold\",\"HotPink\",\"CornflowerBlue\"})\n\n# Draw a categorical scatterplot to show each observation\nax = sns.swarmplot(data=X_train, x='Flipper Length (mm)', y=\"Sex\", hue=\"Island\")\nax.set(ylabel=\"\")\nax.set(title = \"Flipper Length by Island\")\n\n[Text(0.5, 1.0, 'Flipper Length by Island')]\n\n\n\n\n\n\n#%%background OldLace\nsns.set_theme(style=\"whitegrid\", palette={\"Gold\",\"HotPink\",\"CornflowerBlue\"})\n\n# Draw a categorical scatterplot to show each observation\nax = sns.swarmplot(data=X_train, x='Culmen Length (mm)', y=\"Sex\", hue=\"Island\")\nax.set(ylabel=\"\")\nax.set(title = \"Culmen Length by Island\")\n\n[Text(0.5, 1.0, 'Culmen Length by Island')]\n\n\n\n\n\nWhile Biscoe island defintly hosts penguins with more mass and longer flippers, the same is not necessarily true for beak (Culmen) length. The male/female distinction does not appear to make a difference island by island.\n\n\n\nFirst, modify the dataframe using pd.get_dummies to encode nominal data\n\n#%%background OldLace\ndef prepare_data2(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df) #comment in for data analysis, out for table construction\n    return df, y\n\nX_train2, y_train2 = prepare_data2(train)\n\nOur first task is to find three features of the data and a model trained on those features which achieves 100% testing accuracy - One feature must be qualitative (like Island or Clutch Completion). - The other two features must be quantitative (like Body Mass (g) or Culmen Depth (mm)).\nFirst, lets figure out what features are best.\n\n\n\nAfter some experimenting, I decided to select the optimal features from a model.\n\n#%%background OldLace\nridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(X_train2, y_train2) #Ridge regression with built-in cross-validation.\nimportance = np.abs(ridge.coef_) # assign importance to each feature through a specific attribute\n#The features with the highest absolute coef_ value are considered the most important\n\n#create a plot to visualize the importance of the features\nfeature_names = np.array(X_train2.columns)\nplt.barh(feature_names, importance)\nplt.title(\"Feature importances via coefficients\")\nplt.show()\n\n#select a certain number of features\nthreshold = np.sort(importance)[-5]\n\nsfm = SelectFromModel(ridge, threshold=threshold).fit(X_train2, y_train2)\n#only select the first three features from the list\nselected_features = feature_names[sfm.get_support()][0:3]\n\nprint(selected_features)\nprint(f\"Features selected by SelectFromModel: {selected_features}\")\n\n\n\n\n['Culmen Depth (mm)' 'Delta 13 C (o/oo)' 'Island_Torgersen']\nFeatures selected by SelectFromModel: ['Culmen Depth (mm)' 'Delta 13 C (o/oo)' 'Island_Torgersen']\n\n\nUsing the select by model method, Culmen Depth, Delta 13 C and Island_Torgersen are the most important for correct classification of penguins. However, both Sex_Female and Sex_Male rank higher than both Island_Dream and Island_Biscoe. So I experimented with using sex instead of island.\n\n\n\nI will first explore how well linear regression works for this data, and then experiment with DecisionTreeClassifier and RandomForestClassifier.\n\n\n\n#%%background OldLace\n#change selected features to invlude Island_Dream and Island_Biscoe as well \nfeatures_to_test = X_train2[selected_features].join(X_train2[\"Island_Dream\"]).join(X_train2[\"Island_Biscoe\"])\nLR = LogisticRegression(random_state = 10, max_iter = 1000)\nLR.fit(features_to_test, y_train2)\n\nLR.score(features_to_test, y_train2)\n\n0.93359375\n\n\nLogistic regression is 93% accurate on the training data.\n\n\n\n\nNow, we will evaluate the model’s efficacy on the test data\n\n#%%background OldLace\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ndef prepare_data3(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df) #comment in for data analysis, out for table construction\n    return df, y\n\nX_test, y_test = prepare_data3(test)\n# print(X_test)\n# print(y_test)\n\nfeatures_to_test2 = X_test[selected_features].join(X_test[\"Island_Dream\"]).join(X_test[\"Island_Biscoe\"])\n\nLR.fit(features_to_test2, y_test)\nLR.score(features_to_test2, y_test)\n\n0.9411764705882353\n\n\nThe model is slightly more effective on the test data, at 94%. In the two cells below, I experiment with different parameters to allow for this particular model to reach 100% accuracy on the test data.\n\n#%%background OldLace\n#change selected features to invlude Island_Dream and Island_Biscoe as well \nfeatures_to_test_extra = ['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Torgersen', \"Island_Dream\", \"Island_Biscoe\"]\nLR_extra = LogisticRegression(random_state = 10, max_iter = 1000)\nLR_extra.fit(X_train2[features_to_test_extra], y_train2)\n\nLR_extra.score(X_train2[features_to_test_extra], y_train2)\n\n0.99609375\n\n\n\n#%%background OldLace\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ndef prepare_data3(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df) #comment in for data analysis, out for table construction\n    return df, y\n\nX_test, y_test = prepare_data3(test)\n# print(X_test)\n# print(y_test)\n\n# features_to_test2 = X_test[selected_features].join(X_test[\"Island_Dream\"]).join(X_test[\"Island_Biscoe\"])\nfeatures_to_test2_extra = ['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Torgersen', \"Island_Dream\", \"Island_Biscoe\"]\n\nLR_extra.fit(X_test[features_to_test2_extra], y_test)\nLR_extra.score(X_test[features_to_test2_extra], y_test)\n\n1.0\n\n\nWhen I changed my selected features to Culmen Depth (mm), Culment Length (mm), Island_Torgersen, Island_Dream, Island_Biscoe, the model achieved 100% accuracy on the test data. I will not plot this data, but it is nice that some selected features allow for this model to reach 100% accuracy, just not the features chosen by the “Select by Model” classification system.\n\n\n\nnow we will plot the decision regions, seperated by qualitative feature (Island: Torgersen, Dream, Biscoe)\n\n#%%background OldLace\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n\n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (15, 3)) #maybe put 1 there\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n        XY = pd.DataFrame({\n            X.columns[0] : XX,\n            X.columns[1] : YY\n        })\n\n        for j in qual_features:\n            XY[j] = 0\n\n        XY[qual_features[i]] = 1\n\n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n        axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n        ix = X[qual_features[i]] == 1\n      # plot the data\n        axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n        axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n        patches = []\n        for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n            patches.append(Patch(color = color, label = spec))\n\n        plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n        plt.tight_layout()\n\n\n#%%background OldLace\nplot_regions(LR, features_to_test, y_train2)\n\n\n\n\nNext, we will experiment with a couple other classifiers.\n\n\n\n#%%background OldLace\ndtc = DecisionTreeClassifier(max_depth=5)\n\ncross_val_score(dtc, features_to_test, y_train2, cv=5)\n\narray([0.90384615, 0.92156863, 0.84313725, 0.90196078, 0.90196078])\n\n\n\n#%%background OldLace\ndtc.fit(features_to_test, y_train2, sample_weight=None, check_input=True)\n\ndtc.score(features_to_test, y_train2, sample_weight=None)\n\n0.96875\n\n\nThis model is more effective than regression on the training data.\n\n\n\n#%%background OldLace\ndtc.fit(features_to_test2, y_test, sample_weight=None, check_input=True)\n\ndtc.score(features_to_test2, y_test)\n\n0.9852941176470589\n\n\nIt is also more effective than regression on the test data.\n\n\n\n\n#%%background OldLace\nplot_regions(dtc, features_to_test, y_train2)\n\n\n\n\nThis model uses more advanced geometries to separate the data\n\n\n\n\nLet’s look at one more type of classifier!\n\n#%%background OldLace\nrfc = RandomForestClassifier(max_depth=7, random_state=1)\nrfc.fit(features_to_test, y_train2)\n\nrfc.apply(features_to_test)\nrfc.score(features_to_test, y_train2, sample_weight=None)\n\n0.99609375\n\n\nThis model is incredibly effective for the chosen parameters!\n\n\n\n#%%background OldLace\nrfc.fit(features_to_test2, y_test)\n\nrfc.score(features_to_test2, y_test)\n\n1.0\n\n\nThis model achieves 100% testing accuracy, making it the best choice for the features chosen by the select from model classificiation.\n\n\n\n\n#%%background OldLace\nplot_regions(rfc, features_to_test, y_train2)\n\n\n\n\nThis model also uses complex geometries, but is the superior choice in classifying the penguins for the selected features."
  },
  {
    "objectID": "posts/quat_bias_blog/quantBiasBlog.html",
    "href": "posts/quat_bias_blog/quantBiasBlog.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness Blog",
    "section": "",
    "text": "References\n\nAhmad, Afra Saeed, Isaac Sabat, Rachel Trump-Steele, and Eden King. 2019. “Evidence-Based Strategies for Improving Diversity and Inclusion in Undergraduate Research Labs.” Frontiers in Psychology 10: 1305.\n\n\nArellano, Lucy. 2022. “Questioning the Science: How Quantitative Methodologies Perpetuate Inequity in Higher Education.” Education Sciences 12 (2): 116.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” In Conference on Fairness, Accountability and Transparency, 77–91. PMLR.\n\n\nGillborn, David, Paul Warmington, and Sean Demack. 2018. “QuantCrit: Education, Policy,‘big Data’and Principles for a Critical Race Theory of Statistics.” Race Ethnicity and Education 21 (2): 158–79.\n\n\nNarayanan, Arvind. 2022. “The Limits of the Quantitative Approach to Discrimination.” Speech.\n\n\nOakley, Ann. 1998. “Gender, Methodology and People’s Ways of Knowing: Some Problems with Feminism and the Paradigm Debate in Social Science.” Sociology 32 (4): 707–31."
  },
  {
    "objectID": "posts/quat_bias_blog/quantBiasBlog.html#introduction",
    "href": "posts/quat_bias_blog/quantBiasBlog.html#introduction",
    "title": "Limits of the Quantitative Approach to Bias and Fairness Blog",
    "section": "Introduction",
    "text": "Introduction\n In his 2022 speech at Princeton University, professor of Computer Science Narayanan asserts that: “currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (Narayanan (2022)).\n At the heart of Naryanan’s position is his belief “that baked into the practice of quantitative methods is a worldview that sees the status quo as unproblematic” (2022, 3). He explains that qualitative methods tend to be overly simplistic (Ibid, 4) and subjective (Ibid, 16) and describes several ways in which qualitative research is limited in its ability to study discrimination (Ibid, 3). The following paper will outline Narayanan’s argument and contextualize quantitative research within a larger body of literature. Ultimately, it will provide an alternative framework to consider quantitative research, and suggest more equitable approaches to its methodology."
  },
  {
    "objectID": "posts/quant_bias_blog/quantBiasBlog.html",
    "href": "posts/quant_bias_blog/quantBiasBlog.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness Blog",
    "section": "",
    "text": "References\n\nAhmad, Afra Saeed, Isaac Sabat, Rachel Trump-Steele, and Eden King. 2019. “Evidence-Based Strategies for Improving Diversity and Inclusion in Undergraduate Research Labs.” Frontiers in Psychology 10: 1305.\n\n\nArellano, Lucy. 2022. “Questioning the Science: How Quantitative Methodologies Perpetuate Inequity in Higher Education.” Education Sciences 12 (2): 116.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” In Conference on Fairness, Accountability and Transparency, 77–91. PMLR.\n\n\nGillborn, David, Paul Warmington, and Sean Demack. 2018. “QuantCrit: Education, Policy,‘big Data’and Principles for a Critical Race Theory of Statistics.” Race Ethnicity and Education 21 (2): 158–79.\n\n\nNarayanan, Arvind. 2022. “The Limits of the Quantitative Approach to Discrimination.” Speech.\n\n\nOakley, Ann. 1998. “Gender, Methodology and People’s Ways of Knowing: Some Problems with Feminism and the Paradigm Debate in Social Science.” Sociology 32 (4): 707–31."
  }
]