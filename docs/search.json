[
  {
    "objectID": "posts/GEOG0324/gis_science_blog/2023-09-14-gis-science.html",
    "href": "posts/GEOG0324/gis_science_blog/2023-09-14-gis-science.html",
    "title": "Is GIS a Science?",
    "section": "",
    "text": "In their engaging 1997 article titled GIS: Tool or science? Demystifying the persistent ambiguity of GIS as ‘tool’ versus ‘science’, Wright, Goodchild, and Proctor delve into the multifaceted nature of Geographic Information Systems (GIS). The authors argue that there are three distinct discourses surrounding GIS and its applications:\n\nGIS is a tool\nGIS is toolmaking\nGIS is science\n\nAfter reading about these three perspectives and reflecting on my own personal journey with GIS, I find myself increasingly drawn to the notion of GIS as a science in its own right.\nI first encountered GIS as a student in GEOG120. This course introduced me to spatial problems and the vast array of tools available in the Processing Toolbox which I could employ to solve them. Before I became accustomed to spatial analysis and methodologies, I was more familiar with using tools like Python for “data problems” similar to the ones I encountered in GIS. Consequently, I regarded GIS as simply another technical tool I could turn to.\nHowever, as I began spending more time using GIS for research purposes and even for problems in my computer science classes, my perspective shifted. Now, I recognize the unique capabilities GIS offers for designing, understanding and solving spatial problems.\nI find the concept of GIS as a process to be particularly interesting. I could consider the very act of devising workflows of refining the tools GIS bundles together to be a scientific endeavor in itself. In alignment with Wright et al.’s perspective, I agree that GIS facilitates the scientific method. It is a software suited to help us form and test hypotheses, describe, explain, predict, and ultimately gain a deeper understanding of the world around us. As the authors describe it, GIS is “an environment as well as a method used to discover, explore, and test spatial theory”.\nThrough my journey with GIS, I have had the chance to both pose spatial questions and to actively work towards comprehending and addressing them. To me, this embodies the essence of science—a continuous process and framework for generating knowledge and understanding."
  },
  {
    "objectID": "posts/GEOG0324/reproducibility_blog/2023-09-19-reproducibility.html",
    "href": "posts/GEOG0324/reproducibility_blog/2023-09-19-reproducibility.html",
    "title": "Reproducible and Open GIScience",
    "section": "",
    "text": "In the recent past, the geography community, like many scientific fields, has had to deal with a pressing challenge, the reproducibility crisis. The explosion of available data and the surplus of tools at our disposal to work with the data have triggered a few fundamental questions:\n\nHow can we generate reliable knowledge?\nHow can we enhance the transparency of research outcomes?\nHow can we recognize and mitigate bias?\nHow can we identify and eliminate misrepresentation?\n\nIn this context, reproducibility and replicability have emerged as foundational concepts (National Academies of Sciences, Medicine, et al. 2019). According to NASEM, “Reproducibility is obtaining consistent results using the same input data, computational steps, methods, and code, and conditions of analysis… Replicability is obtaining consistent results across studies aimed at answering the same scientific question, each of which has obtained its own data” (2019).\nOpen source GIS, driven by principles of openness and collaboration, aligns seamlessly with the reproducibility goals geographers and the scientific community are increasingly pursuing.\nRey’s 2009 study “Show me the code: Spatial analysis and open source” highlights the transformative potential of open source to enhance reproducibility in geography and spatial analysis. Rey characterizes open source as “a revolutionary collection of tools and processes through which individuals create, share, and apply new software and knowledge”, and that this movement has coincided with the resurgence of spatial analysis (Rey 2009).\nOpen source has a number of clear advantages. It provides access to geographic data and analytical tools, introduces a wider audience to new skills, fosters collaboration and education, and encourages knowledge sharing. Rey notes that “the value of the open source community is nested in not what you own, but in what you share and contribute” (2009). An open source community that nurtures innovation both removes barriers from science and ultimately can lead to better results.\nIn fact, the open source framework leads to a new diversity in research questions, and allows developers and scientists to adapt their tools to their projects, removing their dependence on closed softwares (Rey 2009). If open source continues gaining widespread acceptance, it has the potential to end the perpetual race among researchers and to foster a healthier, more productive and innovative research environment (TEDxTalks 2021).\nHowever, for open source GIS to reach its potential, the scientific community must confront several challenges. As emphasized by NASEM, transparency is non-negotiable in effective scientific reporting, throughout the data collection, preparation, and sharing process (2019). In an open source environment, scientists must be transparent and include thorough documentation.\nAnother challenge, as Ray points out, is the “‘‘developer-centric’’ nature of open source projects, which can foster technological elitism in the sense that only those individuals with adequate programming skills can participate in the development” (2009). To avoid creating an echo chamber for engineers and developers, open source GIS must expand participation and prioritize the needs of users.\nFinally, open source science must challenge the predominant academic/scientific model of “publish or perish” (TEDxTalks 2021). Open source science presents an alternate framework to the competition, prestige, and blame in academia, but its success depends on its recognition and adoption by prestigious institutions. If we can overcome these challenges, open source science is a shining opportunity to foster a new era of collaborative spatial science. Together, open source GIS and a commitment to openness and transparency can propel us closer to solving the reproducibility crisis in geography.\n\n\n\n\nReferences\n\nNational Academies of Sciences, Engineering, Medicine, et al. 2019. “Reproducibility and Replicability in Science.”\n\n\nRey, Sergio J. 2009. “Show Me the Code: Spatial Analysis and Open Source.” Journal of Geographical Systems 11: 191–207.\n\n\nTEDxTalks. 2021. “Research Culture Is Broken; Open Science Can Fix It.” YouTube. https://www.youtube.com/watch?v=c-bemNZ-IqA."
  },
  {
    "objectID": "posts/CSCI04/quant_bias_blog/quantBiasBlog.html",
    "href": "posts/CSCI04/quant_bias_blog/quantBiasBlog.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness Blog",
    "section": "",
    "text": "References\n\nAhmad, Afra Saeed, Isaac Sabat, Rachel Trump-Steele, and Eden King. 2019. “Evidence-Based Strategies for Improving Diversity and Inclusion in Undergraduate Research Labs.” Frontiers in Psychology 10: 1305.\n\n\nArellano, Lucy. 2022. “Questioning the Science: How Quantitative Methodologies Perpetuate Inequity in Higher Education.” Education Sciences 12 (2): 116.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” In Conference on Fairness, Accountability and Transparency, 77–91. PMLR.\n\n\nGillborn, David, Paul Warmington, and Sean Demack. 2018. “QuantCrit: Education, Policy,‘big Data’and Principles for a Critical Race Theory of Statistics.” Race Ethnicity and Education 21 (2): 158–79.\n\n\nNarayanan, Arvind. 2022. “The Limits of the Quantitative Approach to Discrimination.” Speech.\n\n\nOakley, Ann. 1998. “Gender, Methodology and People’s Ways of Knowing: Some Problems with Feminism and the Paradigm Debate in Social Science.” Sociology 32 (4): 707–31."
  },
  {
    "objectID": "posts/CSCI04/penguins_blog/penguins_blog.html",
    "href": "posts/CSCI04/penguins_blog/penguins_blog.html",
    "title": "Classifying Palmer Penguins Blog",
    "section": "",
    "text": "Chinstrap!\n\nGentoo!\n\nAdelie!\n\nTo view the source code for this project, please visit this link: https://github.com/madgallop/madgallop.github.io/blob/main/posts/penguins_blog/penguins_blog.ipynb\n\n\nThe following post will run through a complete example of a standard machine learning workflow. It will ultimately confidently classify the species of a penguin into the three categories pictured above in the smallest number of measurements necessary.\nThis post classifies penguins into three different species based on three key penguin characteristics, and builds upon on more basic machine learning models (such as perceptron) that only classify two features.\n\n\nFirst, we will import the required packages.\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom matplotlib.patches import Patch\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nNext, we will import our dataset. This dataset is the Palmer Penguins data set collected by the Palmer Station in Antarcitica in collaboration with Dr. Kristen Gorman. The data contains a number of measurements for a number of penguins from each of the three species pictured above.\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n#train.head()\n\n\n\n\nThe end goal of this post is to predict the species of a penguin based on its measurements.\nTo begin, I will explore the penguins dataset by constructing a figure and a table.\nI will first create a dataframe so that I can better group data and create more informative charts\n\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    #df = pd.get_dummies(df) #comment in for data analysis, out for table construction\n    return df, y\n\nX_train, y_train = prepare_data(train)\n\nBefore making some tables and figures, I am curious to run some basic statistics on the data\n\nspecies3pct = (y_train==2).mean()\nspecies2pct = (y_train==1).mean()\nspecies1pct = (y_train==0).mean() \nspecies3cnt = (y_train==2).sum()\nspecies2cnt = (y_train==1).sum()\nspecies1cnt = (y_train==0).sum()\n\nprint(\"There are approximately\",species3cnt,\"penguins in Species 3, or\",species3pct.round(2)*100,\"% of the penguins\")\nprint(\"There are approximately\",species2cnt,\"penguins in Species 2, or\",species2pct.round(2)*100,\"% of the penguins\")\nprint(\"There are approximately\",species1cnt,\"penguins in Species 1, or\",species1pct.round(2)*100,\"% of the penguins\")\n\nThere are approximately 95 penguins in Species 3, or 37.0 % of the penguins\nThere are approximately 55 penguins in Species 2, or 21.0 % of the penguins\nThere are approximately 106 penguins in Species 1, or 41.0 % of the penguins\n\n\nThese numbers are the base rates for our model. The base rate is the accuracy rate of a trivial model that doesn’t use the features. It is the accuracy I would have if I made predictions without looking at the data.\nNext, I am curious in the following:\n\nWhere do the penguins live?\nDoes their flipper length/culmen length vary by sex or island?\nDoes weight vary by sex or island?\n\n\nX_train.groupby([\"Island\",\"Sex\"])[['Body Mass (g)','Culmen Length (mm)','Flipper Length (mm)']].aggregate([np.mean]).round(2)\n\n\n\n\n\n  \n    \n      \n      \n      Body Mass (g)\n      Culmen Length (mm)\n      Flipper Length (mm)\n    \n    \n      \n      \n      mean\n      mean\n      mean\n    \n    \n      Island\n      Sex\n      \n      \n      \n    \n  \n  \n    \n      Biscoe\n      FEMALE\n      4253.28\n      42.94\n      204.67\n    \n    \n      MALE\n      5168.12\n      47.54\n      214.49\n    \n    \n      Dream\n      FEMALE\n      3435.20\n      42.45\n      190.08\n    \n    \n      MALE\n      3973.30\n      46.64\n      196.73\n    \n    \n      Torgersen\n      FEMALE\n      3371.88\n      37.46\n      189.50\n    \n    \n      MALE\n      4016.18\n      40.88\n      195.65\n    \n  \n\n\n\n\n\nX_train.groupby([\"Island\"])[['Body Mass (g)','Culmen Length (mm)','Flipper Length (mm)']].aggregate([np.mean]).round(2)\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n      Culmen Length (mm)\n      Flipper Length (mm)\n    \n    \n      \n      mean\n      mean\n      mean\n    \n    \n      Island\n      \n      \n      \n    \n  \n  \n    \n      Biscoe\n      4738.85\n      45.38\n      209.88\n    \n    \n      Dream\n      3689.78\n      44.43\n      193.23\n    \n    \n      Torgersen\n      3703.79\n      39.22\n      192.67\n    \n  \n\n\n\n\n\nX_train.groupby([\"Island\", \"Sex\"]).size().reset_index()\n\n\n\n\n\n  \n    \n      \n      Island\n      Sex\n      0\n    \n  \n  \n    \n      0\n      Biscoe\n      FEMALE\n      61\n    \n    \n      1\n      Biscoe\n      MALE\n      69\n    \n    \n      2\n      Dream\n      FEMALE\n      49\n    \n    \n      3\n      Dream\n      MALE\n      44\n    \n    \n      4\n      Torgersen\n      FEMALE\n      16\n    \n    \n      5\n      Torgersen\n      MALE\n      17\n    \n  \n\n\n\n\n\nX_train.groupby([\"Island\"]).size().reset_index()\n\n\n\n\n\n  \n    \n      \n      Island\n      0\n    \n  \n  \n    \n      0\n      Biscoe\n      130\n    \n    \n      1\n      Dream\n      93\n    \n    \n      2\n      Torgersen\n      33\n    \n  \n\n\n\n\n\nX_train.groupby([\"Sex\"])[['Body Mass (g)']].aggregate([len]).round(2)\n\n\n\n\n\n  \n    \n      \n      Body Mass (g)\n    \n    \n      \n      len\n    \n    \n      Sex\n      \n    \n  \n  \n    \n      FEMALE\n      126\n    \n    \n      MALE\n      130\n    \n  \n\n\n\n\nI learned: - females tend to weigh less than males - females generally have shorter beaks - females generally have and shorter flippers. - there are slightly more males in the study - there are the most penguins on Biscoe island, then Dream, then Torgersen - there are similar amounts of penguins of different sexes on the different islands - Bisco island is home to penguins with heavier bodies, and longer beaks and flippers.\nNow I want to create a chart to better understand weight distributions of the penguins by island.\n\nsns.set_theme(style=\"whitegrid\", palette={\"Gold\",\"HotPink\",\"CornflowerBlue\"})\n\n# Draw a categorical scatterplot to show each observation\nax = sns.swarmplot(data=X_train, x='Body Mass (g)', y=\"Sex\", hue=\"Island\")\nax.set(ylabel=\"\")\nax.set(title = \"Body Mass by Island\")\n\n[Text(0.5, 1.0, 'Body Mass by Island')]\n\n\n\n\n\n\nsns.set_theme(style=\"whitegrid\", palette={\"Gold\",\"HotPink\",\"CornflowerBlue\"})\n\n# Draw a categorical scatterplot to show each observation\nax = sns.swarmplot(data=X_train, x='Flipper Length (mm)', y=\"Sex\", hue=\"Island\")\nax.set(ylabel=\"\")\nax.set(title = \"Flipper Length by Island\")\n\n[Text(0.5, 1.0, 'Flipper Length by Island')]\n\n\n\n\n\n\nsns.set_theme(style=\"whitegrid\", palette={\"Gold\",\"HotPink\",\"CornflowerBlue\"})\n\n# Draw a categorical scatterplot to show each observation\nax = sns.swarmplot(data=X_train, x='Culmen Length (mm)', y=\"Sex\", hue=\"Island\")\nax.set(ylabel=\"\")\nax.set(title = \"Culmen Length by Island\")\n\n[Text(0.5, 1.0, 'Culmen Length by Island')]\n\n\n\n\n\nWhile Biscoe island defintly hosts penguins with more mass and longer flippers, the same is not necessarily true for beak (Culmen) length. The male/female distinction does not appear to make a difference island by island.\n\n\n\nNow that we have a better sense of the data, I will prepare to implement a model. First, I modify the dataframe using pd.get_dummies to encode nominal data:\n\ndef prepare_data2(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df) #comment in for data analysis, out for table construction\n    return df, y\n\nX_train2, y_train2 = prepare_data2(train)\n\nOur first task is to find three features of the data and a model trained on those features which achieves 100% testing accuracy - One feature must be qualitative (like Island or Clutch Completion). - The other two features must be quantitative (like Body Mass (g) or Culmen Depth (mm)).\nFirst, lets figure out what features are best.\n\n\n\nAfter some experimenting, I decided to select the optimal features using Ridge CV.\n\nridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(X_train2, y_train2) #Ridge regression with built-in cross-validation.\nimportance = np.abs(ridge.coef_) # assign importance to each feature through a specific attribute\n#The features with the highest absolute coef_ value are considered the most important\n\n#create a plot to visualize the importance of the features\nfeature_names = np.array(X_train2.columns)\nplt.barh(feature_names, importance)\nplt.title(\"Feature importances via coefficients\")\nplt.show()\n\n#select a certain number of features\nthreshold = np.sort(importance)[-5]\n\nsfm = SelectFromModel(ridge, threshold=threshold).fit(X_train2, y_train2)\n#only select the first three features from the list\nselected_features = feature_names[sfm.get_support()][0:3]\n\nprint(selected_features)\nprint(f\"Features selected by SelectFromModel: {selected_features}\")\n\n\n\n\n['Culmen Depth (mm)' 'Delta 13 C (o/oo)' 'Island_Torgersen']\nFeatures selected by SelectFromModel: ['Culmen Depth (mm)' 'Delta 13 C (o/oo)' 'Island_Torgersen']\n\n\nUsing the select by model method, Culmen Depth, Delta 13 C and Island_Torgersen are the most important for correct classification of penguins.\n\n\n\nI will now explore how well linear regression works for this data, and then experiment with DecisionTreeClassifier and RandomForestClassifier.\n\n\n\n#change selected features to invlude Island_Dream and Island_Biscoe as well \nfeatures_to_test = X_train2[selected_features].join(X_train2[\"Island_Dream\"]).join(X_train2[\"Island_Biscoe\"])\nLR = LogisticRegression(random_state = 10, max_iter = 1000)\nLR.fit(features_to_test, y_train2)\n\nLR.score(features_to_test, y_train2)\n\n0.93359375\n\n\nLogistic regression is 93% accurate on the training data.\n\n\n\n\nNow, we will evaluate the model’s efficacy on the test data\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ndef prepare_data3(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df) #comment in for data analysis, out for table construction\n    return df, y\n\nX_test, y_test = prepare_data3(test)\n# print(X_test)\n# print(y_test)\n\nfeatures_to_test2 = X_test[selected_features].join(X_test[\"Island_Dream\"]).join(X_test[\"Island_Biscoe\"])\n\nLR.fit(features_to_test2, y_test)\nLR.score(features_to_test2, y_test)\n\n0.9411764705882353\n\n\nThe model is slightly more effective on the test data, at 94%. In the two cells below, I experiment with different parameters to allow for this particular model to reach 100% accuracy on the test data.\n\n#change selected features to invlude Island_Dream and Island_Biscoe as well \nfeatures_to_test_extra = ['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Torgersen', \"Island_Dream\", \"Island_Biscoe\"]\nLR_extra = LogisticRegression(random_state = 10, max_iter = 1000)\nLR_extra.fit(X_train2[features_to_test_extra], y_train2)\n\nLR_extra.score(X_train2[features_to_test_extra], y_train2)\n\n0.99609375\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ndef prepare_data3(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis = 1)\n    df = pd.get_dummies(df) #comment in for data analysis, out for table construction\n    return df, y\n\nX_test, y_test = prepare_data3(test)\n# print(X_test)\n# print(y_test)\n\n# features_to_test2 = X_test[selected_features].join(X_test[\"Island_Dream\"]).join(X_test[\"Island_Biscoe\"])\nfeatures_to_test2_extra = ['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Torgersen', \"Island_Dream\", \"Island_Biscoe\"]\n\nLR_extra.fit(X_test[features_to_test2_extra], y_test)\nLR_extra.score(X_test[features_to_test2_extra], y_test)\n\n1.0\n\n\nWhen I changed my selected features to Culmen Depth (mm), Culment Length (mm), Island_Torgersen, Island_Dream, Island_Biscoe, the model achieved 100% accuracy on the test data. I will not plot this data, but it is nice that some selected features allow for this model to reach 100% accuracy, just not the features chosen by the “Select by Model” classification system.\n\n\n\nNow we will plot the decision regions, seperated by qualitative feature (Island: Torgersen, Dream, Biscoe)\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n\n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (15, 3)) #maybe put 1 there\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n        XY = pd.DataFrame({\n            X.columns[0] : XX,\n            X.columns[1] : YY\n        })\n\n        for j in qual_features:\n            XY[j] = 0\n\n        XY[qual_features[i]] = 1\n\n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n        axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n        ix = X[qual_features[i]] == 1\n      # plot the data\n        axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n        axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n        patches = []\n        for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n            patches.append(Patch(color = color, label = spec))\n\n        plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n        plt.tight_layout()\n\n\nplot_regions(LR, features_to_test, y_train2)\n\n\n\n\nNext, we will experiment with a couple other classifiers.\n\n\n\ndtc = DecisionTreeClassifier(max_depth=5)\n\ncross_val_score(dtc, features_to_test, y_train2, cv=5)\n\narray([0.92307692, 0.92156863, 0.84313725, 0.90196078, 0.92156863])\n\n\n\ndtc.fit(features_to_test, y_train2, sample_weight=None, check_input=True)\n\ndtc.score(features_to_test, y_train2, sample_weight=None)\n\n0.96875\n\n\nThis model is more effective than regression on the training data.\n\n\n\ndtc.fit(features_to_test2, y_test, sample_weight=None, check_input=True)\n\ndtc.score(features_to_test2, y_test)\n\n0.9852941176470589\n\n\nIt is also more effective than regression on the test data.\n\n\n\n\nplot_regions(dtc, features_to_test, y_train2)\n\n\n\n\nThis model uses more advanced geometries to separate the data\n\n\n\n\nLet’s look at one more type of classifier!\n\nrfc = RandomForestClassifier(max_depth=7, random_state=1)\nrfc.fit(features_to_test, y_train2)\n\nrfc.apply(features_to_test)\nrfc.score(features_to_test, y_train2, sample_weight=None)\n\n0.99609375\n\n\nThis model is incredibly effective for the chosen parameters!\n\n\n\nrfc.fit(features_to_test2, y_test)\n\nrfc.score(features_to_test2, y_test)\n\n1.0\n\n\nThis model achieves 100% testing accuracy, making it the best choice for the features chosen by the select from model classificiation.\n\n\n\n\nplot_regions(rfc, features_to_test, y_train2)\n\n\n\n\nThis model also uses complex geometries, but is the superior choice in classifying the penguins for the selected features.\n\n\n\n\n\n\nIn this post, we selected features from the Palmer dataset and classified penguins using machine learning. We found Ridge CV to be an effective approach in selecting features, and selected Culmen Depth, Delta 13 C and Island_Torgersen. Then, we tried a number of machine learning algorithms, including logistic regression, decision trees, and random forest with the goal of obtaining 100% accuracy on the testing data. While all of the models performed relatively well, the Random Forest Classifier yielded the best results (100% accuracy on the test data, and 99.6% on the training data), making it the best choice for these features. Thanks!"
  },
  {
    "objectID": "posts/CSCI04/unsupervised_learning_blog/unsupervised_blog.html",
    "href": "posts/CSCI04/unsupervised_learning_blog/unsupervised_blog.html",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "",
    "text": "To view the source code on git, pelase follow this link: https://github.com/madgallop/madgallop.github.io/blob/main/posts/unsupervised_learning_blog/unsupervised_blog.ipynb\n\n\n\n\nIn the following blog post, I implement a simple machine learning approach for image compression and image segmentation. I will explore linear algebra methods (focusing on singluar value decomposition) for unsupervised learning using images.\nIn machine learning, unsupervised learning refers to machine learning techniques in which we do not have a target matrix y cooresponding to our feature matrix X. Thus, unsupervised learning focuses on finding patterns in the features X instead of making predictions and validating them.\nDimensionality reduction is a form of unsupervised learning in which we identify similar or related features (columns) in our feature matrix.\nMatrix factorization is one common approach to dimensionality reduction. In this approach, we wnat to write X≈UW where U and W are constrained matrices. These constraints include: \\(X \\in R^{nxp}\\), then we must have \\(U \\in R^{nxk}\\) and \\(W \\in R^{kxp}\\) for some k (the latent dimension) that is less than p. \n\n\n\nSVD is an important and versitile technique for matrix factorization.\nFor a real matrix \\(A \\in R^{mxn}\\), a singluar value decomposition is \\(A = UDV^{T}\\) where \\(D \\in R^{mx n}\\) has nonzero entries only along the diagonal, and where \\(U \\in R^{mxm}\\) and \\(V \\in R^{nxn}\\) are orthogonal matrices. The nonzero entreis of D (\\(\\sigma_i\\)) provide a measure for how “large matrix A is.\nNumpy makes computing SVD of a matrix easier:\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(12345)\n\n\na_1 = np.random.randint(1, 3, (5, 3))\na_2 = np.random.randint(1, 3, (3, 7))\n\nA = a_1 @ a_2 + 0.1*np.random.randn(5, 7)\nA\n\narray([[ 9.13529168, 10.08864293,  9.79983627,  7.96281575,  9.16690253,\n         4.95614303,  6.94602586],\n       [ 8.0476985 , 10.32489439,  9.89787725,  8.94229127,  8.01241213,\n         5.03026136,  7.05237721],\n       [ 7.00009403,  8.13438098,  7.9286456 ,  6.91688465,  6.76297683,\n         3.81392392,  5.91392426],\n       [ 9.05601453,  9.87340655, 10.01198271,  7.89364876,  9.03328827,\n         4.76405812,  6.9800457 ],\n       [ 8.84580045,  9.90292641,  9.86929697,  8.02863497,  9.03779841,\n         4.92461135,  7.03312857]])\n\n\nNow, we can visualize the matrix as an image:\n\nplt.imshow(A, cmap = \"Greys\")\na = plt.gca().axis(\"off\")\n\n\n\n\nNow, we can compute a singular value decomposition, where sigma is a numpy array (vector) that contains the singlular values of A:\n\nU, sigma, V = np.linalg.svd(A)\n\nNow, we can create a diagonal matrix using matrix multiplication with the elements of sigma.\n\n# create the D matrix in the SVD\nD = np.zeros_like(A,dtype=float) # matrix of zeros of same shape as A\nD[:min(A.shape),:min(A.shape)] = np.diag(sigma)        # singular values on the main diagonal\nD\n\narray([[47.92787806,  0.        ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  1.59525638,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.35772778,  0.        ,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.23426796,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.15684841,\n         0.        ,  0.        ]])\n\n\nNow, we can reconstruct A:\n\nU @ D @ V # == A up to numerical precision\n\narray([[ 9.13529168, 10.08864293,  9.79983627,  7.96281575,  9.16690253,\n         4.95614303,  6.94602586],\n       [ 8.0476985 , 10.32489439,  9.89787725,  8.94229127,  8.01241213,\n         5.03026136,  7.05237721],\n       [ 7.00009403,  8.13438098,  7.9286456 ,  6.91688465,  6.76297683,\n         3.81392392,  5.91392426],\n       [ 9.05601453,  9.87340655, 10.01198271,  7.89364876,  9.03328827,\n         4.76405812,  6.9800457 ],\n       [ 8.84580045,  9.90292641,  9.86929697,  8.02863497,  9.03779841,\n         4.92461135,  7.03312857]])\n\n\nSVD lets us also approximate using smaller representations. To see this approximation, we can select the first k columns of U, the top k singular values of D, and the first k rows of V.\nIf k=2:\n\nk = 2\nU_ = U[:,:k]\nD_ = D[:k, :k]\nV_ = V[:k, :]\n\n\nA_ = U_ @ D_ @ V_\n\nNow, with this k value (that must be smaller than m and n, we can compare these results:\n\ndef compare_images(A, A_):\n\n    fig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\n    axarr[0].imshow(A, cmap = \"Greys\")\n    axarr[0].axis(\"off\")\n    axarr[0].set(title = \"original image\")\n\n    axarr[1].imshow(A_, cmap = \"Greys\")\n    axarr[1].axis(\"off\")\n    axarr[1].set(title = \"reconstructed image\")\n\ncompare_images(A, A_)\n\n\n\n\nWe can also try k=1:\n\nk = 1\nU_ = U[:,:k]\nD_ = D[:k, :k]\nV_ = V[:k, :]\nA_ = U_ @ D_ @ V_\ncompare_images(A, A_)\n\n\n\n\nFor both k values, the reconstructed image looks very similar to the original.\n\n\n\n\nLet’s employ a function to read an image from the internet and save it as a numpy array:\n\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\nI’ve selected an image of my favorite town in Colorado, Crested Butte.\n\nurl = \"https://images.fineartamerica.com/images-medium-large-5/colorful-crested-butte-dusty-demerson.jpg\"\n\nimg = read_image(url)\n\nWe will first convert the image to greyscale:\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\nLet’s find the size of the greyscale image:\n\ngrey_img.shape \n\n(603, 900)\n\n\nNow, we are ready to use our SVD pipeline to approximate this image through image compression. Image compression allows us to store large images in a manner that takes up less space on hard drives with limited storage.\n\n\n\nNow, we will write a function svd_reconstruct that will reconstruct an image from its singular value decomposition.\n\ndef svd_reconstruct(image, k): #take in the grayscale image and a value of k.\n    n = image.shape[0]\n    m = image.shape[1]\n    \n    #get U, sigma and V from svd\n    U, sigma, V = np.linalg.svd(image)\n    #create the diagonal matrix D\n    D = np.zeros_like(image,dtype=float) # matrix of zeros of same shape as A\n    D[:min(image.shape),:min(image.shape)] = np.diag(sigma)        # singular values on the main diagonal\n    \n    #reconstruct A from U D and V\n    reconstructed_image =U @ D @ V # numerical precision\n    \n    U_ = U[:,:k]\n    D_ = D[:k, :k]\n    V_ = V[:k, :]\n\n    image_ = U_ @ D_ @ V_\n    \n    compare_images(image,image_)\n    \n    return reconstructed_image\n\n\n\n\n\n\n\ngrey_img_reconstruct_1 = svd_reconstruct(grey_img, 1)\n\n\n\n\nWith 1 component, the image is extremely blurry and almost unrecognizable.\n\n\n\n\ngrey_img_reconstruct_10 = svd_reconstruct(grey_img, 10)\n\n\n\n\n\n\n\n\ngrey_img_reconstruct_100 = svd_reconstruct(grey_img, 100)\n\n\n\n\n\n\n\n\ngrey_img_reconstruct_100000 = svd_reconstruct(grey_img, 100000)\n\n\n\n\nThe smaller the number of components we have is, the more we have simplified our data, but, at the same time, small k values cause us to lose a lot of interesting structure and detail in the image that we might want to capture.\n\n\n\n\nIn this example, the user does not know how many components k to use, but knows the amount of space the image will likely take up. Here, I allow the user to select a desired compression factor and then I calculate the number of components k to use based on this selection.\nEnter the factor by which to compress the image by:\n\ncompression = float(input(\"Enter what factor to compress the image by: \"))\nprint(compression)\n\nEnter what percent to compress the image by:  20\n\n\n20.0\n\n\nNow, we can modify our reconstruction function to take in the compression factor instead of k\n\ndef svd_reconstruct_2(image, compression_factor):\n    n = image.shape[0]\n    m = image.shape[1]\n    #calculate k from the compression factor. \n    k = round((n*m)/(n*compression_factor+m*compression_factor+compression_factor))\n    #storing n and m, as well as the compression factor \n    \n    #get U, sigma and V from svd\n    U, sigma, V = np.linalg.svd(image)\n    #create the diagonal matrix D\n    D = np.zeros_like(image,dtype=float) # matrix of zeros of same shape as A\n    D[:min(image.shape),:min(image.shape)] = np.diag(sigma)        # singular values on the main diagonal\n    \n    #reconstruct A from U D and V\n    reconstructed_image =U @ D @ V # numerical precision\n    \n    U_ = U[:,:k]\n    D_ = D[:k, :k]\n    V_ = V[:k, :]\n\n    image_ = U_ @ D_ @ V_\n    print(str(k)+\" components, % compressed = \"+str(compression_factor))\n    compare_images(image,image_)\n    \n    return reconstructed_image\n\n\nNow, visualize the reconstructed images:\n\ngrey_img_reconstruct_1 = svd_reconstruct_2(grey_img, compression)\n\n18 components, % compressed = 20.0\n\n\n\n\n\nLet’s experiment with more compression factors:\n\n\n\ngrey_img_reconstruct_1 = svd_reconstruct_2(grey_img, 0.1)\n\n3608 components, % compressed = 0.1\n\n\n\n\n\n\n\n\n\ngrey_img_reconstruct_1 = svd_reconstruct_2(grey_img, 1)\n\n361 components, % compressed = 1\n\n\n\n\n\nFor a compression factor of 1, I still need 100% of the storage needed for the original image\n\n\n\n\ngrey_img_reconstruct_1 = svd_reconstruct_2(grey_img, 10) \n\n36 components, % compressed = 10\n\n\n\n\n\nFor a compression factor of 10, I need 10% of the storage needed for the original iamge.\n\n\n\n\ngrey_img_reconstruct_1 = svd_reconstruct_2(grey_img, 100)\n\n4 components, % compressed = 100\n\n\n\n\n\nFor a compression factor of 100, I need only 1% of the storage needed for the original image.\nAs we can see in the above examples, as the compression factor increases, k (# of components) decreases. This means that the amount of storage needed for the image decreases as the compression factor increases. Fewer components k indicates that we need less storage for the image.\nIn this blog, we have learned how to compress image through controlling the number of components, and have seen the effects of compression on image storage. The blog provides an example of linear algebra methods for unsupervised learning with images."
  },
  {
    "objectID": "posts/CSCI04/gebru_blog/GebruBlogpost.html",
    "href": "posts/CSCI04/gebru_blog/GebruBlogpost.html",
    "title": "Learning from Timnit Gebru",
    "section": "",
    "text": "Next Monday, Dr. Timnit Gebru is coming to Middlebury to visit our class and give a talk to the wider campus community. I am excited to have the opportunity to learn from Dr. Gebru next week, as I found her research, advocacy, persistence, and honesty in sharing her personal experiences to be incredibly inspiring (even reading through her Wiki page made me a little emotional!)\nDr. Gebru is a well-recognized voice in artificial intelligence and its ethical implications. She was born in Ethiopia and attended Stanford, then worked for Google and on projects such using Street View and deep learning to predict social variables of communities based on observations of cars in different places. Dr. Gebru experienced discrimination throughout her early life and professional career, and noticed a lack of Black researchers (and especially Black women) in AI. She founded Black in AI to provide a community for Black researchers in the field.\nIn 2019, Gebru and other researchers called out Amazon’s facial-recognition system for model discrimination against darker-skinned females, and in 2020, when Gebru was the co-lead of its Ethical Artificial Intelligence Team, Google dismissed her for refusing to withdraw an unpublished research paper she co-authored on the dangers of large language models (i.e. bais, costs, deception). The incident became a public controversy, as over 7,000 people signed a letter condemning her firing.\nIn December of 2021, Dr. Gebru launched the Distributed Artificial Intelligence Research Institute to research the effects of AI on marginalized communities."
  },
  {
    "objectID": "posts/CSCI04/gebru_blog/GebruBlogpost.html#dr.-gebrus-2020-talk",
    "href": "posts/CSCI04/gebru_blog/GebruBlogpost.html#dr.-gebrus-2020-talk",
    "title": "Learning from Timnit Gebru",
    "section": "Dr. Gebru’s 2020 Talk",
    "text": "Dr. Gebru’s 2020 Talk\nIn 2020, Dr. Gebru gave a talk as part of a Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision at the conference on Computer Vision and Pattern Recognition.\nIn this talk, I really appreciated Dr. Gebru’s honest description of the lack of diversity and representation in artificial intelligence. I also found it interesting that she feels there have been improvements in diversity in the machine learning field, but that computer vision is not quite there yet. As a woman in STEM fields, I have definitely experienced being one of only a few women in classes or at conferences or other events. Even in my Conservation Planning class this semester (which is a technical geography course based in Python), I am one of two women in the class. While I have experienced sexism, I have obviously never experienced intersectional discrimination, and value Dr. Gebru’s candor in describing her experiences.\nAfter speaking about her own experiences, Dr. Gebru discusses what computer vision is, and how it has the potential to disproportionately harm marginalized communities. She goes into the racist way that computer vision screens potential employees through facial recognition. She urges us to consider questions that we have engaged with a little bit in this course already, such as who produces these technologies, and what groups they serve/leave out. She brings up an important quote from Mimi Onuoha, which describes how “Every data set that involves people implies subjects and objects… it is imperative to remember that on both sides we have human beings.” She discusses how in ML and a variety of other fields, we do not see a diversity in datasets. But she notes that even in cases where we do, they do little good if we do not think critically about how we acquire them.\nI think the main takeaways I have and one of the most important points that Dr. Gebru discusses is that making something fair is not equivalent to making it the same for everyone. Fairness is not about math or statistics, but more about society and social structure. Even tools that are inclusive of multiple communities can still do harm, and the tools that we rely on, such as in computer vision, disproportionately harm marginalized communities on a national level."
  },
  {
    "objectID": "posts/CSCI04/gebru_blog/GebruBlogpost.html#question",
    "href": "posts/CSCI04/gebru_blog/GebruBlogpost.html#question",
    "title": "Learning from Timnit Gebru",
    "section": "Question",
    "text": "Question\nYou spoke at the end of your talk on fairness in computer vision at the conference on Computer Vision and Pattern Recognition about technology that harms and marginalizes various communities, as well as examples of refusal to combat these strategies. Since you gave this talk, have you seen improvements in either diversity in the field of computer vision or further examples of resistance to harmful technologies in the AI world?"
  },
  {
    "objectID": "posts/CSCI04/gebru_blog/GebruBlogpost.html#part-2-dr.-gebrus-talk-at-middlebury",
    "href": "posts/CSCI04/gebru_blog/GebruBlogpost.html#part-2-dr.-gebrus-talk-at-middlebury",
    "title": "Learning from Timnit Gebru",
    "section": "Part 2: Dr. Gebru’s Talk “At” Middlebury",
    "text": "Part 2: Dr. Gebru’s Talk “At” Middlebury\nOn April 24th, Dr. Gebru gave a talk on “Eugenics and the Promise of Utopia through Artificial General Intelligence” in which she ultimately explained and problematized the vision of “utopia” created by AI and calls for more accountability in the field.\nDr. Gebru opened her talk by discussing Artificial General Intelligence (AGI) and its connection to 20th century eugenics. She began by exemplifying the ways people at the forefront of this technology are trying to create an all-powerful god, and continued to describe 2nd wave eugenics as the desire to improve the human stock by designing more intelligent people and children. Dr. Gebru then explained some of the primary tenets of TESCREAL, focusing on transhumanism, or the aim to transcend humanity all together to fulfill our predestined potential. Beginning in the 1990s, AI builds on this idea, with some scientists aiming to create “posthumans” and a new superior species, leaving “legacy humans” behind, or perhaps destroying them for the greater good. Dr. Gebru shared tweets and papers, explaining the views of proponents of AGI and its connections to eugenics, mainly through the idea of utopia, but only for the best and brightest.\nAfter discussing some of the letters of TESCREAL, Dr. Gebru explaind the history of AGI and the extent of its current funding (over $10 billion), and how this has led to large companies each creating larger and larger language models, each one claiming it will get us to this AGI utopia. Importantly, she discussed some of the hidden, negative aspects of this kind of technology. These effects include data theft, the use of often use other models such as Google Translate to get results (equivalent to training on the test data), and theft of numerous other datasets without authentically sourcing data. These practices divert funding away from other smaller organizations that are doing good work and specialize in a certain field (African language, for example). She also talked about the environmental costs of these large models, and the human cost for the workers abroad who moderate horrific content for little pay, all to help concentrate power in the hands of a few.\nDr. Gebru concluded her talk by noting that when AGI scientists, billionaires, and other supporters speak about the impending utopia, they discuss it as if it is science fiction, with machines becoming “all powerful” and escaping human control. She warned the audience that these machines are not hypothetical and uncontrollable. Real people build them and deploy them. By framing the AGI craze as a fantastic revolution, we take accountability away from the people and corporations responsible for the technology and for the harm they inflict. She ultimately calls for more regulation on the tech field, smaller, focused models, and collective action.\nOverall, I agree with Dr. Gebru’s points. Before this talk, I had learned nothing about both the negative human and environmental effects of large models or about AGI and its utopian promises. I found Dr. Gebru’s inclusion of tweets and scientific papers describing AGI’s promises to be rather frightening, and while I had hoped for a little more clarification of the links between 20th century eugenics and AI, her conclusions definitely resonated with me. I agree that beyond speculation of whether or not we are heading towards a future where only the brightest will be able to survive as transhumans, large corporations that are profiting from the increasing ubiquity of their models must be held accountable for the real and serious negative effects of their work."
  },
  {
    "objectID": "posts/CSCI04/gebru_blog/GebruBlogpost.html#part-3-reflect-on-the-process",
    "href": "posts/CSCI04/gebru_blog/GebruBlogpost.html#part-3-reflect-on-the-process",
    "title": "Learning from Timnit Gebru",
    "section": "Part 3: Reflect on the Process",
    "text": "Part 3: Reflect on the Process\nI really enjoyed interacting with Dr. Gebru and learning about her work. I learned a lot about AI from her evening presentation, and learned even more about the industry and equity during our in-class conversation with Dr. Gebru earlier in the day. Excitingly, I had the opportunity to ask Dr. Gebru the question I outlined above about whether or not equity in computer vision has increased and about further examples of resistance to harmful tech. Dr. Gebru answered the first part of the question with a definitive NO. She shared an anecdote about how the computer vision conference used to be called the NIPS conference, with nips.com leading people to a porn site. She talked about rampant harassment at this conference, and that people received death threats after calling for a name change. She said that because every change in the field is so contentious, it sometimes seems hardly worth it to fight for them. I found the history of the conference appalling and find it sad that even someone as inspiring and driven as Dr. Gebru feels that change is almost impossible to achieve in this field.\nNonetheless, Dr. Gebru spoke about some initiatives of resistance to harmful technologies such as a group at the University of Chicago that made a tool called GLAZE to protect artists’ work from big AI models. I am curious to learn about more companies that are doing good with computer vision and AI, as if I eventually work in these fields, I hope to be working for a company that promotes social justice and resistance.\nFinally, like many of my classmates, I was inspired by Dr. Gebru’s reflection on imposter syndrome. She stated her frustration with the endless ability of unqualified men to achieve, noting that “imposter syndrome” is the wrong term for the feeling of under qualification as a woman or as a minority stems. Instead, this feeling stems from the conscious effort of people who do not value your work or expertise, perpetuated through institutions like sexism and racism. This answer served as a reminder to trust my experience and knowledge, and to always seek out supportive communities.\nThank you Dr. Gebru!\nLink to the post on Git: https://github.com/madgallop/madgallop.github.io/blob/main/posts/gebru_blog/GebruBlogpost.ipynb"
  },
  {
    "objectID": "posts/CSCI04/linear_regression_blog/linear_regression_blog.html",
    "href": "posts/CSCI04/linear_regression_blog/linear_regression_blog.html",
    "title": "Implementing Linear Regression Blog",
    "section": "",
    "text": "Regression, as opposed to classification, is a method to predict a real number for each data point based on its features. This post will focus on least squares linear regression, which falls into our framework of convex linear models.\nThe following post begins by implementing least-squares linear regression in two ways: using the analytical formula for the optimal weight vector (requiring matrix inversion and several matrix multiplications), and using the formula for the gradient of the loss function to implement gradient descent for linear regression.\nTo view the source code, pelase follow this link: https://github.com/madgallop/madgallop.github.io/blob/main/posts/linear_regression_blog/linear_regression.py\n\n\nFirst, import required packages:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import Lasso\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nNow, create a function to make both testing and validation data to test the implementation:\n\ndef pad(X): #ensure X is the proper shape\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nVisualize the testing and validation data:\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nIt is only possible to easily visualize this problem when p_features = 1. Now, let’s implement linear regression using two different methods.\n\n\n\nWe begin by implementing the analytical formula for the optimal weight vector w_hat from the lecture notes. The following is the explicit formula for w:\n\\[\\\\{\\\\{wHat}}={(X^TX)^{-1}X^Ty}\\]\nFirst, auto-refresh the source code:\n\n%load_ext autoreload\n%autoreload 2\n\n\nfrom linear_regression import LinearRegression #call method from source code\nLR = LinearRegression()\nLR.fit_analytical(X_train, y_train) #use implementation of above formula \n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.7271\nValidation score = 0.6894\n\n\nThe score (coefficient) is always smaller than 1, with a higher value indicating better predictive performance. As usual, gaps between the training and validation scores suggest the possibility of overfitting, although further investigation is required to see whether improvement on validation data is possible. In this case, the gap between the scores is relatively small.\nThe estimated weight vector w is:\n\nLR.w\n\narray([1.07026516, 1.05566706])\n\n\nNow, let’s visualize the performance of the analytic formula on our training and validation data.\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\naxarr[0].plot(X_train, LR.predict(X_train), color = \"black\")\naxarr[1].plot(X_val, LR.predict(X_val), color = \"black\")\nplt.tight_layout()\n\n\n\n\nThese predicted regression lines coorespond pretty well with both the training and test data! Let’s compare the weights and scores using the analytical method to those using gradient descent.\n\n\n\nThe following is the formula for the gradient that our source code implements:\n\\[\\\\{\\\\{w^{(t+1)}}}={w^{(t)}- \\alpha (Pw ^{(t)}-q})\\]\nNow, let’s see if we get the same weight vector from this method using the same testing and training data:\n\nLR2 = LinearRegression() #call method\n\nLR2.fit_gradient(X_train, y_train, alpha = 0.01, max_iter = 100) #gradient does not have the divide by points thing, need to adapt the learning rate to account for fact gradietn is too bigh .\nLR2.w\n\narray([1.07026515, 1.05566706])\n\n\nThis method lends the same value for w as the analytic method. Now, let’s visualize how the score using gradient descent changes over time.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\nAs pictured, the score increases monotonically in each iteration, increasing rapidly until about 50 iterations, and then increasing at a much slower rate. Now, we will experiment with increasing the number of features.\n\n\n\nIn the following experiment, I allow p_features, the number of features used, to increase, while holding n_train, the number of training points, constant. I then assess and explain the differences between the training and validation scores.\n\nLR_ex = LinearRegression()\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\ntrainScoreArray = []\ntestScoreArray = []\n\nfor feature in range(0,(n_train-1)):\n    p_features = feature\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR_ex.fit_analytical(X_train, y_train) \n    trainScore = LR_ex.score(X_train, y_train).round(4)\n    testScore = LR_ex.score(X_val, y_val).round(4)\n    trainScoreArray.append(trainScore)\n    testScoreArray.append(testScore)\nplt.plot(trainScoreArray, label = 'training')\nplt.plot(testScoreArray, label = 'validation')\nlabels = plt.gca().set(xlabel = \"Features\", ylabel = \"Score\")\nplt.legend(loc=\"lower left\")\n\nplt.show()    \n    \n\n\n\n\nAs the number of features in this experiment increases, the validation score tends to decrease relative to the training score. It follows that increasing the number of features leads to overfitting, where the model is too finely tuned to the training data. An overfit model struggles to simply representing overarching trends. In this case, as the number of features approaches the number of training points, the model began scoring much more highly on the training data (show in blue) while not achieving as high of an accuracy score for the test data (shown in orange). Next, we will try to combat overfit models using LASSO regularization.\n\n\n\nThe LASSO algorithm uses a modified loss function with a regularization term, the effect of which is to make the entries of the weight vector w small. The LASSO algorithm tends to force each entry of the weight vector to be exactly zero. This is a helpful property in overfitted problems, especially when the number of features p is larger than the number of data points n.\n\nL = Lasso(alpha = 0.001)\n\nHere, alpha controls the strength of the regularization. Now, let’s fit this model on some data and check the coefficients:\n\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\n\nLasso(alpha=0.001)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=0.001)\n\n\nNow, let’s calculate the score:\n\nL.score(X_val, y_val)\n\n0.8268398547106713\n\n\nThe LASSO algorithm yields a high validation score.\nNow, I will replicate the same experiment I did with linear regression, increasing the number of to exceed the number of training points using the LASSO algorithm. The following also experiments with different values of alpha.\n\nL1 = Lasso(alpha = 0.001)\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\ntrainScoreArray = []\ntestScoreArray = []\n\n#alpha = 0.001\nfor feature in range(1,(n_train+1)): \n    p_features = feature\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L1.fit(X_train, y_train) \n    trainScore = L1.score(X_train, y_train).round(4)\n    testScore = L1.score(X_val, y_val).round(4)\n    trainScoreArray.append(trainScore)\n    testScoreArray.append(testScore)\n    \n#alpha = 0.01\nL2 = Lasso(alpha = 0.01)\ntrainScoreArray2 = []\ntestScoreArray2 = []\n\nfor feature in range(1,(n_train+1)): \n    p_features = feature\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L2.fit(X_train, y_train) \n    trainScore2 = L2.score(X_train, y_train).round(4)\n    testScore2 = L2.score(X_val, y_val).round(4)\n    trainScoreArray2.append(trainScore2)\n    testScoreArray2.append(testScore2)   \n    \n#alpha = 0.0001, made 0.0004 to allow model to converge \nL0 = Lasso(alpha = 0.0004)\ntrainScoreArray0 = []\ntestScoreArray0 = []\n\nfor feature in range(1,(n_train+1)): \n    p_features = feature\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L0.fit(X_train, y_train) \n    trainScore0 = L0.score(X_train, y_train).round(4)\n    testScore0 = L0.score(X_val, y_val).round(4)\n    trainScoreArray0.append(trainScore0)\n    testScoreArray0.append(testScore0)    \n    \n    \nfig, axarr = plt.subplots(1, 3, sharex = True, sharey = True, figsize = (15,6))\n#alpha = 0.0001\naxarr[0].plot(trainScoreArray0, label = 'training')\naxarr[0].plot(testScoreArray0, label = 'validation')\nlabels = plt.gca().set(xlabel = \"Features\", ylabel = \"Score\")\naxarr[0].legend(loc=\"lower left\")\naxarr[0].set(xlabel = \"Features\", ylabel = \"Score\", title = \"alpha = 0.0004\")\n\n#alpha = 0.001\naxarr[1].plot(trainScoreArray, label = 'training')\naxarr[1].plot(testScoreArray, label = 'validation')\nlabels = plt.gca().set(xlabel = \"Features\", ylabel = \"Score\")\naxarr[1].legend(loc=\"lower left\")\naxarr[1].set(xlabel = \"Features\", ylabel = \"Score\", title = \"alpha = 0.001\")\n\n#alpha = 0.01\naxarr[2].plot(trainScoreArray2, label = 'training')\naxarr[2].plot(testScoreArray2, label = 'validation')\nlabels = plt.gca().set(xlabel = \"Features\", ylabel = \"Score\")\naxarr[2].legend(loc=\"lower left\")\nplt.title(\"alpha = 0.01\")\naxarr[2].set(xlabel = \"Features\", ylabel = \"Score\", title = \"alpha = 0.01\")\n\n\nplt.show()     \n   \n\n\n\n\nCompared to standard linear regression, the LASSO algorithm tends to improve the validation score relative to the training score, especially when alpha (regularization strength) is small. For example, in standard linear regression, after about 95 features, the validation score dips into the negatives. Using the LASSO algorithm, the validation score never dips below 0.4, even when alpha is larger. Finally, let’s apply the linear regression algorithm to a real-world dataset.\n\n\n\nThe following code will use data from the Capital Bikeshare system in Washington DC, which incldues the count of bicycle users on each day for two years. The following experiment uses linear regression to see what factors most influence the numer of bike users over time.\n\nbikeshare = pd.read_csv(\"https://philchodrow.github.io/PIC16A/datasets/Bike-Sharing-Dataset/day.csv\")\n\nbikeshare.head()\n\n\n\n\n\n  \n    \n      \n      instant\n      dteday\n      season\n      yr\n      mnth\n      holiday\n      weekday\n      workingday\n      weathersit\n      temp\n      atemp\n      hum\n      windspeed\n      casual\n      registered\n      cnt\n    \n  \n  \n    \n      0\n      1\n      2011-01-01\n      1\n      0\n      1\n      0\n      6\n      0\n      2\n      0.344167\n      0.363625\n      0.805833\n      0.160446\n      331\n      654\n      985\n    \n    \n      1\n      2\n      2011-01-02\n      1\n      0\n      1\n      0\n      0\n      0\n      2\n      0.363478\n      0.353739\n      0.696087\n      0.248539\n      131\n      670\n      801\n    \n    \n      2\n      3\n      2011-01-03\n      1\n      0\n      1\n      0\n      1\n      1\n      1\n      0.196364\n      0.189405\n      0.437273\n      0.248309\n      120\n      1229\n      1349\n    \n    \n      3\n      4\n      2011-01-04\n      1\n      0\n      1\n      0\n      2\n      1\n      1\n      0.200000\n      0.212122\n      0.590435\n      0.160296\n      108\n      1454\n      1562\n    \n    \n      4\n      5\n      2011-01-05\n      1\n      0\n      1\n      0\n      3\n      1\n      1\n      0.226957\n      0.229270\n      0.436957\n      0.186900\n      82\n      1518\n      1600\n    \n  \n\n\n\n\nIn this example, we want to understand trends only for casual users, not registered ones. First, let’s plot casual users over time:\n\n# import datetime\nfig, ax = plt.subplots(1, figsize = (7, 3))\nax.plot(pd.to_datetime(bikeshare['dteday']), bikeshare['casual'])\nax.set(xlabel = \"Day\", ylabel = \"# of casual users\")\nl = plt.tight_layout()\n\n\n\n\nNow, let’s work with a smaller subset of columns:\n\ncols = [\"casual\", \n        \"mnth\", \n        \"weathersit\", \n        \"workingday\",\n        \"yr\",\n        \"temp\", \n        \"hum\", \n        \"windspeed\",\n        \"holiday\"]\n\nbikeshare = bikeshare[cols]\n\nbikeshare = pd.get_dummies(bikeshare, columns = ['mnth'], drop_first = \"if_binary\")\nbikeshare\n\n\n\n\n\n  \n    \n      \n      casual\n      weathersit\n      workingday\n      yr\n      temp\n      hum\n      windspeed\n      holiday\n      mnth_2\n      mnth_3\n      mnth_4\n      mnth_5\n      mnth_6\n      mnth_7\n      mnth_8\n      mnth_9\n      mnth_10\n      mnth_11\n      mnth_12\n    \n  \n  \n    \n      0\n      331\n      2\n      0\n      0\n      0.344167\n      0.805833\n      0.160446\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      131\n      2\n      0\n      0\n      0.363478\n      0.696087\n      0.248539\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      120\n      1\n      1\n      0\n      0.196364\n      0.437273\n      0.248309\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      108\n      1\n      1\n      0\n      0.200000\n      0.590435\n      0.160296\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      82\n      1\n      1\n      0\n      0.226957\n      0.436957\n      0.186900\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      726\n      247\n      2\n      1\n      1\n      0.254167\n      0.652917\n      0.350133\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      727\n      644\n      2\n      1\n      1\n      0.253333\n      0.590000\n      0.155471\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      728\n      159\n      2\n      0\n      1\n      0.253333\n      0.752917\n      0.124383\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      729\n      364\n      1\n      0\n      1\n      0.255833\n      0.483333\n      0.350754\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      730\n      439\n      2\n      1\n      1\n      0.215833\n      0.577500\n      0.154846\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n  \n\n731 rows × 19 columns\n\n\n\nNow that our data is clean, let’s split the data into test data and training data:\n\ntrain, test = train_test_split(bikeshare, test_size = .2, shuffle = False)\n\nX_train = train.drop([\"casual\"], axis = 1)\ny_train = train[\"casual\"]\n\nX_test = test.drop([\"casual\"], axis = 1)\ny_test = test[\"casual\"]\n\nWe can then run linear regression on the bikeshare training data. I chose to run regression using the analytical method, becuase the gradient descent runtime seemed to be much longer.\n\nLR_bike = LinearRegression()\nLR_bike.fit_analytical(X_train, y_train)\n\narray([ -108.37113627,  -791.69054913,   280.58692733,  1498.71511272,\n        -490.10033978, -1242.80038075,  -235.87934918,    -3.35439712,\n         369.27195552,   518.40875345,   537.30188616,   360.80799815,\n         228.88148125,   241.31641202,   371.50385387,   437.60084787,\n         252.43300405,    90.8214605 ,   919.07676215])\n\n\nNow let’s compute the score:\n\nprint(f\"Training score = {LR_bike.score(X_train, y_train).round(4)}\")\n\nTraining score = 0.7318\n\n\nThe training score is fairly high! Now let’s compute the predictions for each day and visualize them in relation to the actual ridership on the test set:\n\nprint(len(y_test)) #147 days will be predicted and then compared\n\n147\n\n\n\ny_pred = LR_bike.predict(X_test) #predicitons on the test data. \n\nfig, ax = plt.subplots(1, figsize = (7, 3))\nax.plot(np.arange(len(y_test)), y_test, label=\"test\") #test data\nax.plot(np.arange(len(y_test)), y_pred, label=\"prediction\") #model prediction data\nax.set(xlabel = \"Day\", ylabel = \"Casual Riders\")\nl = plt.tight_layout()\n\nlegend = plt.legend()\n\n\n\n\nOur predicitons (orange) coorespond pretty well with the test data (blue). Now, we will compare the entries w to the entries in X_train.columns to see which features our model thinks best predict ridership.\n\nLR_bike.w\n\narray([ -108.37113627,  -791.69054913,   280.58692733,  1498.71511272,\n        -490.10033978, -1242.80038075,  -235.87934918,    -3.35439712,\n         369.27195552,   518.40875345,   537.30188616,   360.80799815,\n         228.88148125,   241.31641202,   371.50385387,   437.60084787,\n         252.43300405,    90.8214605 ,   919.07676215])\n\n\n\nX_train.columns\n\nIndex(['weathersit', 'workingday', 'yr', 'temp', 'hum', 'windspeed', 'holiday',\n       'mnth_2', 'mnth_3', 'mnth_4', 'mnth_5', 'mnth_6', 'mnth_7', 'mnth_8',\n       'mnth_9', 'mnth_10', 'mnth_11', 'mnth_12'],\n      dtype='object')\n\n\nThe most negative coeficient is windspeed, and the most positive coeficient is temperature. Thus, windspeed is the factor that most deters causal useres from biking, and temperature is the one that most entices these users to biking. Our model predicts that people want to bike more when it is warm outside, and less when it is windy. Our model also identifies factors such as workday and humidity as having negative relationships with the number of casual riders. Thus, we predict that less people are likely to bike outside of the weekends, and also if humidity is high. On the flip side, our model predicts a positive coorelation between the number of casual riders and the months of March and April, indicating that casual riders might be more likely to bike and get outside once the weather starts becoming nicer in the springtime. Cool!"
  },
  {
    "objectID": "posts/CSCI04/regression_blog/LogisticRegressionBlogPost.html",
    "href": "posts/CSCI04/regression_blog/LogisticRegressionBlogPost.html",
    "title": "Optimization for Logistic Regression Blog",
    "section": "",
    "text": "The following blog post will implement gradient descent for logistic regression in an object-oriented paradigm and implement a key variant of gradient descent called stochastic gradient descent. It will then perform several simple experiments on synthetic data to see which of these algorithms converges most quickly to a satisfactory logistic regression model.\nTo view the source code, pelase follow this link: https://github.com/madgallop/madgallop.github.io/blob/main/posts/regression_blog/logistic.py\n\n\nLinear regression is an approach to modeling the relationship between independent variable/s and a dependent variable. It is the best-fit regression line to describes data with minimal error, as measured by loss. Gradient descent and the algorithm in this blogpost aim to minimize the loss function.\n\n\n\nIn order to implement this algorithm, I built up tools and concepts we discussed in class. I used functions from the lecture on Convex Linear Models and Logistic Regression (predict, sigmoid, loss), and adapted the score method from the perceptron blogpost. The primary challenge of this assignment was implementing the gradient. I first time to spend some time figuring out what the gradient descent function actually is for linear regression. I first tried to implement a partial deriatives approach to find the slope and intercept, but eventaully found the key to be in equation below from the Optimization with Gradient Descent lecture. I finally understood what is happening in gradient descent of logisitic regression, and was able to replicate the equation in code.\n\n\n\nScreen Shot 2023-03-03 at 7.35.26 PM.png\n\n\nThe other challenging part of this assignment was formatting vectors and matrices to have dimensions that allowed for multiplication and other operations. I had to always be asking myself what dimension matrices certain lines of code (like the gradient) were taking in and spitting out, and adjusting the algorithm with lines such as y[:,np.newaxis] and .T to transform matrices.\n\n\n\nBefore beginning, import all the required packages:\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n{'divide': 'ignore', 'over': 'ignore', 'under': 'ignore', 'invalid': 'ignore'}\n\n\nScript to autoload logistic.py file\n\n%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\nVisualize the example data\n\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nNow, fit the model\n\nfrom logistic import LogisticRegression \nLR = LogisticRegression()\nLR.fit(X, y, alpha = 1, max_epochs = 1000)\n\n# inspect the fitted value of w\nprint(LR.w)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\nprint(LR.w.shape)\n\nplt.plot(f1, (LR.w[2] - f1*LR.w[0])/LR.w[1], color = \"black\")\nplt.title(\"Gradient Descent\")\n\n[[ 2.01961227]\n [ 2.46881413]\n [-0.32132074]]\n(3, 1)\n\n\nText(0.5, 1.0, 'Gradient Descent')\n\n\n\n\n\nAs you can see, the model performs fairly well. Let’s take a look at the evolution of the loss over time, as well as the overall accuracy.\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \nplt.title(\"Loss History of Gradient Descent\")\n\nText(0.5, 1.0, 'Loss History of Gradient Descent')\n\n\n\n\n\n\nLR.score(X,y)\n\n0.95\n\n\nGradient descent computes a fairly accurate value of the parameter vector w. Because the algorithm mainly utilized numpy operations instead of loops, it is efficient, and as the loss history graph depicts, starts to converge. Let’s see if we can improve upon this algorithm by altering batch size!\n\n\n\nStochastic gradient descent computes the stochastic gradient by computing the gradient on subsets of the data. The algorithm works as follows: 1. Shuffle the points randomly. 2. Pick the first k random points, compute the stochastic gradient, and then perform an update. 3. Pick the next k random points and repeat.. 4. When we have gone through all n points, reshuffle them all randomly and proceed again.\n\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X, y, \n                  alpha = .05, \n                  batch_size = 100,\n                  m_epochs = 100\n                  )\n# inspect the fitted value of w\nprint(LR2.w)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\nplt.plot(f1, (LR2.w[2] - f1*LR2.w[0])/LR2.w[1], color = \"black\")\nplt.title(\"Stochastic Gradient Descent\")\n\n[[1.38594862]\n [1.58157147]\n [0.35018386]]\n\n\nText(0.5, 1.0, 'Stochastic Gradient Descent')\n\n\n\n\n\nThe model fits the data quite well. Let’s examine the loss history and accuracy again!\n\nnum_steps = len(LR2.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"stochastic gradient\")\nplt.title(\"Loss History Stochastic Gradient Descent\")\n\nText(0.5, 1.0, 'Loss History Stochastic Gradient Descent')\n\n\n\n\n\n\nLR2.score(X,y)\n\n0.935\n\n\nThe loss history chart bounces around more than for typical gradient descent, but gives us a good sense of how the stochastic gradient affects the regression over time. The algorithm is quite accurate as well! As I’ve learned, stochastic gradient descent tends to get to a “pretty good” result faster than standard gradient descent, but can “bounce around” near the good solution. Standard gradient descent might need more epochs to find a good solution, but quickly “settles down” once it finds it.\n\n\n\n\nLR_01 = LogisticRegression()\n\nLR_01.fit(X, y, alpha = .01, max_epochs = 10000)\nnum_steps01 = len(LR_01.loss_history)\n\nLR_1 = LogisticRegression()\n\nLR_1.fit(X, y, alpha = 1, max_epochs = 1000)\nnum_steps1 = len(LR_1.loss_history)\n\nLR_100 = LogisticRegression()\n\nLR_100.fit(X, y, alpha = 100, max_epochs = 100)\nnum_steps100 = len(LR_100.loss_history)\n\n\nfig, axarr = plt.subplots(1,4, layout = \"tight\", figsize = (12,6))\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = \".01\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (LR_01.w[2] - f1*LR_01.w[0])/LR_01.w[1], color = \"black\")\n\naxarr[1].plot(np.arange(num_steps01) + 1, LR_01.loss_history, label = \"Loss alpha = .01\")\naxarr[1].loglog()\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \".01\")\n\naxarr[2].plot(np.arange(num_steps1) + 1, LR_1.loss_history, label = \"Loss alpha = 1\")\naxarr[2].loglog()\naxarr[2].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"1\")\n\naxarr[3].plot(np.arange(num_steps100) + 1, LR_100.loss_history, label = \"Loss alpha = 100\")\naxarr[3].loglog()\naxarr[3].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"100\")\n\n[Text(0.5, 0, 'Iteration number'),\n Text(0, 0.5, 'Empirical Risk'),\n Text(0.5, 1.0, '100')]\n\n\n\n\n\nThis example illustrates the loss history for different learning rate choices. When the learning rate is small, the algorithm converges relatively slowely. With a larger alpha value (such as 1), the algorithm converges with fewer iterations, and in a more direct manner. And when alpha is very large, say, 100, the algorithm fails to converge, and fails rather fast.\n\n\n\nCreate a new data set with 10 features\n\np_features = 10\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nRun stochastic gradient descent, investigating how choice of batch size affects how fast the algorithm converges\n\nLR_ex2 = LogisticRegression()\nLR_ex2.fit_stochastic(X, y, alpha = .01, batch_size = 2, m_epochs = 1000)\nnum_steps2 = len(LR_ex2.loss_history)\n\nLR_ex10 = LogisticRegression()\nLR_ex10.fit_stochastic(X, y, alpha = .01, batch_size = 10, m_epochs = 1000)\nnum_steps10 = len(LR_ex10.loss_history)\n\nLR_ex100 = LogisticRegression()\nLR_ex100.fit_stochastic(X, y, alpha = .01, batch_size = 100, m_epochs = 1000)\nnum_steps100 = len(LR_ex100.loss_history)\n\nLR_ex5000 = LogisticRegression()\nLR_ex5000.fit_stochastic(X, y, alpha = .01, batch_size = 5000, m_epochs = 1000)\nnum_steps5000 = len(LR_ex5000.loss_history)\n\n\nfig, axarr = plt.subplots(1,4, layout = \"tight\", figsize = (15,6))\n\naxarr[0].plot(np.arange(num_steps2) + 1, LR_ex2.loss_history, label = \"2\")\naxarr[0].loglog()\naxarr[0].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"2\")\n\naxarr[1].plot(np.arange(num_steps10) + 1, LR_ex10.loss_history, label = \"Loss alpha = 10\")\naxarr[1].loglog()\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"10\")\n\naxarr[2].plot(np.arange(num_steps100) + 1, LR_ex100.loss_history, label = \"Loss alpha = 100\")\naxarr[2].loglog()\naxarr[2].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"100\")\n\naxarr[3].plot(np.arange(num_steps5000) + 1, LR_ex5000.loss_history, label = \"Loss alpha = 5000\")\naxarr[3].loglog()\naxarr[3].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\", title = \"5000\")\n\n[Text(0.5, 0, 'Iteration number'),\n Text(0, 0.5, 'Empirical Risk'),\n Text(0.5, 1.0, '5000')]\n\n\n\n\n\nAs the batch size increases, the model works through more training samples before updating its parameters. Thus, really large batch sizes converge faster without as much “bouncing” as compared to smaller batch sizes. While still displaying a downward trend, smaller batch sizes bounce around much more and converge at a slower rate."
  },
  {
    "objectID": "posts/CSCI04/projectBlog/project_blog.html",
    "href": "posts/CSCI04/projectBlog/project_blog.html",
    "title": "Wildfire Prediction Project Blog Post",
    "section": "",
    "text": "Source code: https://github.com/ebwieman/wildfire-risk-tool/blob/main/main.ipynb\nMapping tool: https://github.com/ebwieman/wildfire-risk-tool/blob/main/wildfire_risk_mapping_tool.ipynb"
  },
  {
    "objectID": "posts/CSCI04/projectBlog/project_blog.html#abstract",
    "href": "posts/CSCI04/projectBlog/project_blog.html#abstract",
    "title": "Wildfire Prediction Project Blog Post",
    "section": "Abstract",
    "text": "Abstract\nWildfires have increased in recent years as a result of climate change, posing a threat to humans and the environment. Understanding the conditions that lead to wildfire and which areas are most at risk is important for developing mitigation strategies and allocating resources. In this project, we utilize machine learning algorithms to predict both wildfire occurrence and area of wildfires. To predict wildfire occurrence, we trained several machine learning models on a dataset containing meteorological information about potential wildfires in Algeria. An accuracy of 100% was achieved using a logistic regression model trained on all features. To predict wildfire area, models were trained on a dataset of wildfire events from Montesinho National Park, Portugal. The area prediction task was much more difficult than the classification task and required additional label transformation to achieve higher accuracies. The highest area prediction accuracy was achieved with a logistic regression model trained on all features and using labels transformed with the scikit-learn lab encoder. Lastly, a model trained on the Algerian dataset was used to predict and map wildfire risk in the United States. The trained model was more simplistic than other models due to the lack of US meteorological data available, but visual comparison to existing fire prediction tools such as the USGS Fire Danger Map Tool shows that the model was somewhat able to predict fire risk in the United States. One shortcoming of this work is that all datasets used to train our models were small and regionally specific, making it difficult to create generalizable tools for use on larger scales or different regions. Developing larger and more regionally dispersed wildfire datasets will aid in future creation of more robust fire prediction tools."
  },
  {
    "objectID": "posts/CSCI04/projectBlog/project_blog.html#introduction",
    "href": "posts/CSCI04/projectBlog/project_blog.html#introduction",
    "title": "Wildfire Prediction Project Blog Post",
    "section": "Introduction",
    "text": "Introduction\nClimate change has increased the likelihood of a variety of extreme weather events, including wildfires. These extreme events pose definite risks to both human and ecological communities. At the same time, machine learning is emerging as an important predictive tool in natural disaster prevention; numerous studies have already used machine learning to classify risk of natural hazards. For example, Youssef et al. used machine learning models to predict an area’s susceptibility to landslides, floods, and erosion in Saudi Arabia (Youssef et al. 2023). This study experimented with a few different models, ultimately settling on Random Forest. In Choubin et al.’s study of avalanches in mountainous regions, Support Vector Machine (SVM) and Multivariate Discriminant Analysis were found to be the best models in assessing avalanche risk based on meteorological and locational data (Choubin et al. 2019).\nPrior studies have also focused specifically on wildfire prediction. A 2023 paper even developed a new model to better predict burned areas in Africa and South America (Li et al. 2023). In addition, the dataset used in our project to predict Portuguese fires was originally the topic of a 2007 paper focusing on training models on readily available meteorological data to predict the area of wildfires (Cortez and Morais 2007). This study also experimented with different predictive features and models, finding SVM and random forest to be the best predictors (Cortez and Morais 2007).\nIn this project, we build on these prior studies, using many similar techniques and models. We use two datasets to predict wildfire likelihood and wildfire area given meteorological data. Our first dataset, the Algerian Forest Fires Dataset, contains fire and non-fire events with 11 attributes that describe weather characteristics at the time of that event. Our second dataset, the Forest Fires Dataset, contains data from Montesinho National Park, Portugal. This dataset contains many of the same weather characteristics as the Algerian dataset, but contains only fire events, and also includes the areas of these events. Rather than using this dataset to predict whether a fire occurred in a given area, we used it to predict the size a fire is likely to reach given certain meteorological conditions.\nOver the course of this project, we followed in the footsteps of prior studies, experimenting with different models to predict risk. We implemented several existing machine learning models such as Random Forest and Logistic Regression, ultimately choosing the models and sets of features that yielded the highest accuracy score. We trained and validated models on the Algerian dataset to predict whether or not a forest fire will occur given certain meteorological conditions and also trained/validated models on the Portugal dataset to predict forest fire area given many of the same features.\nAlthough we trained our models with localized data, we also assessed the extent to which we could apply our models across datasets and geographies. Similarly to the multi-hazard susceptibility map produced by Youssef et al., we created a fire susceptibility map of fire risk using our models and county-level temperature and precipitation data for the entire United States (Youssef et al. 2023). While we cannot assess the accuracy of this map directly, we can compare it to existing US wildfire prediction tools to at least visually assess our accuracy.\nFinally, we build on existing research to assess the ethics of using machine learning models in predicting risk of natural hazards. As Wagenaar et al. caution in their 2020 study of how machine learning will change flood risk and impact assessment, “applicability, bias and ethics must be considered carefully to avoid misuse” of machine learning algorithms (Wagenaar et al. 2020).\nAs extreme weather events become more prevalent with climate change, we must learn how to best predict and manage these events. The methods explored detail our own attempt to do so."
  },
  {
    "objectID": "posts/CSCI04/projectBlog/project_blog.html#values-statement",
    "href": "posts/CSCI04/projectBlog/project_blog.html#values-statement",
    "title": "Wildfire Prediction Project Blog Post",
    "section": "Values Statement",
    "text": "Values Statement\nBeing able to accurately predict the risks of wildfires offers numerous benefits for various stakeholders. Predictive models like ours have the potential to revolutionize wildfire management and mitigation strategies, leading to better preparedness, timely response, and ultimately, the protection of lives and property. Thus, we wanted to build upon prior studies and further the ability to accurately predict wildfires.\nThe main beneficiaries of this project are the wildfire departments and emergency responders. These agencies can gain valuable insights into the probability and severity of wildfire occurrences in specific areas though our project. These predictions allow for better allocation of resources and more effective mitigation strategies, reducing the overall damage and saving tons of valuable resources. If we can assess areas with high-risk for wildfires ahead of time, they could be better equipped and protected.\nInsurance companies can also leverage our predictive models to assess risks associated with wildfires. By integrating these algorithms into their underwriting processes, insurers can accurately evaluate the potential impact of wildfires on properties, allowing for more precise risk assessment. Moreover, the models can aid in post–fire analysis, enabling insurance companies to provide timely assistance to policyholders living in at-risk areas.\nGovernment agencies responsible for land and forest management are also another one of our potential users. Policymakers can make more informed decisions regarding land use, forest management, and allocation of resources for fire prevention efforts. Ultimately, this can lead to more proactive measures such as improved firebreak construction and targeted vegetation management.\nAnother group that may not directly utilize our predictive wildfire models, but still stands to benefit is the general population. Information that trickles down from fire management authorities can help individuals living in fire-prone areas make informed decisions and take the necessary precautions. People are able to take property protection measures and implement evacuation plans they deem necessary. Overall, the predictions from these models can increase safety measures while minimizing property damage within the predicted at-risk areas.\nWhile the use of machine learning models to predict wildfire risks can be highly beneficial, there are potential downsides with their implementation that should be considered. Our models rely heavily on well documented weather and geographical data. Data collection takes time and money, which could potentially be a barrier to using our models. Remote areas without government or research funding most likely will not be able to produce the data needed to benefit from our models. Moreover, communities lacking internet access, computing devices, or technological literacy are unable to take advantage of our models. This can disproportionately affect rural areas and low-income communities, further exacerbating existing inequalities.\nAdditionally, we recognize that our models are trained and tested on data that is in English. This language barrier can hinder individuals who don’t have the proficiency in the language, limiting their ability to use these predictive models. Additionally, cultural differences and contextual nuances might not be well captured in our models, leading to potential misunderstandings and biases. Thus, we want to be mindful of the potential barriers and hope to address these shortcomings in our future work. Failing to address these disparities could perpetuate social inequalities in wildfire management.\nUpon reflecting these potential harms and benefits, we still believe this project will improve wildfire management and be a crucial resource to communities in fire-prone areas. Additionally, this project furthers our understanding of factors contributing to wildfires. With this information, we can accurately predict the likelihood of wildfires in a given region, enabling better fire management, mitigation, and evacuation. Wildfire predictions can help protect the land, wildlife, and local communities. Still, efforts should be made to actively involve marginalized groups in wildfire preparedness and response initiatives."
  },
  {
    "objectID": "posts/CSCI04/projectBlog/project_blog.html#materials-and-methods",
    "href": "posts/CSCI04/projectBlog/project_blog.html#materials-and-methods",
    "title": "Wildfire Prediction Project Blog Post",
    "section": "Materials and Methods",
    "text": "Materials and Methods\n\nThe Datasets\nIn this project, we utilized various machine learning models from scikit-learn to better understand wildfire occurrences. Specifically, we trained and tested on two datasets to predict wildfire likelihood and wildfire area given the meteorological and geographical features. These datasets were taken from the University of California Irvine Machine Learning Repository. Our first dataset, the Algerian Forest Fires Dataset, contains fire and non-fire occurrences with 11 attributes describing weather characteristics at the time the event occurred. Our second dataset, the Forest Fires Data, has data from Montesinho National Park, Portugal. This dataset contains many of the same weather attributes as the Algerian one. However, rather than using this dataset to predict whether a fire occurred in a given area, it can be used to predict the size of the fire given certain meteorological and geographical conditions.\nThe Algerian dataset has 224 instances that regrouped data from two regions in Algeria, namely the Bejaia region located in the northeast of Algeria and the Sidi Bel-abbes region in the northwest of Algeria. There are 122 instances for each of the two regions. This data was collected from June 2012 to September 2012. As stated, the dataset contains fire and non-fire events with 13 attributes that describe weather conditions. The 13 features are: day, month, year, temperature in Celsius, relative humidity, wind speed, rain, fine fuel moisture code, duff moisture code, drought code, initial speed index, build up index, and fire weather index. Our target label is “fire”, with a label of 1.0 meaning that a fire has occurred and 0.0 being no fire.\nThe data collected in the Portuguese dataset was taken from Montesino park in the northeast region of Portugal. The 12 features in this dataset are similar to the ones in the Algerian dataset. These features include: x-axis spatial coordinate, y-axis spatial coordinate, month, day, fine fuel moisture code, duff moisture code, drought code, initial speed index, temperature in Celsius, relative humidity, wind, and rain. The target label is “fire area”, which determines the total burned area of a fire given the meteorological and geographical conditions.\nWe used RidgeCV from scikit-learn to pull out five important features in each dataset that we later trained on. RidgeCV is a cross validation method in ridge regression. The higher absolute coefficient, the more important that feature is. After applying RidgeCV to the Portuguese dataset, we yielded the five most important features: x-axis spatial coordinate, y-axis spatial coordinate, initial speed index, temperature, and wind. The five most important features for the Algerian dataset are: relative humidity, rain, fine fuel moisture code, initial speed index, and fire weather index.\n\n\nModeling\nBoth datasets were trained on various algorithms taken from scikit-learn and each model was analyzed based on their accuracy score.\nThe Algerian training data was trained on four different algorithms: decision tree, logistic regression, random forest, and stochastic gradient descent. For each of the algorithms, we trained the Algerian training set with its complete set of 13 features and also with the 5 features selected via our feature selection process. Additionally, we used cross validation to evaluate the models’ performance at different settings and to avoid overfitting. All five algorithms performed relatively well on the training data, with the accuracy score ranging from 0.552 to 0.990. Training the complete set of features on the stochastic gradient descent classifier yielded the lowest score of 0.552. Alternatively, training on the logistic regression classifier with the complete set of features gave us the highest accuracy of 0.990. Therefore, we went ahead and used the logistic regression model to test our Algerian data on.\nAs for the Portuguese dataset, we started by training it on the linear regression model and support vector machine for regression, but got very low accuracy scores of 0.048 and -0.026, respectively. It was much harder to train on this dataset because the labels were non-binary and many were skewed towards 0’s. We then performed a log-transform on our labels, hoping to improve the accuracy. However, this did not make much of a difference. The highest score we got after log-transforming our labels and training on a linear regression model is -0.020. After this, we went ahead and transformed our labels through three different methods. First, we used the lab encoder in the scikit-learn preprocessing package to encode the target labels with values between 0 and n number of classes - 1. Second, we divided the labels into 50 different ranges. Last, we transformed the labels into binary labels, making the areas larger than 0.0 as “non 0’s” and keeping labels of 0.0 as “0.0”. While we recognize that this method is not ideal, it was very difficult training the Portuguese dataset since it is a very small dataset and most of the target labels are skewed towards 0.0.\nIn the end, we trained our dataset on four different models: linear regression, logistic regression, support vector machine for classification, and support vector machine for regression. Each model was trained on different combinations of features and transformed target labels. We trained the model on its complete set of features, on the features selected via our feature selection process, and on the features highlighted in the research paper written by (Cortez and Morais 2007). In their paper, they also experienced difficulty training this Portuguese dataset. Ultimately, transforming our labels via the lab encoder got us a training score of 0.513 when trained on a logistic regression model using the complete set of features. Unsurprisingly, changing our y into binary labels and into ranges got us a perfect training accuracy of 1.0. Thus, we decided to test our Portuguese data on the logistic regression model with the transformed target labels.\n\n\nUS Meteorological Data and Mapping Tool\nCounty level precipitation and temperature data was downloaded from the NOAA NClimGrid dataset (Durre 2023). Specific months of interest were selected and data was downloaded for that month as two csvs, one containing precipitation data and one containing temperature data. The precipitation data contained total precipitation in mm for each day of the month for each US county, while the temperature data contained maximum temperature in Celsius for each day. Significant time was spent searching for additional meteorological data, specifically wind speed and humidity, but these efforts were ultimately unsuccessful. The temperature and precipitation datasets were uploaded to GitHub and then loaded into the Mapping Tool Jupyter notebook. Substantial data cleaning was performed to prepare the US data for prediction. Data cleaning tasks included renaming columns after the datasets were read in and resolving discrepancies in the state and county codes so that data frames could be merged by county. The precipitation and temperature datasets were read in, converted to pandas dataframes, and merged. From this dataframe, data was extracted for a specified day and county-level wildfire predictions were generated for that day. Predictions were generated using a new Random Forest model that was trained on the Algeria dataset using only Temperature and Rain as features, as these were the only features we had access to for US data. These predictions were then joined with a dataframe containing geological coordinates for each county and then the predictions were mapped. The final algorithm used for mapping allows the user to put in a date and state and then produces a map of county-level fire risk for that state on the specified day. If there is no data for the specified day, the algorithm returns a message apologizing for the lack of data. If no state is specified, the algorithm returns a map of the entire continental US. While the maps could not be rigorously assessed for accuracy, visual comparison to the USGS Fire Danger Map Tool was used to discuss the accuracy of our Algerian model when applied to US data (Eidenshink 2023)."
  },
  {
    "objectID": "posts/CSCI04/projectBlog/project_blog.html#results",
    "href": "posts/CSCI04/projectBlog/project_blog.html#results",
    "title": "Wildfire Prediction Project Blog Post",
    "section": "Results",
    "text": "Results\nWe came close to achieving all of our goals for this project. Our greatest success was our modeling of the Algerian dataset. We achieved high training accuracy with several models, including > 95% training accuracy for a Random Forest Classifier, Logistic Regression, and Decision Tree Classifier, as well as 100% testing accuracy with a Logistic Regression model. It’s difficult to say how applicable this model is outside of the region of Algeria the data are from. The random forest classifier that we trained on the Algerian model for our mapping tool got many things right – it predicts more widespread fire in the summer months, for example, than in the winter – but we trained it on many fewer features than are in the full Algerian dataset.\nThis is certainly one of the shortcomings of our project. The lack of weather data available for the United States means that we can’t fully say how well our model would perform on data from a region other than Algeria. However, there are positive trends, and were more weather observations available, it seems likely that our model would have some applicability.\nDespite the fact that we did not get to realize the full abilities of our model, creating an interactive map is nonetheless a highlight of the project. It shows that our project has the ability to be broadly applicable, and underscores the power of Machine Learning as a tool for natural hazard prediction and prevention. We explored the ethical dimensions of this application of Machine Learning in a short essay, and while there are certainly ethical considerations that must be made when training models to predict natural hazards, this project shows that there is also a great deal of opportunity to use Machine Learning to predict and manage risks from natural hazards.\nWhile we cannot assess the accuracy of our US prediction tool directly, visual comparison to existing mapping tools such as the USGS Fire Danger Map Tool yields some insights. If we compare the two tools’ predictions for March 2, 2023, we see that our tool vastly overpredicts fire risk in the US (Figure 1). Our tool correctly predicts fire in the locations that the USGS identifies the most high risk, specifically Southwestern Texas and Arizona, but we also predict risk of fire in large swaths of the Midwest and Florida that the USGS tool does not deem particularly high risk.\n \n\nFig 1: Comparison of fire risk predictions by our mapping tool (top) and the USGS Fire Danger Map Tool (bottom) for March 2, 2023.\n\nNext we look at a potentially more interesting case in July 2020, the peak of summer, when we would expect more areas of the country to be susceptible to wildfires. Here the USGS predicts risk of fire in much of the Southwestern United States, and we see that our model does as well. However, we once again see overprediction by our model, specifically in the Midwest. One possible contributing factor is that the USGS model differentiates between agricultural land and does not predict fire risk in these areas. Our model seems to predict fire risk in many of the areas deemed agricultural by the USGS model, so differentiating between land cover classes could be useful for future development of this tool. Additionally, our model predicts on the county scale, while the USGS mapping tool appears to have much better resolution, allowing it to make more refined predictions. One other exciting observation in the July maps is that while our model tends to make generally uniform predictions across states, we do see some agreement between the two tools in identifying pockets of low fire risk within the Southwestern United States. The fact that our mapping tool tends to make relatively uniform predictions across states suggests that the model could just be learning general weather patterns rather than actually learning fire risk. Training on additional features would likely help address this problem, but this is impossible at the moment due to the lack of aggregated US weather data available.\n \n\nFig 2: Comparison of fire risk predictions by our mapping tool (top) and the USGS Fire Danger Map Tool (bottom) for July 2, 2020.\n\nThe final area of our project to discuss is the models we trained for the Portuguese dataset. Replicating the process of the original academic paper didn’t yield results of the same accuracy as those obtained by the study (Cortez and Morais 2007). Log-transforming the data was a complex process that was perhaps more involved than we had anticipated. However, we still managed to achieve about 50% accuracy with both Logistic Regression and Support Vector Machine models, and 100% accuracy when we transformed to binary labels and categorical labels. Figure 3, shown below, shows the results the model achieved on 20 features from the test set:\n\n\n\nimage\n\n\n\nFig 3: While the model correctly predicts 35% of fire areas, it gets close on a number of them, so 35% is not reflective of the actual performance. We see that the model does the best at predicting small fires, though for medium-sized fires, it usually either far overshoots or far undershoots in its prediction.\n\nFigure 4 shows the same observations, predicted using the SVC model. It also gets 35% correct, but unlike the LR model, it only ever underestimates the area of fires, while the LR model overestimates several. This suggests that the SVC model is slightly more conservative, and may fail to predict the largest fires.\n\n\n\nimage\n\n\n\nFig 4: The SVC model seems to be more conservative, predicting the area of all fires – even those that are larger in size – to be 0 ha or close to it.\n\nThese mixed results for the Portuguese data, along with our high testing accuracy for the Algeria dataset, shows that predicting whether or not a fire occurred is a much more straightforward task than predicting the area of the fire. There is certainly room to grow in our modeling of this dataset; perhaps trying different models or features would yield different results. We recognize that transforming the target labels into categorical and binary labels isn’t ideal; however, it was too difficult otherwise. This was because the Portuguese dataset is a very small dataset with the majority of the target labels skewed towards 0.0. Thus, this problem was turned into a classifying problem instead of producing an accurate predictor model. Nonetheless, our process and results show that it is possible to build a model trained on readily-available weather observations that predicts the area of fires with a reasonable degree of accuracy."
  },
  {
    "objectID": "posts/CSCI04/projectBlog/project_blog.html#concluding-discussion",
    "href": "posts/CSCI04/projectBlog/project_blog.html#concluding-discussion",
    "title": "Wildfire Prediction Project Blog Post",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nOverall, our project was successful in a number of ways, and opens the door for future research and experimentation. Our analysis of wildfire risk provided us with an opportunity to practice data science. We successfully found regional data sets, cleaned them, and prepared them for machine learning analyses. Using a Ridge model, we successfully selected features to use for making predictions in each dataset, and through experimenting with a variety of different machine learning models, we selected two that reached reasonably high accuracy. Through examining United States county-level data, we were able to apply our models across scales. Finally, we visualized our results well, and effectively discussed their ethical implications.\nAs a group, we met and even surpassed our original project goals. We have a code repository that details our data preparation and cleaning, model training and prediction, and evaluation, notebooks with in exploratory data analysis and testing of our models, a short essay about the implications of using machine learning and automated tools for ecological forecasting (both risks and benefits) and a map of wildfire risk in the United States constructed using our models and US meteorological data. We had wanted to use machine learning to help with natural hazard risk detection and prevention, and our final product provides detailed and thorough documentation of our attempt to do so.\nBecause our project built on the work of scholars such as Cortez and Morias, we have the ability to compare our results to past work. While Cortez and Morias’ 2007 study that explored the Portugal data we used in our project identified SVM to be the best machine learning model for fire area prediction, we found SVC and Logistic Regression to be equally effective on the Portugal data set (Cortez and Morais 2007). Our work weaves in well with prior studies, allowing us to make predictions about natural risk by choosing and applying machine learning models. Overall, through experimenting with different models, we ultimately were able to predict wildfires and wildfire area more accurately than our base rates, and experimented with applying these models across scales. With more time, data, or computational resources, we could improve our methods and findings. Spending more time finding and downloading more data for the United States (such as wind speed and humidity, prehaps) would likely improve our map and our model and allow us to better apply the model across space. While we trained our machine learning models on regionally-specific data, we applied thier predictions to new and differently-scaled geographies. When applying models across space, it is important to do so in collaboration those people who may be affected by the results of predictive machine learning tools (Wagenaar et al. 2020). Overall, when we pay attention to bias and applicabiltiy, wildfire and natural disaster models have numerous applications, and even the potential to save lives. As the likelihood and intensity of extreme weather events continues increasing in the face of climate change, we should incorporate and continue building on the anaysis presented here to acheive even better methodologies to predict and prepare for these events."
  },
  {
    "objectID": "posts/CSCI04/projectBlog/project_blog.html#group-contributions-statement",
    "href": "posts/CSCI04/projectBlog/project_blog.html#group-contributions-statement",
    "title": "Wildfire Prediction Project Blog Post",
    "section": "Group Contributions Statement",
    "text": "Group Contributions Statement\nMadeleine: Near the beginning of the project, Wright and I worked on preparing the data from the Algeria and Portugal data sets to eventually split into testing and training data. I then worked on selecting features important for fire/fire area prediction using RidgeCV. We then worked on visualizing the relationships between these selected features and fire likelihood/area. Later in the project, I worked with Eliza on our map. Specifically, I worked on rendering the map to show all the counties in the US, and and helped prepare to join the counties on the map to the county-level temperature and precipitation data. I also worked on the map’s design. Finally, I tried to find additional weather data for the US (such as wind speed), but was largely unsuccessful.\nWright: I worked with Madeleine at the beginning of the project doing some data preparation. Specifically, I worked on formatting the Algeria data set so that it could be treated as binary fire/no fire data. I then worked on data vizualizations, making graphs of the relationships between different features and fire occurrence/scale. I wasn’t directly involved with the modeling, but after the models were trained I ran them on some test observations and vizualized their performance. Finally, I worked on a short research essay about the ethics of using Machine Learning for natural hazard prediction, prevention, and adaptation.\nEliza: I helped with initial downloading and importing of the Portugal and Algeria datasets and splitting them into train and test sets. I initially worked on training models on the Portugal dataset (with little success) and did some research to figure out how the authors of the paper that the dataset originated from achieved higher accuracies. I did some of the initial experimentation with log-transforming our y values, but then I pivoted to working on the mapping tool and Nhi took this part of the project over. I definitely spent most of my time and effort working on the mapping tool for this project with Madeleine. I did extensive research on US county-level meteorological data and identified and downloaded the nClimGrid dataset that we ended up using for our US predictions. I did a lot of the data cleaning of these datasets prior to generating our maps, which Madeleine took the lead on, and I helped streamline the map-making process after creating our first map by converting a lot of our code into reusable functions. For the blog post, I took lead on the Materials and Methods section with Nhi, specifically focusing on the mapping tool methods, and the Abstract.\nNhi: Towards the beginning of the project, I helped with cleaning up our datasets and making sure they are ready to be trained on. For the majority of the project, I did a lot of the training and testing for both of the Portuguese and Algerian datasets. The Algerian dataset was a lot easier to train on, and I was able to obtain a very good accuracy score early on. I trained the Algeria dataset on different models and with different combinations of features to see which would yield the highest accuracy. I did a lot of cross validation and fine tuning to make sure the models weren’t overfitting. Then I tested the highest scoring model on the Algerian testing dataset. Eliza started with training the Portuguese dataset, but did not get very far since this was a much harder dataset to train on. Thus, I took over the modeling for this as well. I started by training the dataset on other models since Eliza had only trained on two models to start. I wasn’t very successful with the other models either, so I explored different ways to transform the target labels since the majority of the labels were skewed towards 0’s. I did a lot of digging around on scikit-learn and while not ideal, I transformed the labels via lab encoder, into binary labels, and into different ranges of the target labels. I then trained different combinations of the features and transformed target labels on different models. I analyzed these models to see how they performed against the testing dataset. Overall, I dissected a lot of different variations of the features and target labels. Additionally, I also combed through our code and cleaned them up. I also added a lot of the descriptors before and after each code section to explain our processes. For the blog post, I wrote the Values Statement, the dataset and modeling in the Materials and Methods section, and added a bit to the Results."
  },
  {
    "objectID": "posts/CSCI04/projectBlog/project_blog.html#ethics-of-ml-as-a-tool-for-natural-hazard-prediction-prevention",
    "href": "posts/CSCI04/projectBlog/project_blog.html#ethics-of-ml-as-a-tool-for-natural-hazard-prediction-prevention",
    "title": "Wildfire Prediction Project Blog Post",
    "section": "Ethics of ML as a tool for Natural Hazard Prediction & Prevention",
    "text": "Ethics of ML as a tool for Natural Hazard Prediction & Prevention\nMachine Learning is emerging as an important predictive tool in natural disaster prevention. Numerous studies have used machine learning to classify risk of natural hazards, from landslides, floods, and erosion in Saudi Arabia (Youssef et al. 2023), to avalanche risk in mountainous regions (Choubin et al. 2019), to area of forest fires in the tropics (Li et al. 2023). In fact, the Portuguese fire data used in this project was originally the topic of a 2007 paper focusing on training models on readily available real-time meteorological data to predict the area of wildfires (Cortez and Morais 2007).\nSo, there is a growing library of academic literature and projects using Machine Learning models to predict either the occurrence or the scale of natural hazards. Our project builds on these works, using many similar techniques and models. As our high testing accuracy for the Algerian dataset shows, machine learning clearly can be a powerful tool in natural hazard prediction.\nBut this numerical, scientific, approach comes with risks. As Cortez and Morais note in their study of Monteshino National Park in Portugal, most fires are caused by humans, which is why features like day of the week are important. Natural hazards only become natural disasters when they have a negative impact on people’s lives. As Youseff et al. observe in their study of multi-hazard prediction in Saudi Arabia, multiple hazards often occur at the same time, and the impact of natural hazards disproportionately affects impoverished and underdeveloped countries (Youssef et al. 2023). A risk of our approach is that our models focus only on the fires, disregarding both human influence on the landscape that may lead to increased risk of hazards (e.g. overdrawing from local water sources, deforestation, etc.), as well as the impact fires may have on humans. Predicting whether or not a fire will occur, or how large it will be, is only useful if it is applied to help those at risk from fires.\nWagenaar et al.’s 2020 paper “Invited perspectives: How machine learning will change flood risk and impact assessment” touches on some of these ethical considerations (Wagenaar et al. 2020). One risk they bring up is that improved knowledge of flood risk from ML models might result in protection of only high-value land or property owned by the wealthy. That example is certainly transferable to our fire hazard project. They also raise the question of privacy, noting that some people might not want it widely known that their home is considered “at-risk” for flooding, or other hazards. Finally, there is the question of data. The causes of hazards in one place may not cause the same hazard in another, so it is important to understand which human and geographic features influence hazard risk at a local scale rather than trying to train a one-size-fits-all model.\nWith all that in mind, our project needs to come with an asterisk. We have trained models for forest fire occurrence and scale in two specific places. It is therefore unreasonable to expect that our model will perfectly transfer to other places in the world. If our project were to be used for any application beyond the academic, we would need to make sure that its impact – whether that be for risk management, insurance policies, or some other application – be equitable and nondiscriminatory. We are just scratching the surface of using Machine Learning for natural hazard prevention, and while our results show it to be a powerful tool, we must also stay vigilant to make sure that it is a force for good."
  },
  {
    "objectID": "posts/CSCI04/example-blog-post/index.html",
    "href": "posts/CSCI04/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/CSCI04/example-blog-post/index.html#math",
    "href": "posts/CSCI04/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/CSCI04/perceptron_blog/perceptronBlogPost.html",
    "href": "posts/CSCI04/perceptron_blog/perceptronBlogPost.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "To view the source code containing the perceptron algorithm itself, pelase follow this link: https://github.com/madgallop/madgallop.github.io/blob/main/posts/perceptron_blog/perceptron.py\n\n\n\nFirst of all, what is the perceptron algorithm?\n\n\n\nScreen Shot 2023-02-27 at 5.05.36 PM.png\n\n\nThe source code containing my implementation of the perceptron algorithm can be found here: https://github.com/madgallop/madgallop.github.io/blob/main/posts/perceptron_blog/perceptron.py\nThis source code contains three primary functions: fit, predict, and score. Fit is the primary method of the perceptron, and is the one that actually performs the update. This function initializes a random initial weight vector and then until convergence, picks a random index, and updates the algorithm. Predict and score compare the labels predicted by the update with the true labels provided, and calculate the algorithm’s accuracy at each time stamp.\nThis blog post exemplifies the perceptron algorithm using three different examples: linearly seperable data in 2 dimensions, non-linearly seperable data in 2 dimensions, and data in 5 dimensions.\nBefore beginning, import all necessary packages\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\n\n\n\nIn this example, the perceptron algorithm converges, indicating that the data set is linearly seperable and can be divided well by a certain line.\n\n\n\nnp.random.seed(12345)\n\nn = 100 \np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Seperable Data in 2D\")\n\n\n\n\n\n\n\n\nfrom perceptron import Perceptron #import class from .py file\n\n\n#create an instance of the class and fit it to data.\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\np.w\nprint(\"Accuracy of the last 10 timestamps of the perceptron:\")\nprint(p.history[-10:]) #just the last few values\n\nAccuracy of the last 10 timestamps of the perceptron:\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy Over Time\")\n\n\n\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Perceptron on Seperable Data\")\n\n\n\n\n\n\n\n\np.score(X, y)\n\n1.0\n\n\n\n\n\n\nIn this example, the perceptron algorithm will not converge or settle on a final value, but will run until the specified maximum number of iterations is reached. In this example, the algorithm will not reach perfect accuracy.\n\n\n\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (-1.7, -1.7)])\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\nxlab2 = plt.xlabel(\"Feature 1\")\nylab2 = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Non-Seperable Data in 2D\")\n\n\n\n\n\n\n\n\nfrom perceptron import Perceptron\n\n#create an instance of the class and fit it to data.\np2 = Perceptron()\np2.fit(X2, y2, max_steps = 1000)\n\np2.w\nprint(\"Accuracy of the last 10 timestamps of the perceptron:\")\nprint(p2.history[-10:]) \n\nAccuracy of the last 10 timestamps of the perceptron:\n[0.58, 0.58, 0.5, 0.5, 0.48, 0.48, 0.5, 0.49, 0.49, 0.47]\n\n\n\n\n\n\n#visualize how the score evolved over time\nfig = plt.plot(p2.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy Over Time\")\n\n\n\n\n\n\n\n\n#visualize the line that the algorithm finds to separate the data\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2)\nfig = draw_line(p2.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.title(\"Perceptron on Non-Seperable Data\")\n\n\n\n\nHere, the seperating line is far from perfect. The line simply falls on whichever point was selected in the final iteration of the algorithm.\n\n\n\n\nIn this experiment, the algorithm will run on 5 features and then evaluate whether or not the data is linearly separable.\n\np_features3 = 6\n\nX3, y3 = make_blobs(n_samples = 100, n_features = p_features3 - 1, centers = [(-1.7,-1.7), (1.7,1.7)])\n\n\n\n\nfrom perceptron import Perceptron \n\n#create an instance of the class and fit it to data.\np3 = Perceptron()\np3.fit(X3, y3, max_steps = 1000)\n\np3.w\nprint(p3.history[-10:]) #just the last few values\n\n[0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 0.94, 1.0]\n\n\n\n\n\n\n#visualize how the score evolved over time\nfig = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\ntitle = plt.title(\"Accuracy Over Time\")\n\n\n\n\nIn this example, the perceptron converges and does so realtively quickly. The data originates from the data in Example 1, which was linearly seperable in 2 dimensions. Given that, the data should still be seperable in 5.\n\n\n\n\nThe runtime complexity of the perceptron algorithm is dependent on the number of features p the data has, but not with the number of points n for a single iteration of the perceptron algorithm."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "blog\n\n\nOpen Source GIScience\n\n\n\n\nThis blog debates whether or not open source science can provide a solution to the reproducibility crisis.\n\n\n\n\n\n\nSep 19, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblog\n\n\nOpen Source GIScience\n\n\n\n\nThis blog discusses whether or not, in my experience, GIS can be a science in its own right.\n\n\n\n\n\n\nSep 14, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nproject\n\n\nMachine Learning\n\n\n\n\nThis blog post details my final project for Machine Learning, done in collaboration with Nhi Dang, Wright Frost, and Eliza Weiman.\n\n\n\n\n\n\nMay 17, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblog\n\n\nMachine Learning\n\n\n\n\nThis blog post implements and experiments with linear algebra/ML methods for unsupervised learning for image compression.\n\n\n\n\n\n\nMay 6, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblog\n\n\nMachine Learning\n\n\n\n\nThis blog post includes a reflection on a recorded lecture by Dr. Timit Gebru, proposes a question for her based on this lecture and background research, and reflects on her talk to the wider Middlebury campus.\n\n\n\n\n\n\nApr 29, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblog\n\n\nMachine Learning\n\n\n\n\nThis blog post implements least-squares linear regression, and experiments with LASSO regularization for overparameterized problems.\n\n\n\n\n\n\nApr 1, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblog\n\n\nMachine Learning\n\n\n\n\nThis blog post is an essay discussing the limits of the quantitative approach to bias and fairness in allocative decision-making.\n\n\n\n\n\n\nMar 27, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblog\n\n\nMachine Learning\n\n\n\n\nThis blog post will explore a dataset and construct a model that will predict the species of a pengin based on its measurements with 100% testing accuracy.\n\n\n\n\n\n\nMar 26, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblog\n\n\nMachine Learning\n\n\n\n\nThis blog post implements simple gradient descent and stochastic gradient descent and compares their performances for training logistic regression.\n\n\n\n\n\n\nMar 3, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblog\n\n\nMachine Learning\n\n\n\n\nA blog post implementing and experimenting with the Perceptron Algorithm on a variety of data sets\n\n\n\n\n\n\nFeb 27, 2023\n\n\nMadeleine Gallop\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Madeleine. This blog documents my technical learning in CSCI 0451- Machine Learning in Spring 2023 and in GEOG 0323- Open Source GIScience in Fall 2023."
  }
]