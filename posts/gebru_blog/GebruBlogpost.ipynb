{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c935c021-d25d-4e95-9754-9f3ca956412a",
   "metadata": {},
   "source": [
    "---\n",
    "title: Learning from Timnit Gebru\n",
    "author: Madeleine Gallop\n",
    "date: '2023-04-19'\n",
    "image: \"Gebru.jpg\"\n",
    "description: \"This blog post includes a reflection on a recorded lecture by Dr. Timit Gebru, proposes a question for her based on this lecture and background research, and later will reflecton her talk to the wider Middlebury campus.\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d81d54-94a3-411d-934d-5215ac1233ba",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Next Monday, Dr. Timnit Gebru is coming to Middlebury to visit our class and give a talk to the wider campus community. I am excited to have the opportunity to learn from Dr. Gebru next week, as I found her research, advocacy, persistence, and honesty in sharing her personal experiences to be incredibly inspiring (even reading through her Wiki page made me a little emotional!) \n",
    "\n",
    "Dr. Gebru is a well-recognized voice in artificial intelligence and its ethical implications. She was born in Ethiopia and attended Stanford, then worked for Google and on projects such using Street View and deep learning to predict social variables of communities based on observations of cars in different places. Dr. Gebru experienced discrimination throughout her early life and professional career, and noticed a lack of Black researchers (and especially Black women) in AI. She founded Black in AI to provide a community for Black researchers in the field.   \n",
    "\n",
    "In 2019, Gebru and other researchers called out Amazon’s facial-recognition system for model discrimination against darker-skinned females, and in 2020, when Gebru was the co-lead of its Ethical Artificial Intelligence Team, Google dismissed her for refusing to withdraw an unpublished research paper she co-authored on the dangers of large language models (i.e. bais, costs, deception). The incident became a public controversy, as over 7,000 people signed a letter condemning her firing. \n",
    "\n",
    "In December of 2021, Dr. Gebru launched the Distributed Artificial Intelligence Research Institute to research the effects of AI on marginalized communities. \n",
    "\n",
    "\n",
    "## Dr. Gebru’s 2020 Talk \n",
    "\n",
    "In 2020, Dr. Gebru gave a talk as part of a Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision at the conference on Computer Vision and Pattern Recognition. \n",
    "\n",
    "In this talk, I really appreciated Dr. Gebru’s honest description of the lack of diversity and representation in artificial intelligence. I also found it interesting that she feels there have been improvements in diversity in the machine learning field, but that computer vision is not quite there yet. As a woman in STEM fields, I have definitely experienced being one of only a few women in classes or at conferences or other events. Even in my Conservation Planning class this semester (which is a technical geography course based in Python), I am one of two women in the class. While I have experienced sexism, I have obviously never experienced intersectional discrimination, and value Dr. Gebru’s candor in describing her experiences. \n",
    "\n",
    "After speaking about her own experiences, Dr. Gebru discusses what computer vision is, and how it has the potential to disproportionately harm marginalized communities. She goes into the racist way that computer vision screens potential employees through facial recognition. She urges us to consider questions that we have engaged with a little bit in this course already, such as who produces these technologies, and what groups they serve/leave out.  She brings up an important quote from Mimi Onuoha, which describes how “Every data set that involves people implies subjects and objects… it is imperative to remember that on both sides we have human beings.” She discusses how in ML and a variety of other fields, we do not see a diversity in datasets. But she notes that even in cases where we do, they do little good if we do not think critically about how we acquire them.\n",
    "\n",
    "I think the main takeaways I have and one of the most important points that Dr. Gebru discusses is that making something fair is not equivalent to making it the same for everyone. Fairness is not about math or statistics, but more about society and social structure. Even tools that are inclusive of multiple communities can still do harm, and the tools that we rely on, such as in computer vision, disproportionately harm marginalized communities on a national level. \n",
    "\n",
    "## Question \n",
    "\n",
    "You spoke at the end of your talk on fairness in computer vision at the conference on Computer Vision and Pattern Recognition about technology that harms and marginalizes various communities, as well as examples of refusal to combat these strategies. Since you gave this talk, have you seen improvements in either diversity in the field of computer vision or further examples of resistance to harmful technologies in the AI world? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f19b5-801b-445f-a150-f681183a32ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-0451] *",
   "language": "python",
   "name": "conda-env-ml-0451-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
